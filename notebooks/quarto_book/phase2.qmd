---
title: "Evaluation 2: Subcluster Analysis within Single-Label Groups"
jupyter: python3
lightbox: true
---

In this notebook, we analyze subclusters within specific **single-labeled** groups,
focusing on dual-labeled records that share a primary label of interest.


## Imports

```{python}
#| lines_to_next_cell: 2
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import umap
from matplotlib.lines import Line2D
from matplotlib.patches import Patch
from collections import defaultdict
from sklearn.cluster import KMeans, DBSCAN

# Ensure project path is in sys.path
project_root = Path().absolute().parent.parent
sys.path.append(str(project_root))

from src.visualization.embedding_viz import run_umap
from src.data.unified import UnifiedDataset
from src.data.dataset import DatasetModality

plt.style.use("seaborn-v0_8-whitegrid")
plt.rcParams["figure.figsize"] = (16, 8)
plt.rcParams["font.size"] = 12
```

## Color Palettes and Helpers

```{python}
SINGLE_COLOR_PALETTE = sns.color_palette("tab10", 11)
DUAL_COLOR_PALETTE = sns.color_palette("husl", 20)

FIXED_LABEL_COLORS = {
    "SR": SINGLE_COLOR_PALETTE[0],
    "AFIB": SINGLE_COLOR_PALETTE[1],
    "SB": SINGLE_COLOR_PALETTE[2],
    "GSVT": SINGLE_COLOR_PALETTE[3],
    "PACE": SINGLE_COLOR_PALETTE[4],
}

FIXED_SECONDARY_COLORS = {
    "STACH": (0.85, 0.37, 0.01),
    "SBRAD": (0.01, 0.66, 0.62),
    "SARRH": (0.58, 0.40, 0.74),
    "BIGU":  (0.17, 0.63, 0.17),
    "IVCD":  (0.84, 0.15, 0.16),
    "LAD":   (0.55, 0.35, 0.35),
    "RAD":   (0.94, 0.50, 0.50),
    "LVH":   (0.12, 0.47, 0.71),
    "RVH":   (0.68, 0.78, 0.91),
    "LNGQT": (0.46, 0.77, 0.35),
}

COLOR_PALETTES = {
    "Rhythm": SINGLE_COLOR_PALETTE,
    "Morphology": SINGLE_COLOR_PALETTE,
    "Duration": SINGLE_COLOR_PALETTE,
    "Amplitude": SINGLE_COLOR_PALETTE,
    "Other": SINGLE_COLOR_PALETTE,
}

GROUP_MARKERS = {
    "Rhythm": "o",
    "Morphology": "s",
    "Duration": "^",
    "Amplitude": "D",
    "Other": "X",
}

LABELS_OF_INTEREST = ["SR", "AFIB", "SB", "GSVT", "PACE"]
```

## Downsampling, Outlier Removal, and Color-Mapping Utilities

```{python}
def downsample_points(points, max_points=20, min_cluster_size=3, method="kmeans"):
    """
    Intelligently downsample points while preserving cluster structure.
    """
    if len(points) <= max_points:
        return points
    if method == "random":
        indices = np.random.choice(len(points), size=max_points, replace=False)
        return points[indices]
    elif method == "kmeans":
        n_clusters = min(max(min_cluster_size, len(points) // 5), max_points)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(points)
        sampled_points = []
        for cluster_id in range(n_clusters):
            cluster_points = points[cluster_labels == cluster_id]
            if len(cluster_points) < 3:
                continue
            n_samples = max(1, int(max_points * len(cluster_points) / len(points)))
            idx = np.random.choice(len(cluster_points), size=min(n_samples, len(cluster_points)), replace=False)
            sampled_points.append(cluster_points[idx])
        if sampled_points:
            return np.vstack(sampled_points)
        else:
            return downsample_points(points, max_points, min_cluster_size, method="random")
    elif method == "dbscan":
        dbscan = DBSCAN(eps=0.5, min_samples=min_cluster_size)
        cluster_labels = dbscan.fit_predict(points)
        sampled_points = []
        noise_points = points[cluster_labels == -1]
        if len(noise_points) > 0:
            n_noise_samples = min(max_points // 4, len(noise_points))
            if n_noise_samples > 0:
                idx = np.random.choice(len(noise_points), size=n_noise_samples, replace=False)
                sampled_points.append(noise_points[idx])
        unique_clusters = np.unique(cluster_labels)
        unique_clusters = unique_clusters[unique_clusters >= 0]
        for cluster_id in unique_clusters:
            cluster_points = points[cluster_labels == cluster_id]
            if len(cluster_points) < 3:
                continue
            n_samples = max(min_cluster_size, int(max_points * len(cluster_points) / len(points)))
            idx = np.random.choice(len(cluster_points), size=min(n_samples, len(cluster_points)), replace=False)
            sampled_points.append(cluster_points[idx])
        if sampled_points:
            return np.vstack(sampled_points)
        else:
            return downsample_points(points, max_points, min_cluster_size, method="random")
    else:
        return downsample_points(points, max_points, min_cluster_size, method="random")

def remove_outliers_2d(points, z_thresh=3.0):
    """
    Removes outliers in 2D by z-scoring each dimension
    and discarding points beyond z_thresh in Euclidean distance.
    """
    if len(points) < 5:
        return points
    z_scores = np.abs((points - np.mean(points, axis=0)) / np.std(points, axis=0))
    z_dist = np.sqrt(np.sum(z_scores**2, axis=1))
    mask = z_dist < z_thresh
    return points[mask]

def get_group_color_map(df_labels):
    """
    Generate a dict: group_label_map[group][label] -> color.
    df_labels must have 'integration_name' and 'group'.
    """
    all_labels = df_labels["integration_name"].unique()
    label_to_color = {}
    for label in all_labels:
        if label in FIXED_LABEL_COLORS:
            label_to_color[label] = FIXED_LABEL_COLORS[label]
    color_idx = 0
    for label in all_labels:
        if label not in label_to_color:
            while color_idx < len(SINGLE_COLOR_PALETTE) and any(SINGLE_COLOR_PALETTE[color_idx] == c for c in FIXED_LABEL_COLORS.values()):
                color_idx += 1
            if color_idx < len(SINGLE_COLOR_PALETTE):
                label_to_color[label] = SINGLE_COLOR_PALETTE[color_idx]
                color_idx += 1
            else:
                label_to_color[label] = (0.5, 0.5, 0.5)
    group_label_map = {}
    for _, row in df_labels.iterrows():
        label = row["integration_name"]
        group = row["group"]
        if group not in group_label_map:
            group_label_map[group] = {}
        if label not in group_label_map[group]:
            group_label_map[group][label] = label_to_color[label]
    return group_label_map
```

## Data Loading & Metadata Extraction

```{python}
print("Phase 2: Analyzing subclustering within single-label groups.")
arr_data = UnifiedDataset(Path(project_root) / "data", modality=DatasetModality.ECG, dataset_key="arrhythmia")
arr_splits = arr_data.get_splits()
arr_test_ids = arr_splits.get("test", [])
arr_md_store = arr_data.metadata_store

# Extract demographic metadata for all test records
demographic_data = {}
for rid in arr_test_ids:
    meta = arr_md_store.get(rid, {})
    age = meta.get("age", None)
    is_male = meta.get("is_male", None)
    demographic_data[rid] = {
        "age": age if isinstance(age, (int, float)) else None,
        "is_male": is_male if isinstance(is_male, bool) else None
    }
print(f"Extracted demographic data for {len(demographic_data)} records")

pretrained_embedding = "baseline"
finetuned_embedding = "fine_tuned_50"

def extract_extended_records_info(arr_test_ids, arr_md_store):
    records_info = []
    emb_base_list = []
    emb_ft_list = []
    for rid in arr_test_ids:
        meta = arr_md_store.get(rid, {})
        labels_meta = meta.get("labels_metadata", [])
        age = meta.get("age", None)
        is_male = meta.get("is_male", None)
        age = age if isinstance(age, (int, float)) else None
        is_male = is_male if isinstance(is_male, bool) else None
        try:
            emb_base = arr_data.get_embeddings(rid, embeddings_type=pretrained_embedding)
            emb_ft = arr_data.get_embeddings(rid, embeddings_type=finetuned_embedding)
        except Exception as e:
            print(f"Skipping {rid} (missing embeddings). Err: {e}")
            continue
        records_info.append({
            "record_id": rid,
            "labels_meta": labels_meta,
            "n_labels": len(labels_meta),
            "age": age,
            "is_male": is_male
        })
        emb_base_list.append(emb_base)
        emb_ft_list.append(emb_ft)
    return records_info, emb_base_list, emb_ft_list

records_info, emb_base_list, emb_ft_list = extract_extended_records_info(arr_test_ids, arr_md_store)
if not records_info:
    print("No records found. Exiting.")
    sys.exit()

df_records = pd.DataFrame(records_info)
df_records["row_idx"] = df_records.index
baseline_embeddings = np.vstack(emb_base_list)
finetuned_embeddings = np.vstack(emb_ft_list)

print(f"Total records loaded: {len(df_records)}")
print(" - Baseline shape:", baseline_embeddings.shape)
print(" - Fine-tuned shape:", finetuned_embeddings.shape)
print(f" - With age data: {df_records['age'].notna().sum()}")
print(f" - With sex data: {df_records['is_male'].notna().sum()}")
```

## Single-Labeled and Dual-Labeled Records

```{python}
mask_single = df_records["n_labels"] == 1
df_single = df_records[mask_single].copy()

df_single["integration_name"] = df_single["labels_meta"].apply(
    lambda lm: lm[0].get("integration_name", "unknown") if len(lm) == 1 else "unknown"
)
df_single["group"] = df_single["labels_meta"].apply(
    lambda lm: lm[0].get("group", "Other") if len(lm) == 1 else "Other"
)
print("Single-labeled records:", len(df_single))

mask_dual = df_records["n_labels"] == 2
df_dual = df_records[mask_dual].copy()
dual_label_info = []

for _, row in df_dual.iterrows():
    labels_meta = row["labels_meta"]
    if len(labels_meta) != 2:
        continue
    label1 = labels_meta[0].get("integration_name", "unknown")
    label2 = labels_meta[1].get("integration_name", "unknown")
    group1 = labels_meta[0].get("group", "Other")
    group2 = labels_meta[1].get("group", "Other")
    
    primary_label = None
    secondary_label = None
    primary_group = None
    secondary_group = None
    
    if label1 in LABELS_OF_INTEREST:
        primary_label = label1
        secondary_label = label2
        primary_group = group1
        secondary_group = group2
    elif label2 in LABELS_OF_INTEREST:
        primary_label = label2
        secondary_label = label1
        primary_group = group2
        secondary_group = group1
    if primary_label:
        dual_label_info.append({
            "record_id": row["record_id"],
            "row_idx": row["row_idx"],
            "primary_label": primary_label,
            "primary_group": primary_group,
            "secondary_label": secondary_label,
            "secondary_group": secondary_group,
            "combo_label": f"{primary_label}+{secondary_label}"
        })

df_dual_filtered = pd.DataFrame(dual_label_info)
print(f"Dual-labeled records with one label in {LABELS_OF_INTEREST}:", len(df_dual_filtered))
```

## UMAP on All Records

```{python}
print("\nRunning UMAP (baseline & fine-tuned) on all records...")
umap_params = dict(n_neighbors=15, n_components=2, metric="euclidean", random_state=42)
baseline_umap = run_umap(baseline_embeddings, **umap_params)
finetuned_umap = run_umap(finetuned_embeddings, **umap_params)
print("UMAP finished.\n")

single_labels = df_single[["integration_name", "group"]].drop_duplicates()
group_color_mapping = get_group_color_map(single_labels)

def get_global_secondary_color_map(df_dual):
    all_secondary_labels = sorted(df_dual["secondary_label"].unique())
    secondary_color_map = {}
    for label in all_secondary_labels:
        if label in FIXED_SECONDARY_COLORS:
            secondary_color_map[label] = FIXED_SECONDARY_COLORS[label]
    remaining_labels = [l for l in all_secondary_labels if l not in secondary_color_map]
    remaining_colors = DUAL_COLOR_PALETTE[:len(remaining_labels)]
    for i, label in enumerate(remaining_labels):
        secondary_color_map[label] = remaining_colors[i]
    return secondary_color_map

global_secondary_color_map = get_global_secondary_color_map(df_dual_filtered)
primary_to_secondary = defaultdict(list)
for _, row in df_dual_filtered.iterrows():
    primary_to_secondary[row["primary_label"]].append(row["secondary_label"])
for primary in primary_to_secondary:
    primary_to_secondary[primary] = sorted(set(primary_to_secondary[primary]))
```

## Subcluster Plot for Each Primary Label

```{python}
def plot_subclusters(ax, emb_2d, primary_label, title):
    ax.set_title(f"Label: {primary_label}", fontsize=14, fontweight='bold')
    primary_group = df_single[df_single["integration_name"] == primary_label]["group"].iloc[0]
    primary_color = group_color_mapping[primary_group][primary_label]
    single_mask = df_single["integration_name"] == primary_label
    single_rows = df_single[single_mask]
    if len(single_rows) == 0:
        ax.text(0.5, 0.5, f"No records with label {primary_label}",
                ha='center', va='center', transform=ax.transAxes)
        return
    primary_points = emb_2d[single_rows["row_idx"].values]
    clean_primary_points = remove_outliers_2d(primary_points, z_thresh=3.0)
    if len(clean_primary_points) >= 5:
        primary_df = pd.DataFrame(clean_primary_points, columns=["umap_x", "umap_y"])
        sns.kdeplot(
            data=primary_df,
            x="umap_x",
            y="umap_y",
            fill=True,
            levels=4,
            alpha=0.25,
            color=primary_color,
            ax=ax,
            label=None
        )
        ax.scatter(
            primary_df["umap_x"],
            primary_df["umap_y"],
            c=[primary_color],
            marker=GROUP_MARKERS.get(primary_group, "o"),
            s=40,
            alpha=0.6,
            edgecolors="white",
            linewidth=0.3,
            label=f"{primary_label}"
        )
    dual_mask = df_dual_filtered["primary_label"] == primary_label
    if dual_mask.any():
        dual_rows = df_dual_filtered[dual_mask]
        secondary_labels = sorted(dual_rows["secondary_label"].unique())
        for secondary_label in secondary_labels:
            secondary_mask = dual_rows["secondary_label"] == secondary_label
            if not secondary_mask.any():
                continue
            secondary_points = emb_2d[dual_rows[secondary_mask]["row_idx"].values]
            if len(secondary_points) >= 3:
                max_points_per_secondary = 25
                secondary_points_sampled = downsample_points(
                    secondary_points,
                    max_points=max_points_per_secondary,
                    min_cluster_size=3,
                    method="kmeans"
                )
                secondary_color = global_secondary_color_map.get(secondary_label, (0.5, 0.5, 0.5))
                ax.scatter(
                    secondary_points_sampled[:, 0],
                    secondary_points_sampled[:, 1],
                    c=[secondary_color],
                    marker="X",
                    s=80,
                    alpha=0.9,
                    edgecolors="black",
                    linewidth=0.5,
                    label=f"+{secondary_label} (n={len(secondary_points)})"
                )
    ax.set_xlabel("UMAP Dim 1", fontsize=12)
    ax.set_ylabel("UMAP Dim 2", fontsize=12)
    ax.grid(True, linestyle="--", alpha=0.3)
    handles, labels = ax.get_legend_handles_labels()
    if handles:
        ax.legend(handles=handles, labels=labels, loc="upper right", fontsize=9, framealpha=0.7)
```

## Per-Label Subcluster Plots for Baseline and Fine-Tuned

```{python}
#| lines_to_next_cell: 2
for embedding_name, embedding_data in [("Baseline", baseline_umap), ("Fine-tuned", finetuned_umap)]:
    n_labels = len(LABELS_OF_INTEREST)
    n_cols = 3
    n_rows = (n_labels + n_cols - 1) // n_cols
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))
    fig.suptitle(f"{embedding_name} Embeddings: Subcluster Analysis", fontsize=18, fontweight="bold", y=0.98)
    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes
    for i, label in enumerate(LABELS_OF_INTEREST):
        if i < len(axes):
            plot_subclusters(axes[i], embedding_data, label, embedding_name)
    for j in range(n_labels, len(axes)):
        axes[j].set_visible(False)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()
    plt.close()
```

## Extended Faceted Subcluster Plots

```{python}
from mpl_toolkits.axes_grid1 import make_axes_locatable

def create_faceted_subcluster_plots(embedding_data, title, figsize=(18, 15)):
    n_labels = len(LABELS_OF_INTEREST)
    n_cols = 3
    n_rows = (n_labels + n_cols - 1) // n_cols
    fig = plt.figure(figsize=figsize)
    gs = fig.add_gridspec(n_rows, n_cols, hspace=0.3, wspace=0.3, height_ratios=[1]*n_rows)
    fig.suptitle(f"{title}", fontsize=20, fontweight='bold', y=0.98)
    axes = []
    for i in range(n_rows):
        for j in range(n_cols):
            idx = i * n_cols + j
            if idx < n_labels:
                axes.append(fig.add_subplot(gs[i, j]))
            else:
                ax = fig.add_subplot(gs[i, j])
                ax.set_visible(False)
                axes.append(ax)
    for i, primary_label in enumerate(LABELS_OF_INTEREST):
        if i >= len(axes):
            break
        ax = axes[i]
        primary_group = df_single[df_single["integration_name"] == primary_label]["group"].iloc[0]
        primary_color = group_color_mapping[primary_group][primary_label]
        single_mask = df_single["integration_name"] == primary_label
        single_rows = df_single[single_mask]
        if len(single_rows) < 5:
            ax.text(0.5, 0.5, f"Not enough data for {primary_label}",
                    ha='center', va='center', transform=ax.transAxes)
            continue
        primary_points = embedding_data[single_rows["row_idx"].values]
        clean_primary_points = remove_outliers_2d(primary_points, z_thresh=3.0)
        primary_df = pd.DataFrame(clean_primary_points, columns=["umap_x", "umap_y"])
        primary_df["record_id"] = single_rows["record_id"].values[:len(primary_df)]
        primary_df["age"] = primary_df["record_id"].apply(lambda rid: demographic_data.get(rid, {}).get("age", None))
        primary_df["is_male"] = primary_df["record_id"].apply(lambda rid: demographic_data.get(rid, {}).get("is_male", None))
        valid_ages = primary_df["age"].dropna()
        age_stats = f"Age: {valid_ages.mean():.1f}±{valid_ages.std():.1f}" if len(valid_ages) > 0 else "Age: N/A"
        male_count = primary_df["is_male"].sum()
        female_count = (primary_df["is_male"] == False).sum()
        unknown_sex = primary_df["is_male"].isna().sum()
        sex_stats = f"Sex: {male_count}M/{female_count}F"
        if unknown_sex > 0:
            sex_stats += f" ({unknown_sex} unknown)"
        if "is_male" in primary_df.columns and not primary_df["is_male"].isna().all():
            male_df = primary_df[primary_df["is_male"] == True]
            if len(male_df) > 0:
                sns.scatterplot(
                    data=male_df, x="umap_x", y="umap_y",
                    color=primary_color, marker="o", s=40, alpha=0.7, ax=ax, label=f"{primary_label} (M)"
                )
            female_df = primary_df[primary_df["is_male"] == False]
            if len(female_df) > 0:
                sns.scatterplot(
                    data=female_df, x="umap_x", y="umap_y",
                    color=primary_color, marker="^", s=40, alpha=0.7, ax=ax, label=f"{primary_label} (F)"
                )
            unknown_df = primary_df[primary_df["is_male"].isna()]
            if len(unknown_df) > 0:
                sns.scatterplot(
                    data=unknown_df, x="umap_x", y="umap_y",
                    color=primary_color, marker="s", s=40, alpha=0.5, ax=ax, label=f"{primary_label} (?)"
                )
        else:
            sns.scatterplot(
                data=primary_df, x="umap_x", y="umap_y",
                color=primary_color, s=40, alpha=0.7, ax=ax, label=primary_label
            )
        if len(primary_df) >= 5:
            sns.kdeplot(
                data=primary_df, x="umap_x", y="umap_y",
                fill=True, levels=4, alpha=0.25, color=primary_color,
                bw_adjust=0.7, ax=ax
            )
        dual_mask = df_dual_filtered["primary_label"] == primary_label
        if dual_mask.any():
            dual_rows = df_dual_filtered[dual_mask]
            secondary_labels = sorted(dual_rows["secondary_label"].unique())
            top_secondaries = dual_rows["secondary_label"].value_counts().nlargest(5).index.tolist()
            for secondary_label in top_secondaries:
                secondary_mask = dual_rows["secondary_label"] == secondary_label
                if not secondary_mask.any():
                    continue
                secondary_points = embedding_data[dual_rows[secondary_mask]["row_idx"].values]
                if len(secondary_points) >= 3:
                    secondary_points_sampled = downsample_points(
                        secondary_points, max_points=25, min_cluster_size=3, method="kmeans"
                    )
                    secondary_color = global_secondary_color_map.get(secondary_label, (0.5, 0.5, 0.5))
                    ax.scatter(
                        secondary_points_sampled[:, 0],
                        secondary_points_sampled[:, 1],
                        c=[secondary_color], marker="X", s=100,
                        alpha=0.9, edgecolors="black", linewidth=0.5,
                        label=f"+{secondary_label} (n={len(secondary_points)})"
                    )
        ax.legend(fontsize=8, loc='upper right', framealpha=0.7)
        ax.set_xlabel("UMAP Dim 1")
        ax.set_ylabel("UMAP Dim 2")
        ax.set_aspect('auto')
        ax.set_title(f"Label: {primary_label}\n{age_stats}, {sex_stats}", fontsize=12, pad=12)
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        divider = make_axes_locatable(ax)
        ax_top = divider.append_axes("top", size="15%", pad=0.05)
        ax_top.tick_params(axis='both', which='both', labelbottom=False, labelleft=False)
        if len(primary_df) >= 5:
            sns.kdeplot(data=primary_df, x="umap_x", color=primary_color, bw_adjust=0.7, ax=ax_top)
            ax_top.set_xlim(xlim)
        ax_top.set_yticks([])
        ax_right = divider.append_axes("right", size="15%", pad=0.05)
        ax_right.tick_params(axis='both', which='both', labelbottom=False, labelleft=False)
        if len(primary_df) >= 5:
            sns.kdeplot(data=primary_df, y="umap_y", color=primary_color, bw_adjust=0.7, ax=ax_right)
            ax_right.set_ylim(ylim)
        ax_right.set_xticks([])
        valid_ages = primary_df["age"].dropna()
        if valid_ages.size > 10:
            ax_age = divider.append_axes("bottom", size="15%", pad=0.5)
            sns.histplot(valid_ages, bins=10, color=primary_color, alpha=0.6, ax=ax_age)
            ax_age.set_xlabel("Age")
            ax_age.set_ylabel("Count")
            ax_age.tick_params(labelsize=8)
    plt.tight_layout(rect=[0, 0, 1, 0.95], h_pad=0.2, w_pad=0.2)
    return fig

create_faceted_subcluster_plots(baseline_umap, "Baseline (Pre-trained) Embeddings - Subcluster Analysis")
plt.show()
plt.close()

create_faceted_subcluster_plots(finetuned_umap, "Fine-tuned (Chapman) Embeddings - Subcluster Analysis")
plt.show()
plt.close()
```

## Demographic Subcluster Plots

```{python}
def create_demographic_subcluster_plots(embedding_data, title, figsize=(18, 24)):
    n_labels = len(LABELS_OF_INTEREST)
    n_cols = 3
    n_rows = 2 * ((n_labels + n_cols - 1) // n_cols)
    fig = plt.figure(figsize=figsize)
    gs = fig.add_gridspec(n_rows, n_cols, hspace=0.3, wspace=0.3)
    fig.suptitle(f"{title} - Demographic Analysis", fontsize=20, fontweight='bold', y=0.98)
    axes = []
    for i in range(n_rows):
        for j in range(n_cols):
            idx = i * n_cols + j
            if idx < 2 * n_labels:
                axes.append(fig.add_subplot(gs[i, j]))
            else:
                ax = fig.add_subplot(gs[i, j])
                ax.set_visible(False)
                axes.append(ax)
    for i, primary_label in enumerate(LABELS_OF_INTEREST):
        if 2*i >= len(axes):
            break
        ax_sex = axes[2*i]
        ax_age = axes[2*i+1]
        ax_sex.set_title(f"{primary_label} - By Sex", fontsize=12, pad=10)
        ax_age.set_title(f"{primary_label} - By Age", fontsize=12, pad=10)
        primary_group = df_single[df_single["integration_name"] == primary_label]["group"].iloc[0]
        primary_color = group_color_mapping[primary_group][primary_label]
        single_mask = df_single["integration_name"] == primary_label
        single_rows = df_single[single_mask]
        if len(single_rows) < 5:
            for ax_demo in [ax_sex, ax_age]:
                ax_demo.text(0.5, 0.5, f"Not enough data for {primary_label}",
                             ha='center', va='center', transform=ax_demo.transAxes)
            continue
        primary_points = embedding_data[single_rows["row_idx"].values]
        clean_primary_points = remove_outliers_2d(primary_points, z_thresh=3.0)
        primary_df = pd.DataFrame(clean_primary_points, columns=["umap_x", "umap_y"])
        primary_df["record_id"] = single_rows["record_id"].values[:len(primary_df)]
        primary_df["age"] = primary_df["record_id"].apply(lambda rid: demographic_data.get(rid, {}).get("age", None))
        primary_df["is_male"] = primary_df["record_id"].apply(lambda rid: demographic_data.get(rid, {}).get("is_male", None))
        sex_df = primary_df.dropna(subset=["is_male"]).copy()
        age_df = primary_df.dropna(subset=["age"]).copy()
        if not sex_df.empty:
            sex_df["sex_category"] = sex_df["is_male"].apply(lambda x: "Male" if x else "Female")
            sex_palette = {"Male": "skyblue", "Female": "coral"}
            sns.scatterplot(
                data=sex_df,
                x="umap_x",
                y="umap_y",
                hue="sex_category",
                palette=sex_palette,
                s=40, alpha=0.7, ax=ax_sex
            )
            for sex_cat in ["Male", "Female"]:
                sex_group = sex_df[sex_df["sex_category"] == sex_cat]
                if len(sex_group) >= 10:
                    sns.kdeplot(
                        data=sex_group, x="umap_x", y="umap_y",
                        levels=3, alpha=0.3, fill=True,
                        color=sex_palette[sex_cat], ax=ax_sex
                    )
            male_count = sex_df["is_male"].sum()
            female_count = (sex_df["is_male"] == False).sum()
            sex_stats = f"Males: {male_count}, Females: {female_count}"
            ax_sex.annotate(
                sex_stats, xy=(0.5, 0.02), xycoords="axes fraction",
                ha="center", fontsize=10,
                bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8)
            )
        else:
            ax_sex.text(0.5, 0.5, "No sex data available",
                        ha='center', va='center', transform=ax_sex.transAxes)
        if not age_df.empty:
            norm = plt.Normalize(age_df["age"].min(), age_df["age"].max())
            sm = plt.cm.ScalarMappable(cmap="viridis", norm=norm)
            sm.set_array([])
            ax_age.scatter(
                age_df["umap_x"],
                age_df["umap_y"],
                c=age_df["age"],
                cmap="viridis",
                s=40,
                alpha=0.7
            )
            cbar = plt.colorbar(sm, ax=ax_age)
            cbar.set_label("Age (years)")
            if len(age_df) >= 10:
                sns.kdeplot(
                    data=age_df, x="umap_x", y="umap_y",
                    levels=4, alpha=0.2, linewidths=1,
                    color="black", ax=ax_age
                )
            age_stats = f"{age_df['age'].mean():.1f}±{age_df['age'].std():.1f}"
            ax_age.annotate(
                f"Age: {age_stats}", xy=(0.5, 0.02), xycoords="axes fraction",
                ha="center", fontsize=10,
                bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8)
            )
            if len(age_df) >= 20:
                clustering = DBSCAN(eps=0.5, min_samples=5).fit(age_df[["umap_x", "umap_y"]].values)
                age_df["cluster"] = clustering.labels_
                if max(clustering.labels_) >= 0:
                    cluster_ages = age_df.groupby("cluster")["age"].mean().to_dict()
                    for cluster_id, cluster_age in cluster_ages.items():
                        if cluster_id >= 0:
                            cluster_points = age_df[age_df["cluster"] == cluster_id]
                            center_x = cluster_points["umap_x"].mean()
                            center_y = cluster_points["umap_y"].mean()
                            ax_age.annotate(
                                f"{cluster_age:.0f} yrs", xy=(center_x, center_y),
                                fontsize=9, fontweight="bold",
                                ha="center", va="center",
                                bbox=dict(boxstyle="circle,pad=0.3", fc="white", ec="black", alpha=0.7)
                            )
        else:
            ax_age.text(0.5, 0.5, "No age data available",
                        ha='center', va='center', transform=ax_age.transAxes)
        ax_sex.set_xlabel("UMAP Dim 1")
        ax_sex.set_ylabel("UMAP Dim 2")
        ax_age.set_xlabel("UMAP Dim 1")
        ax_age.set_ylabel("UMAP Dim 2")
        ax_sex.set_aspect('auto')
        ax_age.set_aspect('auto')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    return fig

print("Generating demographic plots for baseline model...")
demographic_fig_baseline = create_demographic_subcluster_plots(
    baseline_umap, "Baseline Model - Demographic Subclustering Analysis"
)
plt.show()
plt.close()

print("Generating demographic plots for fine-tuned model...")
demographic_fig_finetuned = create_demographic_subcluster_plots(
    finetuned_umap, "Fine-tuned Model - Demographic Subclustering Analysis"
)
plt.show()
plt.close()
```

## Statistical Analysis of Subclusters

```{python}
print("\nStatistical Analysis of Subclusters:")
print("=" * 50)
single_counts = {}
dual_counts = {}
total_dual = 0

for primary_label in LABELS_OF_INTEREST:
    print(f"\nPrimary Label: {primary_label}")
    single_count = sum(df_single["integration_name"] == primary_label)
    single_counts[primary_label] = single_count
    print(f" - Single-labeled records: {single_count}")
    dual_mask = df_dual_filtered["primary_label"] == primary_label
    if not dual_mask.any():
        print(f" - No dual-labeled records with {primary_label}")
        dual_counts[primary_label] = 0
        continue
    dual_rows = df_dual_filtered[dual_mask]
    dual_count = len(dual_rows)
    dual_counts[primary_label] = dual_count
    total_dual += dual_count
    print(f" - Dual-labeled records: {dual_count}")
    secondary_counts = dual_rows["secondary_label"].value_counts()
    print(" - Secondary label distribution:")
    for label, count in secondary_counts.items():
        print(f"   * {label}: {count} records ({count/dual_count:.1%})")

print("\n\nSUMMARY OF FINDINGS")
print("=" * 50)
print(f"Total single-labeled records analyzed: {sum(single_counts.values())}")
print(f"Total dual-labeled records analyzed: {total_dual}")

for primary_label in LABELS_OF_INTEREST:
    single_count = single_counts[primary_label]
    dual_count = dual_counts[primary_label]
    total = single_count + dual_count
    if total > 0:
        print(f"\n{primary_label}:")
        print(f" - Single-labeled: {single_count} ({single_count/total:.1%})")
        print(f" - With secondary label: {dual_count} ({dual_count/total:.1%})")
```

