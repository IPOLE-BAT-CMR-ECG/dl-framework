[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A General Overview of Techniques and Visualizations",
    "section": "",
    "text": "This booklet provides an accessible overview of our bachelor thesis project, focusing on fine-tuning pre-trained ECG and CMR encoders using diverse datasets and evaluating their adaptability for cardiovascular diagnostics. Our goal is to establish foundational representations of cardiovascular data that can be expanded upon in future research, ultimately improving diagnostic capabilities for diverse patient populations.\nWe explore how fine-tuning these pre-trained models with diverse data impacts their ability to capture nuanced, population-specific patterns, and whether they can effectively adapt to new contexts, such as varying health conditions, ethnic backgrounds, and socioeconomic profiles. Additionally, we present a modular, machine learning pipeline designed to support ongoing research and future integration of multimodal data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A General Overview of Techniques and Visualizations</span>"
    ]
  },
  {
    "objectID": "dataset_cmr_acdc.html",
    "href": "dataset_cmr_acdc.html",
    "title": "2  Dataset - Automated Cardiac Diagnosis Challenge (ACDC)",
    "section": "",
    "text": "Study Population\nThe dataset includes 150 patients, categorized into the following five subgroups:\nThe groups are defined based on physiological parameters, such as ventricular volumes, ejection fractions, local contractions, LV mass, and the maximum thickness of the myocardium. The classification rules can be found in more detail in the relevant tab.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset - Automated Cardiac Diagnosis Challenge (ACDC)</span>"
    ]
  },
  {
    "objectID": "dataset_cmr_acdc.html#loading-the-cmr-dataset",
    "href": "dataset_cmr_acdc.html#loading-the-cmr-dataset",
    "title": "2  Dataset - Automated Cardiac Diagnosis Challenge (ACDC)",
    "section": "Loading the CMR Dataset",
    "text": "Loading the CMR Dataset\n\n\nCode\nimport sys\nimport os\nfrom pathlib import Path\n\nproject_root = str(Path().absolute().parent.parent)\nsys.path.append(project_root)\n\n# unified dataset\nfrom src.data.unified import UnifiedDataset\nfrom src.data.dataset import DatasetModality\nfrom src.visualization.cmr_viz import plot_processed_sample, plot_raw_sample, create_cardiac_cycle_animation\n\ndata_root = Path(project_root) / \"data\"\n\n\nLet’s explore the ACDC dataset and take a closer look at its content.\n\n\nCode\nacdc_data = UnifiedDataset(data_root, modality=DatasetModality.CMR, dataset_key=\"acdc\")\nacdc_data = acdc_data.raw_dataset\n\n# Load first patient data\npatient_record = acdc_data.load_record(\"patient001\")\n\n# Create patient data dictionary with all available metadata\npatient_data = {\n    \"data\": patient_record.data,\n    \"id\": patient_record.id,\n    \"group\": patient_record.target_labels,\n    \"ed_frame_idx\": patient_record.metadata[\"ed_frame_idx\"],\n    \"mid_frame_idx\": patient_record.metadata[\"mid_frame_idx\"],\n    \"es_frame_idx\": patient_record.metadata[\"es_frame_idx\"]\n}\n\n# Print metadata\nprint(\"\\nPatient Metadata:\")\nfor key, value in patient_record.metadata.items():\n    print(f\"{key}: {value}\")\n\n\n\nPatient Metadata:\ned_frame: 1\nes_frame: 12\nheight: 184.0\nweight: 95.0\nnb_frames: 30\ned_frame_idx: 0\nmid_frame_idx: 5\nes_frame_idx: 11\nnifti_path: training/patient001/patient001_4d.nii.gz\n\n\nThe dataset includes cardiac MRI data from multiple patients, each accompanied by relevant metadata. Here, we are focusing on one patient to get started.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset - Automated Cardiac Diagnosis Challenge (ACDC)</span>"
    ]
  },
  {
    "objectID": "dataset_cmr_acdc.html#visualizing-key-cardiac-frames",
    "href": "dataset_cmr_acdc.html#visualizing-key-cardiac-frames",
    "title": "2  Dataset - Automated Cardiac Diagnosis Challenge (ACDC)",
    "section": "Visualizing Key Cardiac Frames",
    "text": "Visualizing Key Cardiac Frames\nWe begin by examining the three key frames in the cardiac cycle: End-Diastolic (ED), Mid-Phase, and End-Systolic (ES).\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig1 = plot_processed_sample(\n    data=patient_data[\"data\"],\n    title=f'Patient {patient_data[\"id\"]} - {patient_data[\"group\"]}'\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.1: Key cardiac frames showing ED, Mid-Phase, and ES states.\n\n\n\n\n\nThe visualization in Figure 2.1 shows the heart’s state at three critical phases. These states provide important snapshots of the heart’s activity, giving insights into its pumping function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset - Automated Cardiac Diagnosis Challenge (ACDC)</span>"
    ]
  },
  {
    "objectID": "dataset_cmr_acdc.html#visualizing-the-sequence-of-frames",
    "href": "dataset_cmr_acdc.html#visualizing-the-sequence-of-frames",
    "title": "2  Dataset - Automated Cardiac Diagnosis Challenge (ACDC)",
    "section": "Visualizing the Sequence of Frames",
    "text": "Visualizing the Sequence of Frames\nSince our data contains only three frames (ED, Mid-Phase, and ES), we can visualize them sequentially to observe the changes.\n\n\nCode\nfig2 = plot_raw_sample(\n    data=acdc_data._read_nifti(acdc_data.paths[\"raw\"] / patient_record.metadata['nifti_path']),\n    frame_indices=[\n        patient_record.metadata[\"ed_frame_idx\"],\n        patient_record.metadata[\"mid_frame_idx\"],\n        patient_record.metadata[\"es_frame_idx\"]\n    ],\n    title=f'Patient {patient_data[\"id\"]} - Cardiac Phases'\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.2: Sequential visualization of cardiac phases\n\n\n\n\n\nThis sequence shown in Figure 2.2 represents the heart’s movement through the key phases. Observing these frames in order reveals the changing shape and size of the heart chambers, which is vital for understanding cardiac function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset - Automated Cardiac Diagnosis Challenge (ACDC)</span>"
    ]
  },
  {
    "objectID": "dataset_cmr_acdc.html#animated-visualization",
    "href": "dataset_cmr_acdc.html#animated-visualization",
    "title": "2  Dataset - Automated Cardiac Diagnosis Challenge (ACDC)",
    "section": "Animated Visualization",
    "text": "Animated Visualization\nFor an even clearer picture of how the heart changes across these phases, let’s create an animated version.\n\n\nCode\nfrom IPython.display import display, HTML\n\nhtml = create_cardiac_cycle_animation(\n    data=acdc_data._read_nifti(acdc_data.paths[\"raw\"] / patient_record.metadata['nifti_path']),\n    frame_indices=[\n        patient_record.metadata[\"ed_frame_idx\"],\n        patient_record.metadata[\"mid_frame_idx\"],\n        patient_record.metadata[\"es_frame_idx\"]\n    ]\n)\ndisplay(HTML(html))\n\n\n\n\n\n\n\nFigure 2.3: Animated visualization of cardiac cycle\n\n\n\n\nThe animated GIF in Figure 2.3 shows the heart transitioning through the ED, Mid-Phase, and ES frames, providing a clear and intuitive representation of its movement. This animation is especially helpful for understanding the coordinated action of different heart regions as they expand and contract.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset - Automated Cardiac Diagnosis Challenge (ACDC)</span>"
    ]
  },
  {
    "objectID": "dataset_cmr_acdc.html#cropping-based-on-turgut-biobank",
    "href": "dataset_cmr_acdc.html#cropping-based-on-turgut-biobank",
    "title": "2  Dataset - Automated Cardiac Diagnosis Challenge (ACDC)",
    "section": "Cropping based on Turgut Biobank",
    "text": "Cropping based on Turgut Biobank\nThe Turgut Biobank paper implements a custom manual cropping strategy with hard-coded values designed for their specific dataset. While this approach provides a standardized way to focus on cardiac regions, these fixed parameters may not be optimal for our ACDC dataset due to potential differences in image acquisition, patient positioning, and cardiac anatomical variations. Let’s implement and evaluate this cropping approach across our key cardiac frames.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef apply_turgut_crop(image, img_size):\n    \"\"\"Apply Turgut Biobank cropping parameters to numpy array\"\"\"\n    top = int(0.21 * img_size)\n    left = int(0.325 * img_size)\n    height = width = int(0.375 * img_size)\n    \n    # Ensure the image is large enough for cropping\n    if image.shape[0] &lt; top + height or image.shape[1] &lt; left + width:\n        raise ValueError(\"Image too small for specified crop dimensions\")\n        \n    return image[top:top+height, left:left+width]\n\ndef normalize_image(img):\n    \"\"\"Normalize image to 0-1 range\"\"\"\n    return (img - img.min()) / (img.max() - img.min())\n\n# Get the NIFTI data and relevant frames\noriginal_data = acdc_data._read_nifti(acdc_data.paths[\"raw\"] / patient_record.metadata['nifti_path'])\nframes = [\n    patient_record.metadata[\"ed_frame_idx\"],\n    patient_record.metadata[\"mid_frame_idx\"],\n    patient_record.metadata[\"es_frame_idx\"]\n]\nframe_names = ['ED', 'Mid', 'ES']\nmid_slice = original_data.shape[2] // 2\n\n# Create figure with three pairs of images\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\nfig.suptitle('Original vs Turgut Cropped Images', fontsize=14)\n\n# Process each frame\nfor idx, (frame, name) in enumerate(zip(frames, frame_names)):\n    img = original_data[:, :, mid_slice, frame]\n    img = normalize_image(img)\n    \n    try:\n        cropped_img = apply_turgut_crop(img, img.shape[0])\n        \n        # Plot original\n        axes[idx, 0].imshow(img, cmap='gray')\n        axes[idx, 0].set_title(f'Original - {name}')\n        axes[idx, 0].axis('off')\n        \n        # Plot cropped\n        axes[idx, 1].imshow(cropped_img, cmap='gray')\n        axes[idx, 1].set_title(f'Turgut Cropped - {name}')\n        axes[idx, 1].axis('off')\n        \n    except ValueError as e:\n        print(f\"Error processing {name} frame: {e}\")\n\nplt.tight_layout()\nplt.show()\n\n# Print crop dimensions for reference\nimg_size = img.shape[0]\nprint(f\"Original size: {img.shape}\")\nprint(f\"Cropped size: {cropped_img.shape}\")\nprint(f\"\\nTurgut cropping parameters:\")\nprint(f\"Top: {int(0.21 * img_size)} pixels\")\nprint(f\"Left: {int(0.325 * img_size)} pixels\")\nprint(f\"Height/Width: {int(0.375 * img_size)} pixels\")\n\n\n\n\n\n\n\n\nFigure 2.4: Comparison of original and Turgut-cropped cardiac images across ED, Mid, and ES frames\n\n\n\n\n\nOriginal size: (216, 256)\nCropped size: (81, 81)\n\nTurgut cropping parameters:\nTop: 45 pixels\nLeft: 70 pixels\nHeight/Width: 81 pixels\n\n\nThe Turgut Biobank cropping uses fixed relative parameters to focus on the central region of the heart: - Starting at 21% from the top - Starting at 32.5% from the left - Taking a square region that is 37.5% of the original image size\nBy examining all three cardiac phases (ED, Mid, ES), we can better evaluate whether these fixed cropping parameters consistently capture the relevant cardiac structures in our ACDC dataset. This analysis helps determine if we need to adjust these parameters for our specific use case.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset - Automated Cardiac Diagnosis Challenge (ACDC)</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html",
    "href": "dataset_ecg_arrhythmia.html",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "",
    "text": "4 Introduction\nThe 12-lead ECG dataset is a comprehensive repository created by Chapman University, Shaoxing People’s Hospital, and Ningbo First Hospital. It aims to facilitate research in arrhythmia detection and other cardiovascular studies. The dataset includes ECG signals collected from 45,152 patients, all labeled by professional experts, with a 500 Hz sampling rate. More details about the dataset can be found on the A large scale 12-lead electrocardiogram database for arrhythmia study website.\nThe ECG signals were collected as part of a clinical study to detect different types of arrhythmias and cardiovascular conditions. The dataset features ECGs in WFDB format, with both the raw data (.mat files) and the corresponding metadata (.hea files) containing information such as age, gender, lead configuration, and SNOMED CT codes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#comparing-raw-and-preprocessed-signals",
    "href": "dataset_ecg_arrhythmia.html#comparing-raw-and-preprocessed-signals",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Comparing Raw and Preprocessed Signals",
    "text": "Comparing Raw and Preprocessed Signals\nAfter preprocessing the ECG signals, we can view them in a more interpretable format. The preprocessing involves removing ALS baseline drift and normalizing the signals to have zero mean and unit variance. This step is critical for ensuring that subsequent models can learn meaningful patterns.\nDisplay the preprocessed ECG signals:\n\n\nCode\nplot_ecg_signals(sample_record.preprocessed_record.inputs, sample_record.preprocessed_record.metadata)\n\n\n\n\n\n\n\n\n\nTo quantify the changes introduced during preprocessing, we calculate the root mean square error (RMSE) between the raw and preprocessed signals. This metric provides a quantitative measure of the signal distortion due to preprocessing.\n\n\nCode\ndef calculate_rmse(signal1, signal2):\n    return np.sqrt(np.mean((signal1 - signal2) ** 2))\n\nraw_signal = sample_record.raw_record.data\npreprocessed_signal = sample_record.preprocessed_record.inputs.numpy()\n\nassert calculate_rmse(raw_signal, raw_signal) == 0.0, \"RMSE with itself should be zero\"\n\nrmse = calculate_rmse(raw_signal, preprocessed_signal)\nprint(f\"RMSE between raw and preprocessed signals: {rmse:.2f} microvolts\")\n\n\nRMSE between raw and preprocessed signals: 211.13 microvolts",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#visualizing-group-distribution-across-records",
    "href": "dataset_ecg_arrhythmia.html#visualizing-group-distribution-across-records",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Visualizing Group Distribution Across Records",
    "text": "Visualizing Group Distribution Across Records\nEach record may contain multiple labels that belong to one or more groups. We first extract the unique groups for each record so that if multiple labels belong to the same group, they are only counted once. The bar plot below shows the distribution of records per group.\n\n\nCode\ndef extract_unique_groups(label_metadata_list):\n    if not isinstance(label_metadata_list, list):\n        return []\n    # Use the \"group\" key and strip spaces; fallback to \"Unknown\" if missing.\n    groups = [d.get(\"group\", \"Unknown\").strip() for d in label_metadata_list if d.get(\"group\") is not None]\n    return list(set(groups))\n\nmetadata_df[\"unique_groups\"] = metadata_df[\"labels_metadata\"].apply(extract_unique_groups)\n\n# Explode the unique_groups list so that each record appears once per group\ngroup_exploded = metadata_df.explode(\"unique_groups\")\ngroup_counts = group_exploded[\"unique_groups\"].value_counts().reset_index()\ngroup_counts.columns = [\"group\", \"record_count\"]\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"group\", y=\"record_count\", data=group_counts, palette=\"Set2\")\nplt.title(\"Distribution of Records per Group\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Number of Records\")\nplt.xticks(rotation=45)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#analyzing-group-combination-patterns",
    "href": "dataset_ecg_arrhythmia.html#analyzing-group-combination-patterns",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Analyzing Group Combination Patterns",
    "text": "Analyzing Group Combination Patterns\nNext, we analyze the frequency of different group combinations within each record. For every record, we sort and store the unique groups as a tuple. This allows us to count how often each combination occurs.\n\n\nCode\nmetadata_df[\"group_combo\"] = metadata_df[\"unique_groups\"].apply(lambda x: tuple(sorted(x)))\ncombo_counts = metadata_df[\"group_combo\"].value_counts().reset_index()\ncombo_counts.columns = [\"group_combination\", \"record_count\"]\n\ntotal_records = len(metadata_df)\ncombo_counts[\"percentage\"] = 100 * combo_counts[\"record_count\"] / total_records\n\nprint(\"Group Combination Frequencies (Top 10):\")\nprint(combo_counts.head(10))\n\n# Convert tuple to string for more readable labels in the plot\ncombo_counts[\"combo_str\"] = combo_counts[\"group_combination\"].apply(lambda x: \" + \".join(x))\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x=\"record_count\", y=\"combo_str\", data=combo_counts.head(15), palette=\"magma\")\nplt.title(\"Top 15 Group Combinations Across Records\")\nplt.xlabel(\"Number of Records\")\nplt.ylabel(\"Group Combination\")\nplt.tight_layout()\nplt.show()\n\n\nGroup Combination Frequencies (Top 10):\n                           group_combination  record_count  percentage\n0                                  (Rhythm,)         23660   52.792467\n1                       (Morphology, Rhythm)          7850   17.515675\n2                        (Amplitude, Rhythm)          4080    9.103688\n3            (Amplitude, Morphology, Rhythm)          3672    8.193319\n4                         (Duration, Rhythm)          1866    4.163599\n5             (Duration, Morphology, Rhythm)          1423    3.175134\n6  (Amplitude, Duration, Morphology, Rhythm)           919    2.050561\n7              (Amplitude, Duration, Rhythm)           733    1.635540\n8                              (Morphology,)           371    0.827811\n9                     (Duration, Morphology)            75    0.167347",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#within-record-label-patterns-same-group-vs.-multiple-groups",
    "href": "dataset_ecg_arrhythmia.html#within-record-label-patterns-same-group-vs.-multiple-groups",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Within-Record Label Patterns: Same Group vs. Multiple Groups",
    "text": "Within-Record Label Patterns: Same Group vs. Multiple Groups\nHere, we compare the total number of labels per record (raw count) with the number of unique groups (after merging labels within the same group). This comparison highlights records where multiple labels within the same group have been merged.\n\n\nCode\n# Count total number of labels per record (raw, pre-merge)\nmetadata_df[\"total_labels\"] = metadata_df[\"labels_metadata\"].apply(lambda lst: len(lst) if isinstance(lst, list) else 0)\n\n# Count the number of unique groups per record (post merging)\nmetadata_df[\"num_unique_groups\"] = metadata_df[\"unique_groups\"].apply(len)\n\n# The difference indicates how many extra labels per record have been merged\nmetadata_df[\"merging_applied\"] = metadata_df.apply(lambda row: row[\"total_labels\"] - row[\"num_unique_groups\"], axis=1)\n\nsame_group_count = (metadata_df[\"total_labels\"] == metadata_df[\"num_unique_groups\"]).sum()\nmultiple_groups_count = (metadata_df[\"total_labels\"] &gt; metadata_df[\"num_unique_groups\"]).sum()\n\nprint(f\"Records with all labels in different groups: {same_group_count}\")\nprint(f\"Records with duplicate labels in the same group (merging candidates): {multiple_groups_count}\")\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x=\"num_unique_groups\", data=metadata_df, palette=\"coolwarm\")\nplt.title(\"Number of Unique Groups per Record\")\nplt.xlabel(\"Unique Groups Count\")\nplt.ylabel(\"Number of Records\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.histplot(metadata_df[\"merging_applied\"], bins=range(0, metadata_df[\"merging_applied\"].max()+2), kde=False)\nplt.title(\"Distribution of Merging Events per Record\")\nplt.xlabel(\"Number of Extra (Merged) Labels\")\nplt.ylabel(\"Number of Records\")\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n# In the subgroup merging logic, when a record has multiple labels in the same group,\n# only one label is kept (either the most common or the alphabetically first label).\n# Here, we quantify the effect of this logic by comparing the total raw labels (pre-merge)\n# with the merged labels (unique groups per record) in so called \"merging events\".\n\ntotal_raw_labels = metadata_df[\"total_labels\"].sum()\ntotal_merged_labels = metadata_df[\"num_unique_groups\"].sum()\n\nprint(\"Total number of labels (raw, pre-merge):\", total_raw_labels)\nprint(\"Total number of labels (after merging to unique groups):\", total_merged_labels)\nprint(\"Total number of merging events (labels removed):\", total_raw_labels - total_merged_labels)\n\naffected_records = (metadata_df[\"merging_applied\"] &gt; 0).sum()\nprint(f\"Number of records affected by merging: {affected_records} out of {total_records} ({(100*affected_records/total_records):.2f}%)\")\n\n# Visualizing the effect on a sample of records\nsample_records = metadata_df.sample(n=50, random_state=42).copy()\nsample_records = sample_records.sort_index()\n\nplt.figure(figsize=(12, 6))\nplt.plot(sample_records.index, sample_records[\"total_labels\"], \"o-\", label=\"Raw Label Count\", markersize=5)\nplt.plot(sample_records.index, sample_records[\"num_unique_groups\"], \"o-\", label=\"Unique Groups (After Merging)\", markersize=5)\nplt.title(\"Comparison of Raw Label Count vs. Merged Label Count (Sample of Records)\")\nplt.xlabel(\"Record Index (Sampled)\")\nplt.ylabel(\"Label Count\")\nplt.legend()\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\nRecords with all labels in different groups: 39130\nRecords with duplicate labels in the same group (merging candidates): 5687\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal number of labels (raw, pre-merge): 81336\nTotal number of labels (after merging to unique groups): 73224\nTotal number of merging events (labels removed): 8112\nNumber of records affected by merging: 5687 out of 44817 (12.69%)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#heatmap-of-group-co-occurrence",
    "href": "dataset_ecg_arrhythmia.html#heatmap-of-group-co-occurrence",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Heatmap of Group Co-occurrence",
    "text": "Heatmap of Group Co-occurrence\nThe heatmap below shows the normalized cross-correlation of group memberships. It displays the percentage of records that contain each pair of groups. For clarity, only the lower triangle of the matrix is shown.\n\n\nCode\nall_groups = sorted(set(grp for groups in metadata_df[\"unique_groups\"] for grp in groups))\nco_occurrence = pd.DataFrame(0, index=all_groups, columns=all_groups)\n\nfor groups in metadata_df[\"unique_groups\"]:\n    for g1 in groups:\n        for g2 in groups:\n            co_occurrence.loc[g1, g2] += 1\n\n# Normalize the co-occurrence counts to percentages (with respect to the total number of records)\nco_occurrence_percent = (co_occurrence / len(metadata_df)) * 100\n\n# Create a mask to hide the upper triangle of the heatmap\nmask = np.triu(np.ones_like(co_occurrence_percent, dtype=bool))\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(co_occurrence_percent, mask=mask, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\nplt.title(\"Normalized Heatmap of Group Co-occurrence (%)\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Group\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#final-label-distribution-comparison-raw-vs.-merged",
    "href": "dataset_ecg_arrhythmia.html#final-label-distribution-comparison-raw-vs.-merged",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Final Label Distribution Comparison (Raw vs. Merged)",
    "text": "Final Label Distribution Comparison (Raw vs. Merged)\nFinally, we compare the raw label distributions with the final distributions after applying the merging logic. For each group, we normalize the counts to percentages and create side-by-side bar plots. This comparison helps illustrate the impact of the merging process on the dataset.\n\n\nCode\nraw_label_counts = {}\nfor idx, row in metadata_df.iterrows():\n    for meta in row[\"labels_metadata\"]:\n        group = meta.get(\"group\", \"Unknown\").strip()\n        # Use the first diagnosis name as the readable label.\n        readable = get_readable_label(meta)\n        raw_label_counts.setdefault(group, {})\n        raw_label_counts[group][readable] = raw_label_counts[group].get(readable, 0) + 1\n\n# Determine the most common integration code per group (based on raw counts) for merging logic.\nmost_common_label_by_group = {}\nfor group, counts in raw_label_counts.items():\n    # Find the integration code corresponding to the highest count.\n    # Since raw_label_counts is keyed by readable label, we need to recover the integration code.\n    # For simplicity, we assume that the most common readable label corresponds uniquely to an integration code.\n    # To ensure consistency, we iterate over the records again.\n    label_counter = {}\n    for idx, row in metadata_df.iterrows():\n        for meta in row[\"labels_metadata\"]:\n            if meta.get(\"group\", \"Unknown\").strip() == group:\n                code = meta.get(\"integration_code\", \"Unlabeled\")\n                label_counter[code] = label_counter.get(code, 0) + 1\n    if label_counter:\n        most_common_label_by_group[group] = max(label_counter, key=label_counter.get)\n    else:\n        most_common_label_by_group[group] = \"Unlabeled\"\n\n# Now, simulate final (merged) label assignment across all records.\nfinal_distribution = {}\nfor idx, row in metadata_df.iterrows():\n    merged = merge_labels_for_record(row[\"labels_metadata\"], most_common_label_by_group)\n    for group, (code, readable) in merged.items():\n        final_distribution.setdefault(group, {})\n        final_distribution[group][readable] = final_distribution[group].get(readable, 0) + 1\n\n\nAfter computing the final label distributions, we plot side-by-side comparison plots for each group. The left plot shows the raw distribution, while the right plot displays the final distribution after merging.\n\n\nCode\n# For each group, create side-by-side comparison plots:\n# Left: Raw distribution (normalized to percentage)\n# Right: Final (merged) distribution (normalized to percentage)\ngroups_sorted = sorted(raw_label_counts.keys())\n\nfor group in groups_sorted:\n    # Prepare raw data: Convert counts to percentages.\n    raw_counts = raw_label_counts.get(group, {})\n    total_raw = sum(raw_counts.values())\n    raw_df = pd.DataFrame([\n        {\"diagnosis\": diag, \"count\": cnt, \"percentage\": (cnt / total_raw) * 100}\n        for diag, cnt in raw_counts.items()\n    ]).sort_values(\"percentage\", ascending=True)  # sort ascending for horizontal barplot\n    \n    # Prepare final data: Convert counts to percentages.\n    final_counts = final_distribution.get(group, {})\n    total_final = sum(final_counts.values())\n    final_df = pd.DataFrame([\n        {\"diagnosis\": diag, \"count\": cnt, \"percentage\": (cnt / total_final) * 100}\n        for diag, cnt in final_counts.items()\n    ]).sort_values(\"percentage\", ascending=True)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Plot raw distribution using horizontal barplot\n    sns.barplot(x=\"percentage\", y=\"diagnosis\", data=raw_df, ax=axes[0], palette=\"Blues_d\", hue='diagnosis')\n    axes[0].set_title(f\"Raw Label Distribution for Group '{group}'\")\n    axes[0].set_xlabel(\"Percentage (%)\")\n    axes[0].set_ylabel(\"Diagnosis\")\n    axes[0].grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n    # Annotate each bar with its percentage\n    for patch in axes[0].patches:\n        width = patch.get_width()\n        y = patch.get_y() + patch.get_height() / 2\n        axes[0].text(width + 0.5, y, f'{width:.1f}%', va='center', fontsize=9, color='black')\n    \n    # Plot final (merged) distribution using horizontal barplot\n    sns.barplot(x=\"percentage\", y=\"diagnosis\", data=final_df, ax=axes[1], palette=\"Greens_d\", hue='diagnosis')\n    axes[1].set_title(f\"Final Label Distribution for Group '{group}' (After Merging)\")\n    axes[1].set_xlabel(\"Percentage (%)\")\n    axes[1].set_ylabel(\"\")\n    axes[1].grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n    # Annotate each bar with its percentage\n    for patch in axes[1].patches:\n        width = patch.get_width()\n        y = patch.get_y() + patch.get_height() / 2\n        axes[1].text(width + 0.5, y, f'{width:.1f}%', va='center', fontsize=9, color='black')\n    \n    plt.suptitle(f\"Label Distribution Comparison for Group '{group}'\", fontsize=16)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#impact-of-label-merging-on-records-by-group",
    "href": "dataset_ecg_arrhythmia.html#impact-of-label-merging-on-records-by-group",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Impact of Label Merging on Records by Group",
    "text": "Impact of Label Merging on Records by Group\nIn many records, multiple labels may fall into the same group. In our merging logic, if a record has more than one label for a given group, these labels are merged into a single representative label. In this section, we quantify the impact of this merging process on a per‐group basis. For each group, we compute the percentage of records that have multiple labels (and hence are impacted by merging) versus those that have only one label.\nThe stacked bar plot below shows, for each group, the percentage of records where merging is applied (“Affected by Merging”) and those where no merging is needed (“No Merging”). The percentages are annotated on each bar segment. For clarification purposes, a record belong to a group if it has n &gt;= 1 labels for that group. If n &gt; 1, then merging is applied to obtain a single label. Thus, the “Affected by Merging” segment can also be looked at as the percentage of records where n &gt; 1.\n\n\nCode\n# Function to count the number of labels per group for a given record.\ndef count_labels_by_group(label_metadata):\n    counts = {}\n    if isinstance(label_metadata, list):\n        for meta in label_metadata:\n            group = meta.get(\"group\", \"Unknown\").strip()\n            counts[group] = counts.get(group, 0) + 1\n    return counts\n\n# For each record, count how many labels exist for each group.\nmetadata_df[\"group_label_counts\"] = metadata_df[\"labels_metadata\"].apply(count_labels_by_group)\n\n# For each group, count records where merging is applied (more than one label) vs. not applied (exactly one label).\ngroup_stats = {}\nfor idx, row in metadata_df.iterrows():\n    counts = row[\"group_label_counts\"]\n    for group, count in counts.items():\n        if group not in group_stats:\n            group_stats[group] = {\"merged\": 0, \"not_merged\": 0}\n        if count &gt; 1:\n            group_stats[group][\"merged\"] += 1\n        else:\n            group_stats[group][\"not_merged\"] += 1\n\n# Convert the group statistics to a DataFrame for plotting.\ngroup_stats_df = pd.DataFrame([\n    {\n        \"group\": group,\n        \"merged\": stats[\"merged\"],\n        \"not_merged\": stats[\"not_merged\"],\n        \"total\": stats[\"merged\"] + stats[\"not_merged\"]\n    }\n    for group, stats in group_stats.items()\n])\n\n# Calculate percentages for each group.\ngroup_stats_df[\"pct_merged\"] = 100 * group_stats_df[\"merged\"] / group_stats_df[\"total\"]\ngroup_stats_df[\"pct_not_merged\"] = 100 * group_stats_df[\"not_merged\"] / group_stats_df[\"total\"]\n\n# Sort groups for a cleaner plot.\ngroup_stats_df = group_stats_df.sort_values(\"total\", ascending=False)\n\n# Create a stacked bar plot to visualize the impact of merging by group.\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the \"No Merging\" segment as the base.\nbars_not = ax.bar(group_stats_df[\"group\"], group_stats_df[\"pct_not_merged\"],\n                  label=\"No Merging\", color=\"skyblue\")\n\n# Plot the \"Merging Applied\" segment on top.\nbars_merge = ax.bar(group_stats_df[\"group\"], group_stats_df[\"pct_merged\"],\n                    bottom=group_stats_df[\"pct_not_merged\"], label=\"Affected by Merging\", color=\"salmon\")\n\nax.set_ylabel(\"Percentage of Records in Group (%)\")\nax.set_title(\"Impact of Label Merging by Group\")\nax.legend()\nplt.xticks(rotation=45)\n\n# Annotate bar segments with the percentage values.\nfor rect in bars_not:\n    height = rect.get_height()\n    if height &gt; 0:\n        ax.text(rect.get_x() + rect.get_width()/2, rect.get_y() + height/2,\n                f\"{height:.1f}%\", ha=\"center\", va=\"center\", fontsize=9)\n\nfor rect, base in zip(bars_merge, group_stats_df[\"pct_not_merged\"]):\n    height = rect.get_height()\n    if height &gt; 0:\n        ax.text(rect.get_x() + rect.get_width()/2, base + height/2,\n                f\"{height:.1f}%\", ha=\"center\", va=\"center\", fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows the per-group impact of label merging with clear percentage annotations on each segment, allowing you to see the exact proportion of records that required merging versus those that did not. We can see that especially the “Morphology” group has a high percentage of records where merging was applied (about 25%). For the other groups the percentage is lower.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#comparison-of-label-combination-prevalence-raw-vs.-merged",
    "href": "dataset_ecg_arrhythmia.html#comparison-of-label-combination-prevalence-raw-vs.-merged",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Comparison of Label Combination Prevalence: Raw vs. Merged",
    "text": "Comparison of Label Combination Prevalence: Raw vs. Merged\nIn addition to the per‐group merging impact, it is instructive to examine how label combination patterns change as a result of merging. In the raw data, a record may have duplicate group entries (e.g. [\"A\", \"A\", \"B\"]), whereas after merging, each record contains only unique group labels (e.g. [\"A\", \"B\"]). The following visualizations compare the prevalence of label combinations before and after merging.\nWe create two new columns: one for the raw label combinations (including duplicates) and one for the merged (unique) combinations. To avoid issues with tuple-based categories, we convert each tuple into a string representation.\n\n\nCode\n# Create a raw combination by collecting all group entries (including duplicates) per record.\nmetadata_df[\"raw_combo\"] = metadata_df[\"labels_metadata\"].apply(\n    lambda lst: tuple(sorted([meta.get(\"group\", \"Unknown\").strip() for meta in lst]))\n)\n\n# The merged combination is already computed as unique groups.\nmetadata_df[\"merged_combo\"] = metadata_df[\"unique_groups\"].apply(lambda x: tuple(sorted(x)))\n\n# Compute the frequency counts for both raw and merged combinations.\nraw_combo_counts = metadata_df[\"raw_combo\"].value_counts().reset_index()\nraw_combo_counts.columns = [\"combo\", \"count\"]\nmerged_combo_counts = metadata_df[\"merged_combo\"].value_counts().reset_index()\nmerged_combo_counts.columns = [\"combo\", \"count\"]\n\n# Create string representations for the combinations.\nraw_combo_counts[\"combo_str\"] = raw_combo_counts[\"combo\"].apply(lambda x: \" + \".join(x) if isinstance(x, tuple) else str(x))\nmerged_combo_counts[\"combo_str\"] = merged_combo_counts[\"combo\"].apply(lambda x: \" + \".join(x) if isinstance(x, tuple) else str(x))\n\n# Calculate percentages with respect to the total number of records.\ntotal_records = len(metadata_df)\nraw_combo_counts[\"percentage\"] = 100 * raw_combo_counts[\"count\"] / total_records\nmerged_combo_counts[\"percentage\"] = 100 * merged_combo_counts[\"count\"] / total_records\n\n\nThe side-by-side horizontal bar plots below show the top 10 most common label combinations before merging (raw) and after merging. Each bar is annotated with the percentage of records that have that combination.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(16, 8), sharey=True)\n\n# Plot raw combination frequencies.\nsns.barplot(x=\"percentage\", y=\"combo_str\", data=raw_combo_counts.head(10),\n            ax=axes[0], palette=\"Blues_d\")\naxes[0].set_title(\"Top 10 Label Combinations (Raw)\")\naxes[0].set_xlabel(\"Percentage of Records (%)\")\naxes[0].set_ylabel(\"Label Combination\")\nfor p in axes[0].patches:\n    width = p.get_width()\n    axes[0].text(width + 0.5, p.get_y() + p.get_height()/2,\n                 f\"{width:.1f}%\", va=\"center\", fontsize=9)\n\n# Plot merged combination frequencies.\nsns.barplot(x=\"percentage\", y=\"combo_str\", data=merged_combo_counts.head(10),\n            ax=axes[1], palette=\"Greens_d\")\naxes[1].set_title(\"Top 10 Label Combinations (After Merging)\")\naxes[1].set_xlabel(\"Percentage of Records (%)\")\naxes[1].set_ylabel(\"\")\nfor p in axes[1].patches:\n    width = p.get_width()\n    axes[1].text(width + 0.5, p.get_y() + p.get_height()/2,\n                 f\"{width:.1f}%\", va=\"center\", fontsize=9)\n\nplt.suptitle(\"Comparison of Label Combination Prevalence: Raw vs. Merged\", fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plots show the top 10 most common label combinations before and after merging. The annotations on each bar segment provide a clear view of the percentage of records that have that combination. The merging process has effectively has left the set of labels per record as expected with unique group labels.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "dataset_ecg_arrhythmia.html#diagnosis-co-occurrence-heatmaps-using-diagnosis-names",
    "href": "dataset_ecg_arrhythmia.html#diagnosis-co-occurrence-heatmaps-using-diagnosis-names",
    "title": "3  Dataset - 12-lead ECG for Arrhythmia Study",
    "section": "Diagnosis Co-occurrence Heatmaps (Using Diagnosis Names)",
    "text": "Diagnosis Co-occurrence Heatmaps (Using Diagnosis Names)\nIn addition to the previous visualizations based on group names, we now examine how diagnosis names co-occur across records. Because each label’s metadata may contain multiple diagnosis names, we define a helper function that combines them into a single string (separated by \" / \"). We then compute two heatmaps: - One for the raw diagnosis names (from all labels per record), and\n- One for the merged diagnosis names (after applying our merging logic).\n\nRaw Diagnosis Co-occurrence Heatmap\nFor the raw data, we extract the diagnosis name from each label and compute a co-occurrence matrix counting, for every pair of diagnosis names, in how many records they appear together.\n\n\nCode\n# Define a helper function to combine diagnosis names from a label's metadata.\ndef combined_diagnosis(meta):\n    names = meta.get(\"diagnosis_names\")\n    if isinstance(names, list) and len(names) &gt; 0:\n        return \" / \".join(sorted(set(names)))\n    else:\n        return meta.get(\"integration_code\", \"Unlabeled\")\n\n# For each record, compute the set of raw diagnosis names.\nraw_diagnosis_sets = metadata_df[\"labels_metadata\"].apply(\n    lambda lst: set([combined_diagnosis(meta) for meta in lst]) if isinstance(lst, list) else set()\n)\n\n# Get all unique diagnosis names from raw data.\nall_raw_diagnoses = sorted({d for ds in raw_diagnosis_sets for d in ds})\n\n# Initialize the co-occurrence matrix.\nraw_co_occurrence = pd.DataFrame(0, index=all_raw_diagnoses, columns=all_raw_diagnoses)\n\n# Populate the matrix: for each record, for each pair of diagnosis names, increment the count.\nfor ds in raw_diagnosis_sets:\n    for d1 in ds:\n        for d2 in ds:\n            raw_co_occurrence.loc[d1, d2] += 1\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(raw_co_occurrence, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Raw Diagnosis Co-occurrence Heatmap (Counts)\")\nplt.xlabel(\"Diagnosis Name\")\nplt.ylabel(\"Diagnosis Name\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMerged Diagnosis Co-occurrence Heatmap\nFor the merged labels, each record has at most one label per group. We use our previously defined merging function (merge_labels_for_record) and the dictionary most_common_label_by_group (computed earlier) to obtain the merged labels. Then, we extract the diagnosis names from these merged results and compute their co-occurrence counts.\n\n\nCode\n# Compute merged labels for each record (if not already computed).\nmetadata_df[\"merged_labels\"] = metadata_df[\"labels_metadata\"].apply(\n    lambda lst: merge_labels_for_record(lst, most_common_label_by_group) if isinstance(lst, list) else {}\n)\n\n# For each record, extract the set of merged diagnosis names.\nmerged_diagnosis_sets = metadata_df[\"merged_labels\"].apply(\n    lambda d: set(val[1] for val in d.values())\n)\n\n# Get all unique merged diagnosis names.\nall_merged_diagnoses = sorted({d for ds in merged_diagnosis_sets for d in ds})\n\n# Initialize the merged co-occurrence matrix.\nmerged_co_occurrence = pd.DataFrame(0, index=all_merged_diagnoses, columns=all_merged_diagnoses)\n\n# Populate the matrix for merged diagnosis names.\nfor ds in merged_diagnosis_sets:\n    for d1 in ds:\n        for d2 in ds:\n            merged_co_occurrence.loc[d1, d2] += 1\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(merged_co_occurrence, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Merged Diagnosis Co-occurrence Heatmap (Counts)\")\nplt.xlabel(\"Diagnosis Name\")\nplt.ylabel(\"Diagnosis Name\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Diagnosis Co-occurrence Heatmaps (Using Diagnosis Names) by Group\n\n# Get list of all groups from the label metadata in the dataset.\nall_groups = sorted({meta.get(\"group\", \"Unknown\").strip() \n                     for lst in metadata_df[\"labels_metadata\"] if isinstance(lst, list) \n                     for meta in lst})\n\n# Define a helper function to combine diagnosis names from a label's metadata.\ndef combined_diagnosis(meta):\n    names = meta.get(\"diagnosis_names\")\n    if isinstance(names, list) and len(names) &gt; 0:\n        # Combine multiple diagnosis names with a separator.\n        return \" / \".join(sorted(set(names)))\n    else:\n        return meta.get(\"integration_code\", \"Unlabeled\")\n\n# Loop over each group and compute separate heatmaps for raw and merged diagnosis co-occurrence.\nfor grp in all_groups:\n    ### RAW Diagnosis Co-occurrence for Group: grp\n    # For each record, extract diagnosis names from raw labels that belong to the current group.\n    raw_diag_sets = metadata_df[\"labels_metadata\"].apply(\n        lambda lst: set([combined_diagnosis(meta) \n                         for meta in lst \n                         if meta.get(\"group\", \"Unknown\").strip() == grp]) \n                    if isinstance(lst, list) else set()\n    )\n    # Filter out records that don't have any diagnosis for this group.\n    raw_diag_sets = raw_diag_sets[raw_diag_sets.apply(lambda s: len(s) &gt; 0)]\n    \n    # Get all unique diagnosis names for this group.\n    unique_raw_diag = sorted({d for s in raw_diag_sets for d in s})\n    \n    # Initialize the co-occurrence matrix for raw diagnosis names.\n    raw_co_occ = pd.DataFrame(0, index=unique_raw_diag, columns=unique_raw_diag)\n    \n    # Populate the matrix: for each record, increment counts for every pair of diagnosis names that co-occur.\n    for diag_set in raw_diag_sets:\n        for d1 in diag_set:\n            for d2 in diag_set:\n                raw_co_occ.loc[d1, d2] += 1\n    \n    # Plot the heatmap for raw diagnosis co-occurrence for this group.\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(raw_co_occ, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n    plt.title(f\"Raw Diagnosis Co-occurrence Heatmap for Group: {grp}\")\n    plt.xlabel(\"Diagnosis Name\")\n    plt.ylabel(\"Diagnosis Name\")\n    plt.tight_layout()\n    plt.show()\n    \n    ### Merged Diagnosis Co-occurrence for Group: grp\n    # For merged labels, each record has at most one label per group.\n    # Extract the merged diagnosis name for the current group.\n    merged_diag = metadata_df[\"merged_labels\"].apply(\n        lambda d: d.get(grp, (None, None))[1] if isinstance(d, dict) and grp in d else None\n    )\n    # Keep only non-None values.\n    merged_diag = merged_diag.dropna()\n    \n    # In merged labels, since each record contributes only one diagnosis per group,\n    # the co-occurrence matrix will be diagonal (each record only \"co-occurs\" with itself).\n    unique_merged_diag = merged_diag.unique()\n    merged_counts = merged_diag.value_counts().sort_index()\n    merged_co_occ = pd.DataFrame(0, index=unique_merged_diag, columns=unique_merged_diag)\n    for diag in unique_merged_diag:\n        merged_co_occ.loc[diag, diag] = merged_counts[diag]\n    \n    # Plot the heatmap for merged diagnosis co-occurrence for this group.\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(merged_co_occ, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n    plt.title(f\"Merged Diagnosis Co-occurrence Heatmap for Group: {grp}\")\n    plt.xlabel(\"Diagnosis Name\")\n    plt.ylabel(\"Diagnosis Name\")\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset - 12-lead ECG for Arrhythmia Study</span>"
    ]
  },
  {
    "objectID": "phase1.html",
    "href": "phase1.html",
    "title": "4  Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)",
    "section": "",
    "text": "Imports\nHere, we import Python standard libraries and our local project modules.\nCode\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\nimport matplotlib.patheffects as PathEffects\n\n# Ensure project path is in sys.path\nproject_root = Path().absolute().parent.parent\nsys.path.append(str(project_root))\n\n# Our project modules\nfrom src.visualization.embedding_viz import run_umap\nfrom src.data.unified import UnifiedDataset\nfrom src.data.dataset import DatasetModality\n\n# Matplotlib style\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (16, 8)\nplt.rcParams[\"font.size\"] = 12",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)</span>"
    ]
  },
  {
    "objectID": "phase1.html#color-palettes-and-helpers",
    "href": "phase1.html#color-palettes-and-helpers",
    "title": "4  Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)",
    "section": "Color Palettes and Helpers",
    "text": "Color Palettes and Helpers\nWe define fixed colors for specific labels, as well as group-based color assignments.\n\n\nCode\nSINGLE_COLOR_PALETTE = sns.color_palette(\"tab10\", 11)\n\nFIXED_LABEL_COLORS = {\n    \"SR\": SINGLE_COLOR_PALETTE[0],     # Blue\n    \"AFIB\": SINGLE_COLOR_PALETTE[1],   # Orange\n    \"SB\": SINGLE_COLOR_PALETTE[2],     # Green\n    \"GSVT\": SINGLE_COLOR_PALETTE[3],   # Red\n    \"PACE\": SINGLE_COLOR_PALETTE[4],   # Purple\n}\n\nCOLOR_PALETTES = {\n    \"Rhythm\": SINGLE_COLOR_PALETTE,\n    \"Morphology\": SINGLE_COLOR_PALETTE,\n    \"Duration\": SINGLE_COLOR_PALETTE,\n    \"Amplitude\": SINGLE_COLOR_PALETTE,\n    \"Other\": SINGLE_COLOR_PALETTE,\n}\n\ndef get_group_color_map(df_labels):\n    \"\"\"\n    Generate a dict: group_label_map[group][label] -&gt; color.\n    df_labels must have 'integration_name' and 'group'.\n    \"\"\"\n    # Get all unique labels\n    all_labels = df_labels[\"integration_name\"].unique()\n\n    # Map each unique label to a color\n    label_to_color = {}\n    \n    # First, use fixed colors for specific labels\n    for label in all_labels:\n        if label in FIXED_LABEL_COLORS:\n            label_to_color[label] = FIXED_LABEL_COLORS[label]\n    \n    # Then assign colors to remaining labels\n    color_idx = 0\n    for label in all_labels:\n        if label not in label_to_color:\n            # Skip any colors used in FIXED_LABEL_COLORS\n            while (color_idx &lt; len(SINGLE_COLOR_PALETTE) and \n                   any(SINGLE_COLOR_PALETTE[color_idx] == c \n                       for c in FIXED_LABEL_COLORS.values())):\n                color_idx += 1\n            \n            if color_idx &lt; len(SINGLE_COLOR_PALETTE):\n                label_to_color[label] = SINGLE_COLOR_PALETTE[color_idx]\n                color_idx += 1\n            else:\n                # fallback color\n                label_to_color[label] = (0.5, 0.5, 0.5)\n    \n    # Create the group structure\n    group_label_map = {}\n    for _, row in df_labels.iterrows():\n        label = row[\"integration_name\"]\n        group = row[\"group\"]\n        if group not in group_label_map:\n            group_label_map[group] = {}\n        if label not in group_label_map[group]:\n            group_label_map[group][label] = label_to_color[label]\n\n    return group_label_map\n\n# Distinct markers for each group\nGROUP_MARKERS = {\n    \"Rhythm\": \"o\",\n    \"Morphology\": \"s\",\n    \"Duration\": \"^\",\n    \"Amplitude\": \"D\",\n    \"Other\": \"X\",\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)</span>"
    ]
  },
  {
    "objectID": "phase1.html#data-loading",
    "href": "phase1.html#data-loading",
    "title": "4  Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)",
    "section": "Data Loading",
    "text": "Data Loading\nHere, we load the Arrhythmia (Chapman) test set from our UnifiedDataset.\nWe then retrieve embeddings for Baseline and Fine-Tuned models.\n\n\nCode\nprint(\"Phase 1: UMAP on FULL data, highlighting single-labeled records.\")\nprint(\"=\" * 70)\n\narr_data = UnifiedDataset(\n    Path(project_root) / \"data\", modality=DatasetModality.ECG, dataset_key=\"arrhythmia\"\n)\narr_splits = arr_data.get_splits()\narr_test_ids = arr_splits.get(\"test\", [])\n\narr_md_store = arr_data.metadata_store\n\npretrained_embedding = \"baseline\"\nfinetuned_embedding = \"fine_tuned_50\"\n\nrecords_info = []\nemb_base_list = []\nemb_ft_list = []\n\nfor rid in arr_test_ids:\n    meta = arr_md_store.get(rid, {})\n    labels_meta = meta.get(\"labels_metadata\", [])\n\n    try:\n        emb_base = arr_data.get_embeddings(rid, embeddings_type=pretrained_embedding)\n        emb_ft = arr_data.get_embeddings(rid, embeddings_type=finetuned_embedding)\n    except Exception as e:\n        print(f\"Skipping {rid} (missing embeddings). Err: {e}\")\n        continue\n\n    records_info.append(\n        {\"record_id\": rid, \"labels_meta\": labels_meta, \"n_labels\": len(labels_meta)}\n    )\n    emb_base_list.append(emb_base)\n    emb_ft_list.append(emb_ft)\n\nif not records_info:\n    print(\"No records found. Exiting.\")\n    sys.exit()\n\ndf_records = pd.DataFrame(records_info)\ndf_records[\"row_idx\"] = df_records.index  # 0..N-1\n\n# Stack embeddings\nbaseline_embeddings = np.vstack(emb_base_list)\nfinetuned_embeddings = np.vstack(emb_ft_list)\n\nprint(f\"Total records loaded: {len(df_records)}\")\nprint(\" - Baseline shape:\", baseline_embeddings.shape)\nprint(\" - Fine-tuned shape:\", finetuned_embeddings.shape)\n\n\nPhase 1: UMAP on FULL data, highlighting single-labeled records.\n======================================================================\n\n\nTotal records loaded: 6723\n - Baseline shape: (6723, 384)\n - Fine-tuned shape: (6723, 384)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)</span>"
    ]
  },
  {
    "objectID": "phase1.html#single-labeled-subset",
    "href": "phase1.html#single-labeled-subset",
    "title": "4  Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)",
    "section": "Single-Labeled Subset",
    "text": "Single-Labeled Subset\nWe now identify records that have exactly one label and store this subset separately.\n\n\nCode\nmask_single = df_records[\"n_labels\"] == 1\ndf_single = df_records[mask_single].copy()\n\ndf_single[\"integration_name\"] = df_single[\"labels_meta\"].apply(\n    lambda lm: lm[0].get(\"integration_name\", \"unknown\") if len(lm) == 1 else \"unknown\"\n)\ndf_single[\"group\"] = df_single[\"labels_meta\"].apply(\n    lambda lm: lm[0].get(\"group\", \"Other\") if len(lm) == 1 else \"Other\"\n)\n\nprint(\"Single-labeled records:\", len(df_single))\n\n\nSingle-labeled records: 3503",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)</span>"
    ]
  },
  {
    "objectID": "phase1.html#umap-on-the-full-dataset",
    "href": "phase1.html#umap-on-the-full-dataset",
    "title": "4  Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)",
    "section": "UMAP on the Full Dataset",
    "text": "UMAP on the Full Dataset\nWe run UMAP on all records (both single- and multi-labeled) for a global view, then highlight single-labeled in the plot.\n\n\nCode\nprint(\"\\nRunning UMAP (baseline & fine-tuned) on all records...\")\n\numap_params = dict(n_neighbors=15, n_components=2, metric=\"euclidean\", random_state=42)\nbaseline_umap = run_umap(baseline_embeddings, **umap_params)\nfinetuned_umap = run_umap(finetuned_embeddings, **umap_params)\n\nprint(\"UMAP finished.\\n\")\n\n# Prepare color mapping for single-labeled points\nsingle_labels = df_single[[\"integration_name\", \"group\"]].drop_duplicates()\ngroup_color_mapping = get_group_color_map(single_labels)\n\n\n\nRunning UMAP (baseline & fine-tuned) on all records...\n\n\nUMAP finished.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)</span>"
    ]
  },
  {
    "objectID": "phase1.html#visualization",
    "href": "phase1.html#visualization",
    "title": "4  Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)",
    "section": "Visualization",
    "text": "Visualization\nWe create a two-panel figure comparing the Baseline vs. Fine-tuned spaces,\nusing gray for all records and colorful markers for single-labeled examples.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nfig.suptitle(\n    \"Chapman ECG Embedding Visualization: Baseline vs. Fine-Tuned Model\",\n    fontsize=18,\n    fontweight=\"bold\",\n)\n\ndef plot_embedding(ax, emb_2d, title):\n    ax.set_title(title, fontsize=14)\n\n    # (1) All points in light gray\n    ax.scatter(\n        emb_2d[:, 0],\n        emb_2d[:, 1],\n        color=\"lightgray\",\n        edgecolor=\"none\",\n        s=40,\n        alpha=0.6,\n        label=\"All Records\",\n    )\n\n    # (2) Overlay single-labeled points\n    for row in df_single.itertuples():\n        row_idx = row.row_idx\n        label_name = getattr(row, \"integration_name\")\n        group_name = getattr(row, \"group\")\n\n        color = group_color_mapping[group_name].get(label_name, (0.5, 0.5, 0.5))\n        marker = GROUP_MARKERS.get(group_name, \"o\")\n\n        ax.scatter(\n            emb_2d[row_idx, 0],\n            emb_2d[row_idx, 1],\n            c=[color],\n            marker=marker,\n            s=80,\n            alpha=0.9,\n            edgecolors=\"white\",\n            linewidth=0.5,\n        )\n\n    ax.set_xlabel(\"UMAP Dim 1\", fontsize=12)\n    ax.set_ylabel(\"UMAP Dim 2\", fontsize=12)\n    ax.grid(True, linestyle=\"--\", alpha=0.5)\n\nplot_embedding(axes[0], baseline_umap, \"Baseline (Pre-trained) Model\")\nplot_embedding(axes[1], finetuned_umap, \"Fine-tuned (Chapman) Model\")\n\n# Build legend\nhandles = []\n\n# Handle for \"All Records\"\nhandles.append(\n    Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"lightgray\",\n        label=\"All Records\",\n        markersize=10,\n        markeredgecolor=\"none\",\n        linewidth=0,\n    )\n)\n\n# Group header + individual labels\nunique_groups = single_labels[\"group\"].unique()\nfor grp in unique_groups:\n    handles.append(Patch(color=\"none\", label=f\"\\n{grp} Group:\"))\n    grp_labels = single_labels[single_labels[\"group\"] == grp][\"integration_name\"].unique()\n    mkr = GROUP_MARKERS.get(grp, \"o\")\n    for lbl in grp_labels:\n        c = group_color_mapping[grp].get(lbl, (0.5, 0.5, 0.5))\n        handles.append(\n            Line2D(\n                [0],\n                [0],\n                marker=mkr,\n                color=\"w\",\n                markerfacecolor=c,\n                markersize=10,\n                label=f\"  {lbl}\",\n                linewidth=0,\n            )\n        )\n\nlegend = fig.legend(\n    handles=handles,\n    loc=\"center right\",\n    bbox_to_anchor=(1.05, 0.5),\n    fontsize=11,\n    frameon=True,\n    fancybox=True,\n    framealpha=0.95,\n    title=\"Single-Labeled\\nGroups & Labels\",\n    title_fontsize=12,\n)\n\nplt.savefig(\"baseline_vs_finetuned_embedding_visualization.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation 1: Embedding Space Analysis (Pre-trained vs Fine-tuned)</span>"
    ]
  },
  {
    "objectID": "phase2.html",
    "href": "phase2.html",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "",
    "text": "Imports\nCode\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\nfrom collections import defaultdict\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Ensure project path is in sys.path\nproject_root = Path().absolute().parent.parent\nsys.path.append(str(project_root))\n\nfrom src.visualization.embedding_viz import run_umap\nfrom src.data.unified import UnifiedDataset\nfrom src.data.dataset import DatasetModality\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (16, 8)\nplt.rcParams[\"font.size\"] = 12",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#color-palettes-and-helpers",
    "href": "phase2.html#color-palettes-and-helpers",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Color Palettes and Helpers",
    "text": "Color Palettes and Helpers\n\n\nCode\nSINGLE_COLOR_PALETTE = sns.color_palette(\"tab10\", 11)\nDUAL_COLOR_PALETTE = sns.color_palette(\"husl\", 20)\n\nFIXED_LABEL_COLORS = {\n    \"SR\": SINGLE_COLOR_PALETTE[0],\n    \"AFIB\": SINGLE_COLOR_PALETTE[1],\n    \"SB\": SINGLE_COLOR_PALETTE[2],\n    \"GSVT\": SINGLE_COLOR_PALETTE[3],\n    \"PACE\": SINGLE_COLOR_PALETTE[4],\n}\n\nFIXED_SECONDARY_COLORS = {\n    \"STACH\": (0.85, 0.37, 0.01),\n    \"SBRAD\": (0.01, 0.66, 0.62),\n    \"SARRH\": (0.58, 0.40, 0.74),\n    \"BIGU\":  (0.17, 0.63, 0.17),\n    \"IVCD\":  (0.84, 0.15, 0.16),\n    \"LAD\":   (0.55, 0.35, 0.35),\n    \"RAD\":   (0.94, 0.50, 0.50),\n    \"LVH\":   (0.12, 0.47, 0.71),\n    \"RVH\":   (0.68, 0.78, 0.91),\n    \"LNGQT\": (0.46, 0.77, 0.35),\n}\n\nCOLOR_PALETTES = {\n    \"Rhythm\": SINGLE_COLOR_PALETTE,\n    \"Morphology\": SINGLE_COLOR_PALETTE,\n    \"Duration\": SINGLE_COLOR_PALETTE,\n    \"Amplitude\": SINGLE_COLOR_PALETTE,\n    \"Other\": SINGLE_COLOR_PALETTE,\n}\n\nGROUP_MARKERS = {\n    \"Rhythm\": \"o\",\n    \"Morphology\": \"s\",\n    \"Duration\": \"^\",\n    \"Amplitude\": \"D\",\n    \"Other\": \"X\",\n}\n\nLABELS_OF_INTEREST = [\"SR\", \"AFIB\", \"SB\", \"GSVT\", \"PACE\"]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#downsampling-outlier-removal-and-color-mapping-utilities",
    "href": "phase2.html#downsampling-outlier-removal-and-color-mapping-utilities",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Downsampling, Outlier Removal, and Color-Mapping Utilities",
    "text": "Downsampling, Outlier Removal, and Color-Mapping Utilities\n\n\nCode\ndef downsample_points(points, max_points=20, min_cluster_size=3, method=\"kmeans\"):\n    \"\"\"\n    Intelligently downsample points while preserving cluster structure.\n    \"\"\"\n    if len(points) &lt;= max_points:\n        return points\n    if method == \"random\":\n        indices = np.random.choice(len(points), size=max_points, replace=False)\n        return points[indices]\n    elif method == \"kmeans\":\n        n_clusters = min(max(min_cluster_size, len(points) // 5), max_points)\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        cluster_labels = kmeans.fit_predict(points)\n        sampled_points = []\n        for cluster_id in range(n_clusters):\n            cluster_points = points[cluster_labels == cluster_id]\n            if len(cluster_points) &lt; 3:\n                continue\n            n_samples = max(1, int(max_points * len(cluster_points) / len(points)))\n            idx = np.random.choice(len(cluster_points), size=min(n_samples, len(cluster_points)), replace=False)\n            sampled_points.append(cluster_points[idx])\n        if sampled_points:\n            return np.vstack(sampled_points)\n        else:\n            return downsample_points(points, max_points, min_cluster_size, method=\"random\")\n    elif method == \"dbscan\":\n        dbscan = DBSCAN(eps=0.5, min_samples=min_cluster_size)\n        cluster_labels = dbscan.fit_predict(points)\n        sampled_points = []\n        noise_points = points[cluster_labels == -1]\n        if len(noise_points) &gt; 0:\n            n_noise_samples = min(max_points // 4, len(noise_points))\n            if n_noise_samples &gt; 0:\n                idx = np.random.choice(len(noise_points), size=n_noise_samples, replace=False)\n                sampled_points.append(noise_points[idx])\n        unique_clusters = np.unique(cluster_labels)\n        unique_clusters = unique_clusters[unique_clusters &gt;= 0]\n        for cluster_id in unique_clusters:\n            cluster_points = points[cluster_labels == cluster_id]\n            if len(cluster_points) &lt; 3:\n                continue\n            n_samples = max(min_cluster_size, int(max_points * len(cluster_points) / len(points)))\n            idx = np.random.choice(len(cluster_points), size=min(n_samples, len(cluster_points)), replace=False)\n            sampled_points.append(cluster_points[idx])\n        if sampled_points:\n            return np.vstack(sampled_points)\n        else:\n            return downsample_points(points, max_points, min_cluster_size, method=\"random\")\n    else:\n        return downsample_points(points, max_points, min_cluster_size, method=\"random\")\n\ndef remove_outliers_2d(points, z_thresh=3.0):\n    \"\"\"\n    Removes outliers in 2D by z-scoring each dimension\n    and discarding points beyond z_thresh in Euclidean distance.\n    \"\"\"\n    if len(points) &lt; 5:\n        return points\n    z_scores = np.abs((points - np.mean(points, axis=0)) / np.std(points, axis=0))\n    z_dist = np.sqrt(np.sum(z_scores**2, axis=1))\n    mask = z_dist &lt; z_thresh\n    return points[mask]\n\ndef get_group_color_map(df_labels):\n    \"\"\"\n    Generate a dict: group_label_map[group][label] -&gt; color.\n    df_labels must have 'integration_name' and 'group'.\n    \"\"\"\n    all_labels = df_labels[\"integration_name\"].unique()\n    label_to_color = {}\n    for label in all_labels:\n        if label in FIXED_LABEL_COLORS:\n            label_to_color[label] = FIXED_LABEL_COLORS[label]\n    color_idx = 0\n    for label in all_labels:\n        if label not in label_to_color:\n            while color_idx &lt; len(SINGLE_COLOR_PALETTE) and any(SINGLE_COLOR_PALETTE[color_idx] == c for c in FIXED_LABEL_COLORS.values()):\n                color_idx += 1\n            if color_idx &lt; len(SINGLE_COLOR_PALETTE):\n                label_to_color[label] = SINGLE_COLOR_PALETTE[color_idx]\n                color_idx += 1\n            else:\n                label_to_color[label] = (0.5, 0.5, 0.5)\n    group_label_map = {}\n    for _, row in df_labels.iterrows():\n        label = row[\"integration_name\"]\n        group = row[\"group\"]\n        if group not in group_label_map:\n            group_label_map[group] = {}\n        if label not in group_label_map[group]:\n            group_label_map[group][label] = label_to_color[label]\n    return group_label_map",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#data-loading-metadata-extraction",
    "href": "phase2.html#data-loading-metadata-extraction",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Data Loading & Metadata Extraction",
    "text": "Data Loading & Metadata Extraction\n\n\nCode\nprint(\"Phase 2: Analyzing subclustering within single-label groups.\")\narr_data = UnifiedDataset(Path(project_root) / \"data\", modality=DatasetModality.ECG, dataset_key=\"arrhythmia\")\narr_splits = arr_data.get_splits()\narr_test_ids = arr_splits.get(\"test\", [])\narr_md_store = arr_data.metadata_store\n\n# Extract demographic metadata for all test records\ndemographic_data = {}\nfor rid in arr_test_ids:\n    meta = arr_md_store.get(rid, {})\n    age = meta.get(\"age\", None)\n    is_male = meta.get(\"is_male\", None)\n    demographic_data[rid] = {\n        \"age\": age if isinstance(age, (int, float)) else None,\n        \"is_male\": is_male if isinstance(is_male, bool) else None\n    }\nprint(f\"Extracted demographic data for {len(demographic_data)} records\")\n\npretrained_embedding = \"baseline\"\nfinetuned_embedding = \"fine_tuned_50\"\n\ndef extract_extended_records_info(arr_test_ids, arr_md_store):\n    records_info = []\n    emb_base_list = []\n    emb_ft_list = []\n    for rid in arr_test_ids:\n        meta = arr_md_store.get(rid, {})\n        labels_meta = meta.get(\"labels_metadata\", [])\n        age = meta.get(\"age\", None)\n        is_male = meta.get(\"is_male\", None)\n        age = age if isinstance(age, (int, float)) else None\n        is_male = is_male if isinstance(is_male, bool) else None\n        try:\n            emb_base = arr_data.get_embeddings(rid, embeddings_type=pretrained_embedding)\n            emb_ft = arr_data.get_embeddings(rid, embeddings_type=finetuned_embedding)\n        except Exception as e:\n            print(f\"Skipping {rid} (missing embeddings). Err: {e}\")\n            continue\n        records_info.append({\n            \"record_id\": rid,\n            \"labels_meta\": labels_meta,\n            \"n_labels\": len(labels_meta),\n            \"age\": age,\n            \"is_male\": is_male\n        })\n        emb_base_list.append(emb_base)\n        emb_ft_list.append(emb_ft)\n    return records_info, emb_base_list, emb_ft_list\n\nrecords_info, emb_base_list, emb_ft_list = extract_extended_records_info(arr_test_ids, arr_md_store)\nif not records_info:\n    print(\"No records found. Exiting.\")\n    sys.exit()\n\ndf_records = pd.DataFrame(records_info)\ndf_records[\"row_idx\"] = df_records.index\nbaseline_embeddings = np.vstack(emb_base_list)\nfinetuned_embeddings = np.vstack(emb_ft_list)\n\nprint(f\"Total records loaded: {len(df_records)}\")\nprint(\" - Baseline shape:\", baseline_embeddings.shape)\nprint(\" - Fine-tuned shape:\", finetuned_embeddings.shape)\nprint(f\" - With age data: {df_records['age'].notna().sum()}\")\nprint(f\" - With sex data: {df_records['is_male'].notna().sum()}\")\n\n\nPhase 2: Analyzing subclustering within single-label groups.\nExtracted demographic data for 6723 records\n\n\nTotal records loaded: 6723\n - Baseline shape: (6723, 384)\n - Fine-tuned shape: (6723, 384)\n - With age data: 6720\n - With sex data: 6719",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#single-labeled-and-dual-labeled-records",
    "href": "phase2.html#single-labeled-and-dual-labeled-records",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Single-Labeled and Dual-Labeled Records",
    "text": "Single-Labeled and Dual-Labeled Records\n\n\nCode\nmask_single = df_records[\"n_labels\"] == 1\ndf_single = df_records[mask_single].copy()\n\ndf_single[\"integration_name\"] = df_single[\"labels_meta\"].apply(\n    lambda lm: lm[0].get(\"integration_name\", \"unknown\") if len(lm) == 1 else \"unknown\"\n)\ndf_single[\"group\"] = df_single[\"labels_meta\"].apply(\n    lambda lm: lm[0].get(\"group\", \"Other\") if len(lm) == 1 else \"Other\"\n)\nprint(\"Single-labeled records:\", len(df_single))\n\nmask_dual = df_records[\"n_labels\"] == 2\ndf_dual = df_records[mask_dual].copy()\ndual_label_info = []\n\nfor _, row in df_dual.iterrows():\n    labels_meta = row[\"labels_meta\"]\n    if len(labels_meta) != 2:\n        continue\n    label1 = labels_meta[0].get(\"integration_name\", \"unknown\")\n    label2 = labels_meta[1].get(\"integration_name\", \"unknown\")\n    group1 = labels_meta[0].get(\"group\", \"Other\")\n    group2 = labels_meta[1].get(\"group\", \"Other\")\n    \n    primary_label = None\n    secondary_label = None\n    primary_group = None\n    secondary_group = None\n    \n    if label1 in LABELS_OF_INTEREST:\n        primary_label = label1\n        secondary_label = label2\n        primary_group = group1\n        secondary_group = group2\n    elif label2 in LABELS_OF_INTEREST:\n        primary_label = label2\n        secondary_label = label1\n        primary_group = group2\n        secondary_group = group1\n    if primary_label:\n        dual_label_info.append({\n            \"record_id\": row[\"record_id\"],\n            \"row_idx\": row[\"row_idx\"],\n            \"primary_label\": primary_label,\n            \"primary_group\": primary_group,\n            \"secondary_label\": secondary_label,\n            \"secondary_group\": secondary_group,\n            \"combo_label\": f\"{primary_label}+{secondary_label}\"\n        })\n\ndf_dual_filtered = pd.DataFrame(dual_label_info)\nprint(f\"Dual-labeled records with one label in {LABELS_OF_INTEREST}:\", len(df_dual_filtered))\n\n\nSingle-labeled records: 3503\nDual-labeled records with one label in ['SR', 'AFIB', 'SB', 'GSVT', 'PACE']: 1860",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#umap-on-all-records",
    "href": "phase2.html#umap-on-all-records",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "UMAP on All Records",
    "text": "UMAP on All Records\n\n\nCode\nprint(\"\\nRunning UMAP (baseline & fine-tuned) on all records...\")\numap_params = dict(n_neighbors=15, n_components=2, metric=\"euclidean\", random_state=42)\nbaseline_umap = run_umap(baseline_embeddings, **umap_params)\nfinetuned_umap = run_umap(finetuned_embeddings, **umap_params)\nprint(\"UMAP finished.\\n\")\n\nsingle_labels = df_single[[\"integration_name\", \"group\"]].drop_duplicates()\ngroup_color_mapping = get_group_color_map(single_labels)\n\ndef get_global_secondary_color_map(df_dual):\n    all_secondary_labels = sorted(df_dual[\"secondary_label\"].unique())\n    secondary_color_map = {}\n    for label in all_secondary_labels:\n        if label in FIXED_SECONDARY_COLORS:\n            secondary_color_map[label] = FIXED_SECONDARY_COLORS[label]\n    remaining_labels = [l for l in all_secondary_labels if l not in secondary_color_map]\n    remaining_colors = DUAL_COLOR_PALETTE[:len(remaining_labels)]\n    for i, label in enumerate(remaining_labels):\n        secondary_color_map[label] = remaining_colors[i]\n    return secondary_color_map\n\nglobal_secondary_color_map = get_global_secondary_color_map(df_dual_filtered)\nprimary_to_secondary = defaultdict(list)\nfor _, row in df_dual_filtered.iterrows():\n    primary_to_secondary[row[\"primary_label\"]].append(row[\"secondary_label\"])\nfor primary in primary_to_secondary:\n    primary_to_secondary[primary] = sorted(set(primary_to_secondary[primary]))\n\n\n\nRunning UMAP (baseline & fine-tuned) on all records...\n\n\nUMAP finished.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#subcluster-plot-for-each-primary-label",
    "href": "phase2.html#subcluster-plot-for-each-primary-label",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Subcluster Plot for Each Primary Label",
    "text": "Subcluster Plot for Each Primary Label\n\n\nCode\ndef plot_subclusters(ax, emb_2d, primary_label, title):\n    ax.set_title(f\"Label: {primary_label}\", fontsize=14, fontweight='bold')\n    primary_group = df_single[df_single[\"integration_name\"] == primary_label][\"group\"].iloc[0]\n    primary_color = group_color_mapping[primary_group][primary_label]\n    single_mask = df_single[\"integration_name\"] == primary_label\n    single_rows = df_single[single_mask]\n    if len(single_rows) == 0:\n        ax.text(0.5, 0.5, f\"No records with label {primary_label}\",\n                ha='center', va='center', transform=ax.transAxes)\n        return\n    primary_points = emb_2d[single_rows[\"row_idx\"].values]\n    clean_primary_points = remove_outliers_2d(primary_points, z_thresh=3.0)\n    if len(clean_primary_points) &gt;= 5:\n        primary_df = pd.DataFrame(clean_primary_points, columns=[\"umap_x\", \"umap_y\"])\n        sns.kdeplot(\n            data=primary_df,\n            x=\"umap_x\",\n            y=\"umap_y\",\n            fill=True,\n            levels=4,\n            alpha=0.25,\n            color=primary_color,\n            ax=ax,\n            label=None\n        )\n        ax.scatter(\n            primary_df[\"umap_x\"],\n            primary_df[\"umap_y\"],\n            c=[primary_color],\n            marker=GROUP_MARKERS.get(primary_group, \"o\"),\n            s=40,\n            alpha=0.6,\n            edgecolors=\"white\",\n            linewidth=0.3,\n            label=f\"{primary_label}\"\n        )\n    dual_mask = df_dual_filtered[\"primary_label\"] == primary_label\n    if dual_mask.any():\n        dual_rows = df_dual_filtered[dual_mask]\n        secondary_labels = sorted(dual_rows[\"secondary_label\"].unique())\n        for secondary_label in secondary_labels:\n            secondary_mask = dual_rows[\"secondary_label\"] == secondary_label\n            if not secondary_mask.any():\n                continue\n            secondary_points = emb_2d[dual_rows[secondary_mask][\"row_idx\"].values]\n            if len(secondary_points) &gt;= 3:\n                max_points_per_secondary = 25\n                secondary_points_sampled = downsample_points(\n                    secondary_points,\n                    max_points=max_points_per_secondary,\n                    min_cluster_size=3,\n                    method=\"kmeans\"\n                )\n                secondary_color = global_secondary_color_map.get(secondary_label, (0.5, 0.5, 0.5))\n                ax.scatter(\n                    secondary_points_sampled[:, 0],\n                    secondary_points_sampled[:, 1],\n                    c=[secondary_color],\n                    marker=\"X\",\n                    s=80,\n                    alpha=0.9,\n                    edgecolors=\"black\",\n                    linewidth=0.5,\n                    label=f\"+{secondary_label} (n={len(secondary_points)})\"\n                )\n    ax.set_xlabel(\"UMAP Dim 1\", fontsize=12)\n    ax.set_ylabel(\"UMAP Dim 2\", fontsize=12)\n    ax.grid(True, linestyle=\"--\", alpha=0.3)\n    handles, labels = ax.get_legend_handles_labels()\n    if handles:\n        ax.legend(handles=handles, labels=labels, loc=\"upper right\", fontsize=9, framealpha=0.7)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#per-label-subcluster-plots-for-baseline-and-fine-tuned",
    "href": "phase2.html#per-label-subcluster-plots-for-baseline-and-fine-tuned",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Per-Label Subcluster Plots for Baseline and Fine-Tuned",
    "text": "Per-Label Subcluster Plots for Baseline and Fine-Tuned\n\n\nCode\nfor embedding_name, embedding_data in [(\"Baseline\", baseline_umap), (\"Fine-tuned\", finetuned_umap)]:\n    n_labels = len(LABELS_OF_INTEREST)\n    n_cols = 3\n    n_rows = (n_labels + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6*n_rows))\n    fig.suptitle(f\"{embedding_name} Embeddings: Subcluster Analysis\", fontsize=18, fontweight=\"bold\", y=0.98)\n    axes = axes.flatten() if n_rows &gt; 1 else [axes] if n_cols == 1 else axes\n    for i, label in enumerate(LABELS_OF_INTEREST):\n        if i &lt; len(axes):\n            plot_subclusters(axes[i], embedding_data, label, embedding_name)\n    for j in range(n_labels, len(axes)):\n        axes[j].set_visible(False)\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n    plt.close()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#extended-faceted-subcluster-plots",
    "href": "phase2.html#extended-faceted-subcluster-plots",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Extended Faceted Subcluster Plots",
    "text": "Extended Faceted Subcluster Plots\n\n\nCode\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef create_faceted_subcluster_plots(embedding_data, title, figsize=(18, 15)):\n    n_labels = len(LABELS_OF_INTEREST)\n    n_cols = 3\n    n_rows = (n_labels + n_cols - 1) // n_cols\n    fig = plt.figure(figsize=figsize)\n    gs = fig.add_gridspec(n_rows, n_cols, hspace=0.3, wspace=0.3, height_ratios=[1]*n_rows)\n    fig.suptitle(f\"{title}\", fontsize=20, fontweight='bold', y=0.98)\n    axes = []\n    for i in range(n_rows):\n        for j in range(n_cols):\n            idx = i * n_cols + j\n            if idx &lt; n_labels:\n                axes.append(fig.add_subplot(gs[i, j]))\n            else:\n                ax = fig.add_subplot(gs[i, j])\n                ax.set_visible(False)\n                axes.append(ax)\n    for i, primary_label in enumerate(LABELS_OF_INTEREST):\n        if i &gt;= len(axes):\n            break\n        ax = axes[i]\n        primary_group = df_single[df_single[\"integration_name\"] == primary_label][\"group\"].iloc[0]\n        primary_color = group_color_mapping[primary_group][primary_label]\n        single_mask = df_single[\"integration_name\"] == primary_label\n        single_rows = df_single[single_mask]\n        if len(single_rows) &lt; 5:\n            ax.text(0.5, 0.5, f\"Not enough data for {primary_label}\",\n                    ha='center', va='center', transform=ax.transAxes)\n            continue\n        primary_points = embedding_data[single_rows[\"row_idx\"].values]\n        clean_primary_points = remove_outliers_2d(primary_points, z_thresh=3.0)\n        primary_df = pd.DataFrame(clean_primary_points, columns=[\"umap_x\", \"umap_y\"])\n        primary_df[\"record_id\"] = single_rows[\"record_id\"].values[:len(primary_df)]\n        primary_df[\"age\"] = primary_df[\"record_id\"].apply(lambda rid: demographic_data.get(rid, {}).get(\"age\", None))\n        primary_df[\"is_male\"] = primary_df[\"record_id\"].apply(lambda rid: demographic_data.get(rid, {}).get(\"is_male\", None))\n        valid_ages = primary_df[\"age\"].dropna()\n        age_stats = f\"Age: {valid_ages.mean():.1f}±{valid_ages.std():.1f}\" if len(valid_ages) &gt; 0 else \"Age: N/A\"\n        male_count = primary_df[\"is_male\"].sum()\n        female_count = (primary_df[\"is_male\"] == False).sum()\n        unknown_sex = primary_df[\"is_male\"].isna().sum()\n        sex_stats = f\"Sex: {male_count}M/{female_count}F\"\n        if unknown_sex &gt; 0:\n            sex_stats += f\" ({unknown_sex} unknown)\"\n        if \"is_male\" in primary_df.columns and not primary_df[\"is_male\"].isna().all():\n            male_df = primary_df[primary_df[\"is_male\"] == True]\n            if len(male_df) &gt; 0:\n                sns.scatterplot(\n                    data=male_df, x=\"umap_x\", y=\"umap_y\",\n                    color=primary_color, marker=\"o\", s=40, alpha=0.7, ax=ax, label=f\"{primary_label} (M)\"\n                )\n            female_df = primary_df[primary_df[\"is_male\"] == False]\n            if len(female_df) &gt; 0:\n                sns.scatterplot(\n                    data=female_df, x=\"umap_x\", y=\"umap_y\",\n                    color=primary_color, marker=\"^\", s=40, alpha=0.7, ax=ax, label=f\"{primary_label} (F)\"\n                )\n            unknown_df = primary_df[primary_df[\"is_male\"].isna()]\n            if len(unknown_df) &gt; 0:\n                sns.scatterplot(\n                    data=unknown_df, x=\"umap_x\", y=\"umap_y\",\n                    color=primary_color, marker=\"s\", s=40, alpha=0.5, ax=ax, label=f\"{primary_label} (?)\"\n                )\n        else:\n            sns.scatterplot(\n                data=primary_df, x=\"umap_x\", y=\"umap_y\",\n                color=primary_color, s=40, alpha=0.7, ax=ax, label=primary_label\n            )\n        if len(primary_df) &gt;= 5:\n            sns.kdeplot(\n                data=primary_df, x=\"umap_x\", y=\"umap_y\",\n                fill=True, levels=4, alpha=0.25, color=primary_color,\n                bw_adjust=0.7, ax=ax\n            )\n        dual_mask = df_dual_filtered[\"primary_label\"] == primary_label\n        if dual_mask.any():\n            dual_rows = df_dual_filtered[dual_mask]\n            secondary_labels = sorted(dual_rows[\"secondary_label\"].unique())\n            top_secondaries = dual_rows[\"secondary_label\"].value_counts().nlargest(5).index.tolist()\n            for secondary_label in top_secondaries:\n                secondary_mask = dual_rows[\"secondary_label\"] == secondary_label\n                if not secondary_mask.any():\n                    continue\n                secondary_points = embedding_data[dual_rows[secondary_mask][\"row_idx\"].values]\n                if len(secondary_points) &gt;= 3:\n                    secondary_points_sampled = downsample_points(\n                        secondary_points, max_points=25, min_cluster_size=3, method=\"kmeans\"\n                    )\n                    secondary_color = global_secondary_color_map.get(secondary_label, (0.5, 0.5, 0.5))\n                    ax.scatter(\n                        secondary_points_sampled[:, 0],\n                        secondary_points_sampled[:, 1],\n                        c=[secondary_color], marker=\"X\", s=100,\n                        alpha=0.9, edgecolors=\"black\", linewidth=0.5,\n                        label=f\"+{secondary_label} (n={len(secondary_points)})\"\n                    )\n        ax.legend(fontsize=8, loc='upper right', framealpha=0.7)\n        ax.set_xlabel(\"UMAP Dim 1\")\n        ax.set_ylabel(\"UMAP Dim 2\")\n        ax.set_aspect('auto')\n        ax.set_title(f\"Label: {primary_label}\\n{age_stats}, {sex_stats}\", fontsize=12, pad=12)\n        xlim = ax.get_xlim()\n        ylim = ax.get_ylim()\n        divider = make_axes_locatable(ax)\n        ax_top = divider.append_axes(\"top\", size=\"15%\", pad=0.05)\n        ax_top.tick_params(axis='both', which='both', labelbottom=False, labelleft=False)\n        if len(primary_df) &gt;= 5:\n            sns.kdeplot(data=primary_df, x=\"umap_x\", color=primary_color, bw_adjust=0.7, ax=ax_top)\n            ax_top.set_xlim(xlim)\n        ax_top.set_yticks([])\n        ax_right = divider.append_axes(\"right\", size=\"15%\", pad=0.05)\n        ax_right.tick_params(axis='both', which='both', labelbottom=False, labelleft=False)\n        if len(primary_df) &gt;= 5:\n            sns.kdeplot(data=primary_df, y=\"umap_y\", color=primary_color, bw_adjust=0.7, ax=ax_right)\n            ax_right.set_ylim(ylim)\n        ax_right.set_xticks([])\n        valid_ages = primary_df[\"age\"].dropna()\n        if valid_ages.size &gt; 10:\n            ax_age = divider.append_axes(\"bottom\", size=\"15%\", pad=0.5)\n            sns.histplot(valid_ages, bins=10, color=primary_color, alpha=0.6, ax=ax_age)\n            ax_age.set_xlabel(\"Age\")\n            ax_age.set_ylabel(\"Count\")\n            ax_age.tick_params(labelsize=8)\n    plt.tight_layout(rect=[0, 0, 1, 0.95], h_pad=0.2, w_pad=0.2)\n    return fig\n\ncreate_faceted_subcluster_plots(baseline_umap, \"Baseline (Pre-trained) Embeddings - Subcluster Analysis\")\nplt.show()\nplt.close()\n\ncreate_faceted_subcluster_plots(finetuned_umap, \"Fine-tuned (Chapman) Embeddings - Subcluster Analysis\")\nplt.show()\nplt.close()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#demographic-subcluster-plots",
    "href": "phase2.html#demographic-subcluster-plots",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Demographic Subcluster Plots",
    "text": "Demographic Subcluster Plots\n\n\nCode\ndef create_demographic_subcluster_plots(embedding_data, title, figsize=(18, 24)):\n    n_labels = len(LABELS_OF_INTEREST)\n    n_cols = 3\n    n_rows = 2 * ((n_labels + n_cols - 1) // n_cols)\n    fig = plt.figure(figsize=figsize)\n    gs = fig.add_gridspec(n_rows, n_cols, hspace=0.3, wspace=0.3)\n    fig.suptitle(f\"{title} - Demographic Analysis\", fontsize=20, fontweight='bold', y=0.98)\n    axes = []\n    for i in range(n_rows):\n        for j in range(n_cols):\n            idx = i * n_cols + j\n            if idx &lt; 2 * n_labels:\n                axes.append(fig.add_subplot(gs[i, j]))\n            else:\n                ax = fig.add_subplot(gs[i, j])\n                ax.set_visible(False)\n                axes.append(ax)\n    for i, primary_label in enumerate(LABELS_OF_INTEREST):\n        if 2*i &gt;= len(axes):\n            break\n        ax_sex = axes[2*i]\n        ax_age = axes[2*i+1]\n        ax_sex.set_title(f\"{primary_label} - By Sex\", fontsize=12, pad=10)\n        ax_age.set_title(f\"{primary_label} - By Age\", fontsize=12, pad=10)\n        primary_group = df_single[df_single[\"integration_name\"] == primary_label][\"group\"].iloc[0]\n        primary_color = group_color_mapping[primary_group][primary_label]\n        single_mask = df_single[\"integration_name\"] == primary_label\n        single_rows = df_single[single_mask]\n        if len(single_rows) &lt; 5:\n            for ax_demo in [ax_sex, ax_age]:\n                ax_demo.text(0.5, 0.5, f\"Not enough data for {primary_label}\",\n                             ha='center', va='center', transform=ax_demo.transAxes)\n            continue\n        primary_points = embedding_data[single_rows[\"row_idx\"].values]\n        clean_primary_points = remove_outliers_2d(primary_points, z_thresh=3.0)\n        primary_df = pd.DataFrame(clean_primary_points, columns=[\"umap_x\", \"umap_y\"])\n        primary_df[\"record_id\"] = single_rows[\"record_id\"].values[:len(primary_df)]\n        primary_df[\"age\"] = primary_df[\"record_id\"].apply(lambda rid: demographic_data.get(rid, {}).get(\"age\", None))\n        primary_df[\"is_male\"] = primary_df[\"record_id\"].apply(lambda rid: demographic_data.get(rid, {}).get(\"is_male\", None))\n        sex_df = primary_df.dropna(subset=[\"is_male\"]).copy()\n        age_df = primary_df.dropna(subset=[\"age\"]).copy()\n        if not sex_df.empty:\n            sex_df[\"sex_category\"] = sex_df[\"is_male\"].apply(lambda x: \"Male\" if x else \"Female\")\n            sex_palette = {\"Male\": \"skyblue\", \"Female\": \"coral\"}\n            sns.scatterplot(\n                data=sex_df,\n                x=\"umap_x\",\n                y=\"umap_y\",\n                hue=\"sex_category\",\n                palette=sex_palette,\n                s=40, alpha=0.7, ax=ax_sex\n            )\n            for sex_cat in [\"Male\", \"Female\"]:\n                sex_group = sex_df[sex_df[\"sex_category\"] == sex_cat]\n                if len(sex_group) &gt;= 10:\n                    sns.kdeplot(\n                        data=sex_group, x=\"umap_x\", y=\"umap_y\",\n                        levels=3, alpha=0.3, fill=True,\n                        color=sex_palette[sex_cat], ax=ax_sex\n                    )\n            male_count = sex_df[\"is_male\"].sum()\n            female_count = (sex_df[\"is_male\"] == False).sum()\n            sex_stats = f\"Males: {male_count}, Females: {female_count}\"\n            ax_sex.annotate(\n                sex_stats, xy=(0.5, 0.02), xycoords=\"axes fraction\",\n                ha=\"center\", fontsize=10,\n                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n            )\n        else:\n            ax_sex.text(0.5, 0.5, \"No sex data available\",\n                        ha='center', va='center', transform=ax_sex.transAxes)\n        if not age_df.empty:\n            norm = plt.Normalize(age_df[\"age\"].min(), age_df[\"age\"].max())\n            sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n            sm.set_array([])\n            ax_age.scatter(\n                age_df[\"umap_x\"],\n                age_df[\"umap_y\"],\n                c=age_df[\"age\"],\n                cmap=\"viridis\",\n                s=40,\n                alpha=0.7\n            )\n            cbar = plt.colorbar(sm, ax=ax_age)\n            cbar.set_label(\"Age (years)\")\n            if len(age_df) &gt;= 10:\n                sns.kdeplot(\n                    data=age_df, x=\"umap_x\", y=\"umap_y\",\n                    levels=4, alpha=0.2, linewidths=1,\n                    color=\"black\", ax=ax_age\n                )\n            age_stats = f\"{age_df['age'].mean():.1f}±{age_df['age'].std():.1f}\"\n            ax_age.annotate(\n                f\"Age: {age_stats}\", xy=(0.5, 0.02), xycoords=\"axes fraction\",\n                ha=\"center\", fontsize=10,\n                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8)\n            )\n            if len(age_df) &gt;= 20:\n                clustering = DBSCAN(eps=0.5, min_samples=5).fit(age_df[[\"umap_x\", \"umap_y\"]].values)\n                age_df[\"cluster\"] = clustering.labels_\n                if max(clustering.labels_) &gt;= 0:\n                    cluster_ages = age_df.groupby(\"cluster\")[\"age\"].mean().to_dict()\n                    for cluster_id, cluster_age in cluster_ages.items():\n                        if cluster_id &gt;= 0:\n                            cluster_points = age_df[age_df[\"cluster\"] == cluster_id]\n                            center_x = cluster_points[\"umap_x\"].mean()\n                            center_y = cluster_points[\"umap_y\"].mean()\n                            ax_age.annotate(\n                                f\"{cluster_age:.0f} yrs\", xy=(center_x, center_y),\n                                fontsize=9, fontweight=\"bold\",\n                                ha=\"center\", va=\"center\",\n                                bbox=dict(boxstyle=\"circle,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.7)\n                            )\n        else:\n            ax_age.text(0.5, 0.5, \"No age data available\",\n                        ha='center', va='center', transform=ax_age.transAxes)\n        ax_sex.set_xlabel(\"UMAP Dim 1\")\n        ax_sex.set_ylabel(\"UMAP Dim 2\")\n        ax_age.set_xlabel(\"UMAP Dim 1\")\n        ax_age.set_ylabel(\"UMAP Dim 2\")\n        ax_sex.set_aspect('auto')\n        ax_age.set_aspect('auto')\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    return fig\n\nprint(\"Generating demographic plots for baseline model...\")\ndemographic_fig_baseline = create_demographic_subcluster_plots(\n    baseline_umap, \"Baseline Model - Demographic Subclustering Analysis\"\n)\nplt.show()\nplt.close()\n\nprint(\"Generating demographic plots for fine-tuned model...\")\ndemographic_fig_finetuned = create_demographic_subcluster_plots(\n    finetuned_umap, \"Fine-tuned Model - Demographic Subclustering Analysis\"\n)\nplt.show()\nplt.close()\n\n\nGenerating demographic plots for baseline model...\n\n\n\n\n\n\n\n\n\nGenerating demographic plots for fine-tuned model...",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase2.html#statistical-analysis-of-subclusters",
    "href": "phase2.html#statistical-analysis-of-subclusters",
    "title": "5  Evaluation 2: Subcluster Analysis within Single-Label Groups",
    "section": "Statistical Analysis of Subclusters",
    "text": "Statistical Analysis of Subclusters\n\n\nCode\nprint(\"\\nStatistical Analysis of Subclusters:\")\nprint(\"=\" * 50)\nsingle_counts = {}\ndual_counts = {}\ntotal_dual = 0\n\nfor primary_label in LABELS_OF_INTEREST:\n    print(f\"\\nPrimary Label: {primary_label}\")\n    single_count = sum(df_single[\"integration_name\"] == primary_label)\n    single_counts[primary_label] = single_count\n    print(f\" - Single-labeled records: {single_count}\")\n    dual_mask = df_dual_filtered[\"primary_label\"] == primary_label\n    if not dual_mask.any():\n        print(f\" - No dual-labeled records with {primary_label}\")\n        dual_counts[primary_label] = 0\n        continue\n    dual_rows = df_dual_filtered[dual_mask]\n    dual_count = len(dual_rows)\n    dual_counts[primary_label] = dual_count\n    total_dual += dual_count\n    print(f\" - Dual-labeled records: {dual_count}\")\n    secondary_counts = dual_rows[\"secondary_label\"].value_counts()\n    print(\" - Secondary label distribution:\")\n    for label, count in secondary_counts.items():\n        print(f\"   * {label}: {count} records ({count/dual_count:.1%})\")\n\nprint(\"\\n\\nSUMMARY OF FINDINGS\")\nprint(\"=\" * 50)\nprint(f\"Total single-labeled records analyzed: {sum(single_counts.values())}\")\nprint(f\"Total dual-labeled records analyzed: {total_dual}\")\n\nfor primary_label in LABELS_OF_INTEREST:\n    single_count = single_counts[primary_label]\n    dual_count = dual_counts[primary_label]\n    total = single_count + dual_count\n    if total &gt; 0:\n        print(f\"\\n{primary_label}:\")\n        print(f\" - Single-labeled: {single_count} ({single_count/total:.1%})\")\n        print(f\" - With secondary label: {dual_count} ({dual_count/total:.1%})\")\n\n\n\nStatistical Analysis of Subclusters:\n==================================================\n\nPrimary Label: SR\n - Single-labeled records: 1134\n - Dual-labeled records: 232\n - Secondary label distribution:\n   * STTA: 111 records (47.8%)\n   * LVH: 33 records (14.2%)\n   * SR: 15 records (6.5%)\n   * LQV: 13 records (5.6%)\n   * LAD: 12 records (5.2%)\n   * APB: 10 records (4.3%)\n   * QWA: 9 records (3.9%)\n   * CRBBB: 8 records (3.4%)\n   * 1AVB: 5 records (2.2%)\n   * RAD: 4 records (1.7%)\n   * WPW: 4 records (1.7%)\n   * CLBBB: 4 records (1.7%)\n   * PACE: 2 records (0.9%)\n   * VPB: 2 records (0.9%)\n\nPrimary Label: AFIB\n - Single-labeled records: 341\n - Dual-labeled records: 556\n - Secondary label distribution:\n   * STTA: 299 records (53.8%)\n   * LVH: 91 records (16.4%)\n   * PACE: 47 records (8.5%)\n   * LAD: 31 records (5.6%)\n   * LQV: 20 records (3.6%)\n   * CRBBB: 17 records (3.1%)\n   * RAD: 16 records (2.9%)\n   * VPB: 13 records (2.3%)\n   * CLBBB: 8 records (1.4%)\n   * LAFB: 7 records (1.3%)\n   * QWA: 6 records (1.1%)\n   * RVH: 1 records (0.2%)\n\nPrimary Label: SB\n - Single-labeled records: 1338\n - Dual-labeled records: 707\n - Secondary label distribution:\n   * STTA: 224 records (31.7%)\n   * LVH: 221 records (31.3%)\n   * SR: 65 records (9.2%)\n   * 1AVB: 55 records (7.8%)\n   * LAD: 34 records (4.8%)\n   * LQV: 27 records (3.8%)\n   * APB: 19 records (2.7%)\n   * CRBBB: 16 records (2.3%)\n   * VPB: 12 records (1.7%)\n   * QWA: 11 records (1.6%)\n   * RAD: 9 records (1.3%)\n   * LAFB: 6 records (0.8%)\n   * WPW: 3 records (0.4%)\n   * CLBBB: 3 records (0.4%)\n   * PACE: 2 records (0.3%)\n\nPrimary Label: GSVT\n - Single-labeled records: 626\n - Dual-labeled records: 361\n - Secondary label distribution:\n   * STTA: 201 records (55.7%)\n   * RAD: 33 records (9.1%)\n   * LVH: 24 records (6.6%)\n   * LAD: 20 records (5.5%)\n   * LQV: 18 records (5.0%)\n   * APB: 14 records (3.9%)\n   * VPB: 13 records (3.6%)\n   * CRBBB: 12 records (3.3%)\n   * 1AVB: 7 records (1.9%)\n   * CLBBB: 6 records (1.7%)\n   * QWA: 5 records (1.4%)\n   * AFIB: 3 records (0.8%)\n   * PACE: 1 records (0.3%)\n   * SR: 1 records (0.3%)\n   * RVH: 1 records (0.3%)\n   * LAFB: 1 records (0.3%)\n   * GSVT: 1 records (0.3%)\n\nPrimary Label: PACE\n - Single-labeled records: 45\n - Dual-labeled records: 4\n - Secondary label distribution:\n   * LVH: 2 records (50.0%)\n   * VPB: 1 records (25.0%)\n   * STTA: 1 records (25.0%)\n\n\nSUMMARY OF FINDINGS\n==================================================\nTotal single-labeled records analyzed: 3484\nTotal dual-labeled records analyzed: 1860\n\nSR:\n - Single-labeled: 1134 (83.0%)\n - With secondary label: 232 (17.0%)\n\nAFIB:\n - Single-labeled: 341 (38.0%)\n - With secondary label: 556 (62.0%)\n\nSB:\n - Single-labeled: 1338 (65.4%)\n - With secondary label: 707 (34.6%)\n\nGSVT:\n - Single-labeled: 626 (63.4%)\n - With secondary label: 361 (36.6%)\n\nPACE:\n - Single-labeled: 45 (91.8%)\n - With secondary label: 4 (8.2%)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation 2: Subcluster Analysis within Single-Label Groups</span>"
    ]
  },
  {
    "objectID": "phase3.html",
    "href": "phase3.html",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "",
    "text": "Imports and Configuration\nCode\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\n\nproject_root = Path().absolute().parent.parent\nsys.path.append(str(project_root))\n\nfrom src.visualization.embedding_viz import run_umap\nfrom src.data.unified import UnifiedDataset\nfrom src.data.dataset import DatasetModality\n\nplt.rcParams[\"figure.figsize\"] = (16, 8)\nplt.rcParams[\"font.size\"] = 12",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#data-loading-arrhythmia-ptb-xl",
    "href": "phase3.html#data-loading-arrhythmia-ptb-xl",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Data Loading: Arrhythmia & PTB-XL",
    "text": "Data Loading: Arrhythmia & PTB-XL\n\n\nCode\narr_data = UnifiedDataset(\n    project_root / \"data\", modality=DatasetModality.ECG, dataset_key=\"arrhythmia\"\n)\nptbxl_data = UnifiedDataset(\n    project_root / \"data\", modality=DatasetModality.ECG, dataset_key=\"ptbxl\"\n)\n\narr_splits = arr_data.get_splits()\nptbxl_splits = ptbxl_data.get_splits()\n\narr_test_ids = arr_splits.get(\"test\", [])\nptbxl_test_ids = ptbxl_splits.get(\"test\", [])\n\narr_md_store = arr_data.metadata_store\nptbxl_md_store = ptbxl_data.metadata_store\n\npretrained_embedding = \"baseline\"\nfinetuned_embedding = \"fine_tuned_50\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#mapping-and-label-definitions",
    "href": "phase3.html#mapping-and-label-definitions",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Mapping and Label Definitions",
    "text": "Mapping and Label Definitions\n\n\nCode\nPTBXL_ARR_MAP = {\n    \"AFIB\": \"AFIB\",\n    \"AFLT\": \"AFIB\",\n    \"SR\":   \"SR\",\n    \"SARRH\": \"SR\",\n    \"SBRAD\": \"SB\",\n    \"PACE\":  \"PACE\",\n    \"STACH\": \"GSVT\",\n    \"SVARR\": \"GSVT\",\n    \"SVTAC\": \"GSVT\",\n    \"PSVT\":  \"GSVT\",\n}\n\nLABELS_OF_INTEREST = [\"AFIB\", \"GSVT\", \"SB\", \"SR\", \"PACE\"]\n\nLABEL_COLOR_MAP = {\n    \"SR\":   sns.color_palette(\"tab10\")[0],  # Blue\n    \"AFIB\": sns.color_palette(\"tab10\")[1],  # Orange\n    \"SB\":   sns.color_palette(\"tab10\")[2],  # Green\n    \"GSVT\": sns.color_palette(\"tab10\")[3],  # Red\n    \"PACE\": sns.color_palette(\"tab10\")[4],  # Purple\n}\n\nlabel2color = LABEL_COLOR_MAP",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#arrhythmia-phase-1-embeddings-metadata",
    "href": "phase3.html#arrhythmia-phase-1-embeddings-metadata",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Arrhythmia (Phase 1) Embeddings & Metadata",
    "text": "Arrhythmia (Phase 1) Embeddings & Metadata\n\n\nCode\narr_records_info = []\nemb_base_list_arr = []\nemb_ft_list_arr = []\n\nfor rid in arr_test_ids:\n    meta = arr_md_store.get(rid, {})\n    labels_meta = meta.get(\"labels_metadata\", [])\n    try:\n        emb_base = arr_data.get_embeddings(rid, embeddings_type=pretrained_embedding)\n        emb_ft = arr_data.get_embeddings(rid, embeddings_type=finetuned_embedding)\n    except Exception:\n        continue\n    arr_records_info.append({\"record_id\": rid, \"labels_meta\": labels_meta})\n    emb_base_list_arr.append(emb_base)\n    emb_ft_list_arr.append(emb_ft)\n\ndf_arr = pd.DataFrame(arr_records_info)\ndf_arr[\"row_idx\"] = df_arr.index\n\nbaseline_arr = np.vstack(emb_base_list_arr)\nfinetuned_arr = np.vstack(emb_ft_list_arr)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#treat-pace-as-rhythm",
    "href": "phase3.html#treat-pace-as-rhythm",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Treat “PACE” as Rhythm",
    "text": "Treat “PACE” as Rhythm\n\n\nCode\ndef find_rhythm_labels(labels_meta):\n    \"\"\"\n    Return a list of integration_names that are group=\"Rhythm\" or \"PACE\".\n    \"\"\"\n    rhythm_names = []\n    for lm in labels_meta:\n        integration_name = lm.get(\"integration_name\", \"\")\n        group = lm.get(\"group\", \"\")\n        if group == \"Rhythm\" or integration_name == \"PACE\":\n            rhythm_names.append(integration_name)\n    return rhythm_names\n\ndf_arr[\"rhythm_labels\"] = df_arr[\"labels_meta\"].apply(find_rhythm_labels)\ndf_arr[\"n_rhythm_labels\"] = df_arr[\"rhythm_labels\"].apply(len)\n\ndef get_single_label_of_interest(row):\n    if row[\"n_rhythm_labels\"] == 1:\n        lbl = row[\"rhythm_labels\"][0]\n        if lbl in LABELS_OF_INTEREST:\n            return lbl\n    return None\n\ndf_arr[\"single_rhythm_label\"] = df_arr.apply(get_single_label_of_interest, axis=1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#ptb-xl-embeddings-metadata",
    "href": "phase3.html#ptb-xl-embeddings-metadata",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "PTB-XL Embeddings & Metadata",
    "text": "PTB-XL Embeddings & Metadata\n\n\nCode\nptbxl_records_info = []\nemb_base_list_ptb = []\nemb_ft_list_ptb = []\n\nfor rid in ptbxl_test_ids:\n    meta = ptbxl_md_store.get(rid, {})\n    scp_codes = meta.get(\"scp_codes\", {})\n    scp_statements = meta.get(\"scp_statements\", {})\n\n    valid_rhythm_codes = []\n    for code_key in scp_codes:\n        if code_key in PTBXL_ARR_MAP:\n            code_info = scp_statements.get(code_key, {})\n            if code_info.get(\"rhythm\", 0.0) == 1.0:\n                valid_rhythm_codes.append(code_key)\n\n    if len(valid_rhythm_codes) == 1:\n        code_key = valid_rhythm_codes[0]\n        mapped_label = PTBXL_ARR_MAP[code_key]\n        try:\n            emb_base = ptbxl_data.get_embeddings(rid, embeddings_type=pretrained_embedding)\n            emb_ft = ptbxl_data.get_embeddings(rid, embeddings_type=finetuned_embedding)\n        except Exception:\n            continue\n\n        ptbxl_records_info.append(\n            {\"record_id\": rid, \"ptbxl_code\": code_key, \"mapped_label\": mapped_label}\n        )\n        emb_base_list_ptb.append(emb_base)\n        emb_ft_list_ptb.append(emb_ft)\n\ndf_ptbxl = pd.DataFrame(ptbxl_records_info)\ndf_ptbxl[\"row_idx\"] = df_ptbxl.index\n\nbaseline_ptb = np.vstack(emb_base_list_ptb)\nfinetuned_ptb = np.vstack(emb_ft_list_ptb)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#umap-on-combined-arrhythmia-ptb-xl",
    "href": "phase3.html#umap-on-combined-arrhythmia-ptb-xl",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "UMAP on Combined Arrhythmia + PTB-XL",
    "text": "UMAP on Combined Arrhythmia + PTB-XL\n\n\nCode\numap_params = dict(n_neighbors=15, n_components=2, metric=\"euclidean\", random_state=42)\n\ndef combine_and_umap(arr_emb, ptb_emb):\n    combined = np.vstack([arr_emb, ptb_emb])\n    combined_umap = run_umap(combined, **umap_params)\n    coords_arr = combined_umap[: len(arr_emb)]\n    coords_ptb = combined_umap[len(arr_emb) :]\n    return coords_arr, coords_ptb\n\narr_baseline_umap, ptb_baseline_umap = combine_and_umap(baseline_arr, baseline_ptb)\narr_finetuned_umap, ptb_finetuned_umap = combine_and_umap(finetuned_arr, finetuned_ptb)\n\ndf_arr_baseline = df_arr.copy()\ndf_arr_baseline[\"umap_x\"] = arr_baseline_umap[:, 0]\ndf_arr_baseline[\"umap_y\"] = arr_baseline_umap[:, 1]\n\ndf_arr_finetuned = df_arr.copy()\ndf_arr_finetuned[\"umap_x\"] = arr_finetuned_umap[:, 0]\ndf_arr_finetuned[\"umap_y\"] = arr_finetuned_umap[:, 1]\n\ndf_ptbxl_baseline = df_ptbxl.copy()\ndf_ptbxl_baseline[\"umap_x\"] = ptb_baseline_umap[:, 0]\ndf_ptbxl_baseline[\"umap_y\"] = ptb_baseline_umap[:, 1]\n\ndf_ptbxl_finetuned = df_ptbxl.copy()\ndf_ptbxl_finetuned[\"umap_x\"] = ptb_finetuned_umap[:, 0]\ndf_ptbxl_finetuned[\"umap_y\"] = ptb_finetuned_umap[:, 1]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#outlier-removal-helper",
    "href": "phase3.html#outlier-removal-helper",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Outlier Removal Helper",
    "text": "Outlier Removal Helper\n\n\nCode\ndef remove_outliers_2d(points, z_thresh=3.0):\n    if len(points) == 0:\n        return points\n    mean = np.mean(points, axis=0)\n    std = np.std(points, axis=0)\n    std[std &lt; 1e-12] = 1e-12\n    z_scores = (points - mean) / std\n    dist = np.sqrt((z_scores**2).sum(axis=1))\n    return points[dist &lt;= z_thresh]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#joint-plot-with-marginal-kdes",
    "href": "phase3.html#joint-plot-with-marginal-kdes",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Joint Plot with Marginal KDEs",
    "text": "Joint Plot with Marginal KDEs\n\n\nCode\ndef create_joint_plot(df_arr, df_ptb, title, figsize=(12, 8)):\n    arr_data = df_arr[[\"umap_x\", \"umap_y\"]].copy()\n    arr_data[\"dataset\"] = \"Chapman\"\n\n    ptb_data = df_ptb[[\"umap_x\", \"umap_y\"]].copy()\n    ptb_data[\"dataset\"] = \"PTB-XL\"\n\n    combined_data = pd.concat([arr_data, ptb_data])\n\n    g = sns.JointGrid(\n        data=combined_data, x=\"umap_x\", y=\"umap_y\", height=figsize[0], ratio=4\n    )\n\n    g.ax_joint.scatter(\n        df_arr[\"umap_x\"],\n        df_arr[\"umap_y\"],\n        color=\"lightgray\",\n        s=20,\n        alpha=0.6,\n        label=\"Chapman (all)\",\n    )\n\n    for lbl in LABELS_OF_INTEREST:\n        sub = df_arr.loc[df_arr[\"single_rhythm_label\"] == lbl, [\"umap_x\", \"umap_y\"]].values\n        print(len(sub))\n        if len(sub) &lt; 5:\n            continue\n        sub = remove_outliers_2d(sub, z_thresh=3.0 if lbl != \"PACE\" else 1.0)\n        if len(sub) &lt; 5:\n            continue\n        sub_df = pd.DataFrame(sub, columns=[\"umap_x\", \"umap_y\"])\n        sns.kdeplot(\n            data=sub_df,\n            x=\"umap_x\",\n            y=\"umap_y\",\n            fill=True,\n            levels=4,\n            alpha=0.25,\n            color=label2color.get(lbl, \"red\"),\n            ax=g.ax_joint,\n            label=None,\n        )\n\n    g.ax_joint.scatter(\n        df_ptb[\"umap_x\"],\n        df_ptb[\"umap_y\"],\n        c=[label2color.get(m, \"black\") for m in df_ptb[\"mapped_label\"]],\n        marker=\"*\",\n        s=120,\n        edgecolors=\"white\",\n        linewidth=0.6,\n        alpha=0.9,\n        label=\"PTB-XL (single-rhythm, overlay)\",\n    )\n\n    sns.kdeplot(data=arr_data, x=\"umap_x\", color=\"lightblue\", ax=g.ax_marg_x, label=\"Chapman\")\n    sns.kdeplot(data=ptb_data, x=\"umap_x\", color=\"orange\", ax=g.ax_marg_x, label=\"PTB-XL\")\n    sns.kdeplot(data=arr_data, y=\"umap_y\", color=\"lightblue\", ax=g.ax_marg_y)\n    sns.kdeplot(data=ptb_data, y=\"umap_y\", color=\"orange\", ax=g.ax_marg_y)\n\n    g.ax_marg_x.legend(fontsize=10)\n\n    g.ax_joint.set_xlabel(\"UMAP Dim 1\")\n    g.ax_joint.set_ylabel(\"UMAP Dim 2\")\n    g.ax_joint.set_title(title, fontsize=14)\n    g.ax_joint.grid(True, linestyle=\"--\", alpha=0.5)\n\n    return g",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#create-two-joint-plots-for-baseline-vs.-fine-tuned",
    "href": "phase3.html#create-two-joint-plots-for-baseline-vs.-fine-tuned",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Create Two Joint Plots for Baseline vs. Fine-Tuned",
    "text": "Create Two Joint Plots for Baseline vs. Fine-Tuned\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nplt.subplots_adjust(wspace=0.3)\nfig.suptitle(\n    \"Cross-Domain ECG Embedding Visualization: Fine-Tuned Model (Chapman) with PTB-XL Overlay\",\n    fontsize=18,\n)\n\ng1 = create_joint_plot(\n    df_arr_baseline, df_ptbxl_baseline, f\"Baseline (Pre-trained) Embeddings on Chapman\"\n)\ng2 = create_joint_plot(\n    df_arr_finetuned, df_ptbxl_finetuned, f\"Fine-tuned (Chapman) Embeddings\"\n)\n\ng1.savefig(\"baseline_joint_plot.png\")\ng2.savefig(\"finetuned_joint_plot.png\")\n\n\n1345\n1243\n2300\n1459\n61\n1345\n1243\n2300\n1459\n61",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#plot-density-contours-in-a-1x2-grid",
    "href": "phase3.html#plot-density-contours-in-a-1x2-grid",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Plot Density Contours in a 1x2 Grid",
    "text": "Plot Density Contours in a 1x2 Grid\n\n\nCode\ndef plot_density_contours(ax, df_arr_plot, df_ptb_plot, panel_title, ignore_outliers=True):\n    ax.set_title(panel_title, fontsize=14)\n    ax.scatter(\n        df_arr_plot[\"umap_x\"],\n        df_arr_plot[\"umap_y\"],\n        color=\"lightgray\",\n        s=20,\n        alpha=0.6,\n        label=\"Chapman (all)\",\n    )\n    for lbl in LABELS_OF_INTEREST:\n        sub = df_arr_plot.loc[df_arr_plot[\"single_rhythm_label\"] == lbl, [\"umap_x\", \"umap_y\"]].values\n        print(len(sub))\n        if len(sub) &lt; 5:\n            continue\n        if ignore_outliers:\n            sub = remove_outliers_2d(sub, z_thresh=3.0 if lbl != \"PACE\" else 1.0)\n        if len(sub) &lt; 5:\n            continue\n        sub_df = pd.DataFrame(sub, columns=[\"umap_x\", \"umap_y\"])\n        sns.kdeplot(\n            data=sub_df,\n            x=\"umap_x\",\n            y=\"umap_y\",\n            fill=True,\n            levels=4,\n            alpha=0.25,\n            color=label2color.get(lbl, \"red\"),\n            ax=ax,\n            label=None,\n        )\n    ax.scatter(\n        df_ptb_plot[\"umap_x\"],\n        df_ptb_plot[\"umap_y\"],\n        c=[label2color.get(m, \"black\") for m in df_ptb_plot[\"mapped_label\"]],\n        marker=\"*\",\n        s=120,\n        edgecolors=\"white\",\n        linewidth=0.6,\n        alpha=0.9,\n        label=\"PTB-XL (single-rhythm, overlay)\",\n    )\n    ax.set_xlabel(\"UMAP Dim 1\")\n    ax.set_ylabel(\"UMAP Dim 2\")\n    ax.grid(True, linestyle=\"--\", alpha=0.5)\n\nplot_density_contours(\n    axes[0],\n    df_arr_baseline,\n    df_ptbxl_baseline,\n    f\"Baseline (Pre-trained) Embeddings on Chapman\",\n)\nplot_density_contours(\n    axes[1],\n    df_arr_finetuned,\n    df_ptbxl_finetuned,\n    f\"Fine-Tuned (Chapman) Embeddings\",\n)\n\nplt.tight_layout(rect=[0, 0.2, 1, 0.97])\n\n\n1345\n1243\n2300\n1459\n61\n1345\n1243\n2300\n1459\n61\n\n\n&lt;Figure size 1536x768 with 0 Axes&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#legend-construction",
    "href": "phase3.html#legend-construction",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Legend Construction",
    "text": "Legend Construction\n\n\nCode\ngeneral_handles = [\n    Line2D([0], [0], marker=\"o\", color=\"lightgray\", markersize=8, linewidth=0),\n    Line2D([0], [0], marker=\"*\", color=\"black\", markersize=15, linewidth=0),\n]\ngeneral_labels = [\"Chapman (all)\", \"PTB-XL (single-rhythm, overlay)\"]\n\ncontour_handles = [Patch(color=\"none\")]\ncontour_labels = [\"Chapman Single-Label (Rhythm) Contours:\"]\n\nfor lbl in LABELS_OF_INTEREST:\n    clr = label2color.get(lbl, \"black\")\n    contour_handles.append(Patch(facecolor=clr, edgecolor=\"none\", alpha=0.25))\n    contour_labels.append(f\"{lbl} (Chapman)\")\n\nptbxl_handles = [Patch(color=\"none\")]\nptbxl_labels = [\"PTB-XL Single-Rhythm (Mapped Labels):\"]\n\nptbxl_labels_present = sorted(df_ptbxl[\"mapped_label\"].unique())\nfor lbl in ptbxl_labels_present:\n    clr = label2color.get(lbl, \"black\")\n    ptbxl_handles.append(Line2D([0], [0], marker=\"*\", color=clr, markersize=15, linewidth=0))\n    ptbxl_labels.append(f\"{lbl}\")\n\nlegend1 = fig.legend(\n    general_handles,\n    general_labels,\n    loc=\"lower left\",\n    bbox_to_anchor=(0.2, -0.04),\n    frameon=False,\n    fontsize=9,\n    handletextpad=0.5,\n)\n\nlegend2 = fig.legend(\n    contour_handles,\n    contour_labels,\n    loc=\"lower center\",\n    bbox_to_anchor=(0.5, -0.14),\n    frameon=False,\n    fontsize=9,\n    handletextpad=0.5,\n)\n\nlegend3 = fig.legend(\n    ptbxl_handles,\n    ptbxl_labels,\n    loc=\"lower right\",\n    bbox_to_anchor=(0.8, -0.14),\n    frameon=False,\n    fontsize=9,\n    handletextpad=0.5,\n)\n\nlegend_frame = plt.Rectangle(\n    (0.15, -0.14),\n    0.7,\n    0.18,\n    transform=fig.transFigure,\n    fill=False,\n    edgecolor=\"gray\",\n    linewidth=1,\n)\nfig.patches.append(legend_frame)\n\nplt.tight_layout(rect=[0, 0.15, 1, 0.97])\nplt.savefig(\"cross_domain_embedding_visualization.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n\n&lt;Figure size 1536x768 with 0 Axes&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  },
  {
    "objectID": "phase3.html#faceted-joint-plots-per-label",
    "href": "phase3.html#faceted-joint-plots-per-label",
    "title": "6  Phase 3 - Cross-Domain Notebook",
    "section": "Faceted Joint Plots per Label",
    "text": "Faceted Joint Plots per Label\n\n\nCode\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\ndef create_faceted_joint_plots(df_arr, df_ptb, title, figsize=(12, 12)):\n    n_labels = len(LABELS_OF_INTEREST)\n    n_cols = 3\n    n_rows = (n_labels + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n    fig.suptitle(f\"{title} - Label-to-Label Overlap\", fontsize=18)\n    axes = axes.flatten() if n_rows &gt; 1 else axes\n\n    for i, lbl in enumerate(LABELS_OF_INTEREST):\n        if i &gt;= len(axes):\n            break\n        ax = axes[i]\n        arr_lbl = df_arr[df_arr[\"single_rhythm_label\"] == lbl].copy()\n        ptb_lbl = df_ptb[df_ptb[\"mapped_label\"] == lbl].copy()\n        if len(arr_lbl) &lt; 5 or len(ptb_lbl) &lt; 2:\n            ax.text(0.5, 0.5, f\"Not enough data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n            continue\n\n        arr_points = remove_outliers_2d(arr_lbl[[\"umap_x\", \"umap_y\"]].values,\n                                        z_thresh=3.0 if lbl != \"PACE\" else 1.0)\n        if len(arr_points) &gt;= 5:\n            arr_lbl_clean = pd.DataFrame(arr_points, columns=[\"umap_x\", \"umap_y\"])\n        else:\n            arr_lbl_clean = arr_lbl[[\"umap_x\", \"umap_y\"]]\n\n        sns.scatterplot(\n            data=arr_lbl_clean,\n            x=\"umap_x\",\n            y=\"umap_y\",\n            color=\"lightgray\",\n            s=20,\n            alpha=0.6,\n            ax=ax,\n            label=\"Chapman\",\n        )\n\n        sns.scatterplot(\n            data=ptb_lbl,\n            x=\"umap_x\",\n            y=\"umap_y\",\n            color=label2color.get(lbl, \"black\"),\n            marker=\"*\",\n            s=120,\n            edgecolors=\"white\",\n            linewidth=0.6,\n            alpha=0.8,\n            ax=ax,\n            label=\"PTB-XL\",\n        )\n\n        if len(arr_lbl_clean) &gt;= 5:\n            sns.kdeplot(\n                data=arr_lbl_clean,\n                x=\"umap_x\",\n                y=\"umap_y\",\n                fill=True,\n                levels=4,\n                alpha=0.25,\n                color=\"lightgray\",\n                ax=ax,\n                label=None,\n            )\n\n        ax.set_xlabel(\"UMAP Dim 1\" if i &gt;= len(axes) - n_cols else \"\")\n        ax.set_ylabel(\"UMAP Dim 2\" if i % n_cols == 0 else \"\")\n\n        divider = make_axes_locatable(ax)\n        ax_top = divider.append_axes(\"top\", size=\"15%\", pad=0.1)\n        ax_top.tick_params(axis=\"both\", which=\"both\", labelbottom=False, labelleft=False)\n        if len(arr_lbl_clean) &gt;= 5:\n            sns.kdeplot(data=arr_lbl_clean, x=\"umap_x\", color=\"lightgray\", ax=ax_top)\n        if len(ptb_lbl) &gt;= 2:\n            sns.kdeplot(data=ptb_lbl, x=\"umap_x\", color=label2color.get(lbl, \"black\"), ax=ax_top)\n        ax_top.set_title(f\"Label: {lbl}\", fontsize=14, pad=5)\n        ax_top.set_ylabel(\"\")\n        ax_top.set_yticks([])\n\n        ax_right = divider.append_axes(\"right\", size=\"15%\", pad=0.1)\n        ax_right.tick_params(axis=\"both\", which=\"both\", labelbottom=False, labelleft=False)\n        if len(arr_lbl_clean) &gt;= 5:\n            sns.kdeplot(data=arr_lbl_clean, y=\"umap_y\", color=\"lightgray\", ax=ax_right)\n        if len(ptb_lbl) &gt;= 2:\n            sns.kdeplot(data=ptb_lbl, y=\"umap_y\", color=label2color.get(lbl, \"black\"), ax=ax_right)\n        ax_right.set_xlabel(\"\")\n        ax_right.set_xticks([])\n\n        if i == 0:\n            ax.legend(loc=\"lower right\")\n\n    for j in range(i + 1, len(axes)):\n        axes[j].set_visible(False)\n\n    plt.tight_layout(rect=[0, 0, 1, 0.97])\n    return fig\n\ncreate_faceted_joint_plots(\n    df_arr_baseline,\n    df_ptbxl_baseline,\n    \"Baseline (Pre-trained) Embeddings on Chapman (Faceted)\",\n    figsize=(12, 12),\n)\n\ncreate_faceted_joint_plots(\n    df_arr_finetuned,\n    df_ptbxl_finetuned,\n    \"Fine-Tuned (Chapman) Embeddings (Faceted)\",\n    figsize=(12, 12),\n)\n\nplt.savefig(\"baseline_faceted_joint_plots.png\", dpi=150, bbox_inches=\"tight\")\nplt.savefig(\"finetuned_faceted_joint_plots.png\", dpi=150, bbox_inches=\"tight\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Phase 3 - Cross-Domain Notebook</span>"
    ]
  }
]