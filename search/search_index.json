{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aether: Deep Learning Pipeline for ECG and CMR Data","text":"<p>Welcome to Aether, a comprehensive framework for deep learning on Electrocardiogram (ECG) and Cardiac Magnetic Resonance (CMR) imaging data. This pipeline streamlines the process of developing, training, and analyzing deep learning models for cardiac data analysis.</p>"},{"location":"#key-features","title":"\ud83c\udfaf Key Features","text":"<ul> <li> <p>Modular Pipeline Architecture</p> <ul> <li>End-to-end workflow from preprocessing to evaluation</li> <li>Advanced artifact generation for model interpretation</li> <li>Comprehensive metrics and analysis tools</li> </ul> </li> <li> <p>Dataset Integration &amp; Processing</p> <ul> <li>Standardized interface for ECG and CMR datasets</li> <li>Automated validation and preprocessing pipeline</li> <li>Multi-format data support with efficient loading</li> </ul> </li> <li> <p>Model Development</p> <ul> <li>Flexible training configurations with distributed capabilities</li> <li>Support for various deep learning architectures</li> <li>Built-in visualization and interpretability tools</li> </ul> </li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To begin working with Aether:</p> <ol> <li>Follow our Installation Guide to set up your environment</li> <li>Configure your pipeline using the Configuration System</li> <li>Learn about Dataset Integration to prepare your data</li> <li>Explore Model Training to start training your models</li> </ol>"},{"location":"#research-foundation","title":"\ud83d\udd2c Research Foundation","text":"<p>This project builds upon state-of-the-art research in cardiac deep learning, particularly the work by Turgut et al. Their paper on information transfer between ECG and CMR data has provided valuable insights for this implementation:</p> <p>Turgut, \u00d6., M\u00fcller, P., Hager, P., Shit, S., Starck, S., Menten, M. J., \u2026 Rueckert, D. (2025). \"Unlocking the diagnostic potential of electrocardiograms through information transfer from cardiac magnetic resonance imaging.\" arXiv [Eess.SP].</p>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":"<ul> <li>Getting Started: Essential setup and configuration guides in getting-started/</li> <li>Data Pipeline: Data handling and preprocessing in data/</li> <li>Model Development: Training, architectures, and analysis in models/<ul> <li>Model training and configuration</li> <li>Architecture documentation</li> <li>Analysis and visualization tools</li> </ul> </li> <li>Development Guide: Contributing and CI/CD in development/</li> </ul> <p>For detailed information about specific components, please navigate through the documentation sections in the sidebar.</p>"},{"location":"data/data-management/","title":"Data Management","text":"<p>This project uses DVC (Data Version Control) to manage and version large data files. DVC allows us to version control our data alongside our code while keeping the data files themselves out of Git.</p>"},{"location":"data/data-management/#tracked-data-locations","title":"Tracked Data Locations","text":"<p>Currently, the following data directories are tracked with DVC:</p> <ul> <li><code>data/processed/</code>  - Contains processed datasets</li> <li><code>data/interim/</code>    - Contains preprocessed records for every dataset</li> <li><code>data/embeddings/</code> - Contains precomputed embeddings for every dataset in several states and configs</li> <li><code>data/raw-zips/</code>   - Contains downloaded raw zip versions of the datasets</li> </ul>"},{"location":"data/data-management/#remote-storage","title":"Remote Storage","text":"<p>We use AWS S3 as our remote data store. The data is stored at: <pre><code>s3://fhnw-artifacts/data/dvc/\n</code></pre></p>"},{"location":"data/data-management/#aws-authentication","title":"AWS Authentication","text":"<p>Before using DVC with our S3 remote storage, you need to configure AWS credentials. The easiest way is using the AWS CLI:</p> <ol> <li> <p>Install the AWS CLI if you haven't already:    <pre><code>pip install awscli\n</code></pre></p> </li> <li> <p>Configure your AWS credentials:    <pre><code>aws configure\n</code></pre>    You will be prompted for:</p> </li> <li>AWS Access Key ID</li> <li>AWS Secret Access Key</li> <li>Default region</li> <li>Default output format (press Enter for None)</li> </ol> <p>Contact your project administrator if you need AWS credentials.</p> <ol> <li>Verify your configuration:    <pre><code>aws sts get-caller-identity\n</code></pre>    This should show your AWS account information if configured correctly.</li> </ol>"},{"location":"data/data-management/#common-dvc-commands","title":"Common DVC Commands","text":""},{"location":"data/data-management/#pulling-data-from-remote","title":"Pulling Data from Remote","text":"<p>To get the latest version of the data from the remote storage:</p> <pre><code>dvc pull\n</code></pre> <p>This will download all DVC-tracked files that are not present in your local workspace.</p>"},{"location":"data/data-management/#adding-new-data","title":"Adding New Data","text":"<p>To start tracking a new folder or file with DVC:</p> <pre><code># For a folder\ndvc add data/new_folder/\n\n# For a single file\ndvc add data/new_folder/data.csv\n</code></pre> <p>After running these commands:</p> <ol> <li>DVC will create a corresponding <code>.dvc</code> file that should be committed to Git</li> <li>The actual data will be stored in DVC's cache</li> <li>Remember to push your changes to the remote storage using <code>dvc push</code></li> </ol>"},{"location":"data/data-management/#remote-storage-configuration","title":"Remote Storage Configuration","text":"<p>The project is configured to work with two remote storage options:</p> <ol> <li> <p>Hetzner Storage Box (Default) <pre><code># Currently configured as default remote\ndvc pull  # Will pull from Hetzner by default\n</code></pre></p> </li> <li> <p>AWS S3 <pre><code># To use AWS S3 storage instead\ndvc pull -r ipole-aws\n</code></pre></p> </li> </ol> <p>Both configurations are already set up in the <code>.dvc/config</code> file. The Hetzner Storage Box is set as the default remote, but the AWS S3 bucket is available as an alternative. Access to either storage requires appropriate permissions from the team.</p>"},{"location":"data/data-management/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always pull the latest data before starting work:    <pre><code>dvc pull\n</code></pre></p> </li> <li> <p>After adding new data:    <pre><code>dvc add data/new_folder/\ngit add data/new_folder.dvc\ngit commit -m \"Add new dataset\"\ndvc push\n</code></pre></p> </li> <li> <p>Managing Local Cache</p> </li> </ol> <p>Over time, your local DVC cache may accumulate unused data. Use <code>dvc gc</code> (garbage collection) to clean it up:</p> <pre><code># View what would be removed without actually deleting\ndvc gc --workspace --dry\n\n# Remove files only referenced in workspace\ndvc gc --workspace\n\n# Keep files from all branches and tags\ndvc gc -aT\n\n# Also clean remote storage (be careful!)\ndvc gc --workspace --cloud\n</code></pre> <p>Important options for <code>dvc gc</code>:</p> <ul> <li><code>-w</code>, <code>--workspace</code> - keep only files referenced in current workspace</li> <li><code>-a</code>, <code>--all-branches</code> - keep files referenced in all Git branches</li> <li><code>-T</code>, <code>--all-tags</code> - keep files referenced in all Git tags</li> <li><code>-c</code>, <code>--cloud</code> - also remove files from remote storage (use with caution!)</li> <li><code>--dry</code> - show what would be removed without actually deleting</li> <li><code>-f</code>, <code>--force</code> - skip confirmation prompt</li> </ul> <p>\u26a0\ufe0f Warning: Using <code>--cloud</code> will permanently delete data from remote storage. Make sure you have backups if needed.</p>"},{"location":"data/data-management/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Ensure you have proper AWS credentials configured</li> <li>Check if the remote storage is correctly configured:    <pre><code>dvc remote list\n</code></pre></li> <li>Verify that all <code>.dvc</code> files are tracked in Git </li> </ol>"},{"location":"data/datasets/","title":"Working with Datasets","text":""},{"location":"data/datasets/#overview","title":"Overview","text":"<p>This guide explains how to work with datasets in the project. The system is designed to handle multiple modalities (ECG, CMR) through a unified pipeline that consists of three main stages:</p> <ol> <li>Raw Data Handling: Loading and validating original data files</li> <li>Preprocessing: Converting data into standardized tensor format</li> <li>Unified Access: Bringing everything together for analysis and training</li> </ol>"},{"location":"data/datasets/#data-organization","title":"Data Organization","text":"<p>The project follows a consistent directory structure across all datasets:</p> <pre><code>data/\n\u251c\u2500\u2500 raw/                    # Original dataset files\n\u2502   \u251c\u2500\u2500 cmr/                    # Cardiac MRI datasets\n\u2502   \u2502   \u2514\u2500\u2500 acdc/               # ACDC dataset files\n\u2502   \u2514\u2500\u2500 ecg/                    # ECG datasets\n\u2502       \u2514\u2500\u2500 ptbxl/              # PTB-XL dataset files\n\u251c\u2500\u2500 interim/                # Preprocessed tensors (*.pt files)\n\u2502   \u251c\u2500\u2500 acdc/                   # Preprocessed ACDC records\n\u2502   \u2514\u2500\u2500 ptbxl/                  # Preprocessed PTB-XL records\n\u251c\u2500\u2500 processed/              # Final dataset artifacts\n\u2502   \u251c\u2500\u2500 acdc/                   # ACDC splits and metadata\n\u2502   \u2502   \u251c\u2500\u2500 splits.json         # Train/val/test splits\n\u2502   \u2502   \u2514\u2500\u2500 metadata.db         # Record metadata\n\u2502   \u2514\u2500\u2500 ptbxl/                  # PTB-XL splits and metadata\n\u2514\u2500\u2500 embeddings/             # Pre-computed embeddings (optional)\n    \u251c\u2500\u2500 type1/                  # Embeddings from model 1\n    \u2514\u2500\u2500 type2/                  # Embeddings from model 2\n</code></pre>"},{"location":"data/datasets/#dataset-components","title":"Dataset Components","text":""},{"location":"data/datasets/#1-raw-dataset-handlers","title":"1. Raw Dataset Handlers","text":"<p>Raw dataset handlers provide the interface to original data files. They handle:</p> <ul> <li>Data loading and validation</li> <li>Metadata extraction</li> <li>Label processing</li> <li>Format standardization</li> </ul> <p>The base classes that all handlers must implement:</p> <p>Dataset handlers are registered using:</p>"},{"location":"data/datasets/#src.data.raw.data","title":"<code>src.data.raw.data</code>","text":""},{"location":"data/datasets/#src.data.raw.data.RawDataset","title":"<code>RawDataset</code>","text":"<p>               Bases: <code>BaseDataset</code>, <code>ABC</code></p> <p>Base class for handling raw medical data.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>class RawDataset(BaseDataset, ABC):\n    \"\"\"Base class for handling raw medical data.\"\"\"\n\n    def __init__(self, data_root: Path):\n        super().__init__(data_root)\n        if not self.paths[\"raw\"].exists():\n            raise ValueError(f\"Raw data path does not exist: {self.paths['raw']}\")\n\n    @abstractmethod\n    def verify_data(self) -&gt; None:\n        \"\"\"Verify raw data structure and contents. Throw error if invalid.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Get dataset-specific metadata.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_target_labels(self) -&gt; List[str]:\n        \"\"\"Get list of target labels.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_record_ids(self) -&gt; List[str]:\n        \"\"\"Get all available record IDs without loading data.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_record(self, record_id: str) -&gt; RawRecord:\n        \"\"\"Load a single record.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_stream(self) -&gt; Generator[RawRecord, None, None]:\n        \"\"\"Stream raw records one at a time.\"\"\"\n        pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_all_record_ids","title":"<code>get_all_record_ids()</code>  <code>abstractmethod</code>","text":"<p>Get all available record IDs without loading data.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_all_record_ids(self) -&gt; List[str]:\n    \"\"\"Get all available record IDs without loading data.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_metadata","title":"<code>get_metadata()</code>  <code>abstractmethod</code>","text":"<p>Get dataset-specific metadata.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"Get dataset-specific metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Stream raw records one at a time.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; Generator[RawRecord, None, None]:\n    \"\"\"Stream raw records one at a time.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_target_labels","title":"<code>get_target_labels()</code>  <code>abstractmethod</code>","text":"<p>Get list of target labels.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_target_labels(self) -&gt; List[str]:\n    \"\"\"Get list of target labels.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.load_record","title":"<code>load_record(record_id)</code>  <code>abstractmethod</code>","text":"<p>Load a single record.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef load_record(self, record_id: str) -&gt; RawRecord:\n    \"\"\"Load a single record.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.verify_data","title":"<code>verify_data()</code>  <code>abstractmethod</code>","text":"<p>Verify raw data structure and contents. Throw error if invalid.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef verify_data(self) -&gt; None:\n    \"\"\"Verify raw data structure and contents. Throw error if invalid.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawRecord","title":"<code>RawRecord</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetRecord</code></p> <p>Data class for raw samples.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@dataclass\nclass RawRecord(DatasetRecord):\n    \"\"\"Data class for raw samples.\"\"\"\n\n    data: ndarray\n    target_labels: List[str] | None\n    metadata: Dict[str, Any]\n\n    def __str__(self) -&gt; str:\n        return f\"RawRecord(id={self.id}, targets={\";\".join(self.target_labels)})\"\n\n    def __post_init__(self):\n        if not isinstance(self.data, ndarray):\n            raise ValueError(f\"Data must be a numpy array, got {type(self.data)}\")\n        if not isinstance(self.target_labels, list):\n            raise ValueError(f\"Targets must be a list, got {type(self.target_labels)}\")\n        if not isinstance(self.metadata, dict):\n            raise ValueError(\n                f\"Metadata must be a dictionary, got {type(self.metadata)}\"\n            )\n</code></pre>"},{"location":"data/datasets/#src.data.raw.registry","title":"<code>src.data.raw.registry</code>","text":""},{"location":"data/datasets/#src.data.raw.registry.RawDatasetRegistry","title":"<code>RawDatasetRegistry</code>","text":"<p>Registry for raw data handlers.</p> Source code in <code>src/data/raw/registry.py</code> <pre><code>class RawDatasetRegistry:\n    \"\"\"Registry for raw data handlers.\"\"\"\n\n    _registry: Dict[str, Dict[str, Type[RawDataset]]] = {\"ecg\": {}, \"cmr\": {}}\n\n    @classmethod\n    def register(cls, modality: str, dataset_key: str):\n        \"\"\"Decorator to register a raw data handler.\"\"\"\n\n        def decorator(raw_data_class: Type[RawDataset]):\n            if modality not in cls._registry:\n                cls._registry[modality] = {}\n            cls._registry[modality][dataset_key] = raw_data_class\n            return raw_data_class\n\n        return decorator\n\n    @classmethod\n    def get_handler(cls, modality: str, dataset_key: str) -&gt; Type[RawDataset]:\n        \"\"\"Get raw data handler by modality and key.\"\"\"\n        if modality not in cls._registry:\n            raise ValueError(f\"Unknown modality: {modality}\")\n        if dataset_key not in cls._registry[modality]:\n            raise ValueError(f\"Unknown dataset key for {modality}: {dataset_key}\")\n        return cls._registry[modality][dataset_key]\n\n    @classmethod\n    def get_modality(cls, dataset_key: str) -&gt; DatasetModality:\n        \"\"\"Get modality by dataset key.\"\"\"\n        for modality, datasets in cls._registry.items():\n            if dataset_key in datasets:\n                return DatasetModality(modality)\n        raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n\n    @classmethod\n    def list_datasets(cls) -&gt; Dict[str, List[str]]:\n        \"\"\"List available datasets per modality.\"\"\"\n        return {\n            modality: list(datasets.keys())\n            for modality, datasets in cls._registry.items()\n        }\n\n    @classmethod\n    def list_modalities(cls) -&gt; List[str]:\n        \"\"\"List available modalities.\"\"\"\n        return list(cls._registry.keys())\n</code></pre>"},{"location":"data/datasets/#src.data.raw.registry.RawDatasetRegistry.get_handler","title":"<code>get_handler(modality, dataset_key)</code>  <code>classmethod</code>","text":"<p>Get raw data handler by modality and key.</p> Source code in <code>src/data/raw/registry.py</code> <pre><code>@classmethod\ndef get_handler(cls, modality: str, dataset_key: str) -&gt; Type[RawDataset]:\n    \"\"\"Get raw data handler by modality and key.\"\"\"\n    if modality not in cls._registry:\n        raise ValueError(f\"Unknown modality: {modality}\")\n    if dataset_key not in cls._registry[modality]:\n        raise ValueError(f\"Unknown dataset key for {modality}: {dataset_key}\")\n    return cls._registry[modality][dataset_key]\n</code></pre>"},{"location":"data/datasets/#src.data.raw.registry.RawDatasetRegistry.get_modality","title":"<code>get_modality(dataset_key)</code>  <code>classmethod</code>","text":"<p>Get modality by dataset key.</p> Source code in <code>src/data/raw/registry.py</code> <pre><code>@classmethod\ndef get_modality(cls, dataset_key: str) -&gt; DatasetModality:\n    \"\"\"Get modality by dataset key.\"\"\"\n    for modality, datasets in cls._registry.items():\n        if dataset_key in datasets:\n            return DatasetModality(modality)\n    raise ValueError(f\"Unknown dataset key: {dataset_key}\")\n</code></pre>"},{"location":"data/datasets/#src.data.raw.registry.RawDatasetRegistry.list_datasets","title":"<code>list_datasets()</code>  <code>classmethod</code>","text":"<p>List available datasets per modality.</p> Source code in <code>src/data/raw/registry.py</code> <pre><code>@classmethod\ndef list_datasets(cls) -&gt; Dict[str, List[str]]:\n    \"\"\"List available datasets per modality.\"\"\"\n    return {\n        modality: list(datasets.keys())\n        for modality, datasets in cls._registry.items()\n    }\n</code></pre>"},{"location":"data/datasets/#src.data.raw.registry.RawDatasetRegistry.list_modalities","title":"<code>list_modalities()</code>  <code>classmethod</code>","text":"<p>List available modalities.</p> Source code in <code>src/data/raw/registry.py</code> <pre><code>@classmethod\ndef list_modalities(cls) -&gt; List[str]:\n    \"\"\"List available modalities.\"\"\"\n    return list(cls._registry.keys())\n</code></pre>"},{"location":"data/datasets/#src.data.raw.registry.RawDatasetRegistry.register","title":"<code>register(modality, dataset_key)</code>  <code>classmethod</code>","text":"<p>Decorator to register a raw data handler.</p> Source code in <code>src/data/raw/registry.py</code> <pre><code>@classmethod\ndef register(cls, modality: str, dataset_key: str):\n    \"\"\"Decorator to register a raw data handler.\"\"\"\n\n    def decorator(raw_data_class: Type[RawDataset]):\n        if modality not in cls._registry:\n            cls._registry[modality] = {}\n        cls._registry[modality][dataset_key] = raw_data_class\n        return raw_data_class\n\n    return decorator\n</code></pre>"},{"location":"data/datasets/#2-unified-dataset-system","title":"2. Unified Dataset System","text":"<p>The unified dataset system brings everything together, providing:</p> <ul> <li>Access to raw, preprocessed, and embedded data</li> <li>Automatic data integrity validation</li> <li>Efficient caching</li> <li>Comprehensive metadata management</li> </ul>"},{"location":"data/datasets/#src.data.unified","title":"<code>src.data.unified</code>","text":""},{"location":"data/datasets/#src.data.unified.UnifiedDataset","title":"<code>UnifiedDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>src/data/unified.py</code> <pre><code>class UnifiedDataset(BaseDataset):\n    def __init__(self, data_root: Path, modality: DatasetModality, dataset_key: str):\n        super().__init__(data_root)\n        self._dataset_key = dataset_key\n        self._modality = modality\n\n    @property\n    def dataset_key(self) -&gt; str:\n        return self._dataset_key\n\n    @property\n    def modality(self) -&gt; DatasetModality:\n        return self._modality\n\n    @cached_property\n    def metadata_store(self) -&gt; MetadataStore:\n        return MetadataStore(data_root=self.paths[\"processed\"])\n\n    @cached_property\n    def raw_dataset(self) -&gt; RawDataset:\n        return RawDatasetRegistry.get_handler(self.modality.value, self.dataset_key)(\n            self.data_root\n        )\n\n    def has_dataset_info(self) -&gt; bool:\n        return self.paths[\"misc\"][\"dataset_info\"].exists()\n\n    def get_dataset_info(self) -&gt; Dict[str, Any]:\n        if not self.has_dataset_info():\n            raise ValueError(\"No dataset info found for this dataset.\")\n\n        with open(self.paths[\"misc\"][\"dataset_info\"], \"r\") as f:\n            return json.load(f)\n\n    def has_splits(self) -&gt; bool:\n        return self.paths[\"misc\"][\"splits_file\"].exists()\n\n    def get_splits(self) -&gt; Dict[str, Dict[str, Any]]:\n        if not self.has_splits():\n            raise ValueError(\"No splits found for this dataset.\")\n\n        with open(self.paths[\"misc\"][\"splits_file\"], \"r\") as f:\n            splits = json.load(f)\n        return splits\n\n    def get_split_by_record_id(self, record_id: str) -&gt; str:\n        for split_name, record_ids in self.get_splits().items():\n            if record_id in record_ids:\n                return split_name\n        raise ValueError(f\"Record ID '{record_id}' not found in any split.\")\n\n    def __get_all_record_ids_from_splits(self) -&gt; List[str]:\n        return [\n            record_id for split in self.get_splits().values() for record_id in split\n        ]\n\n    def __get_all_record_ids_from_interim(self) -&gt; List[str]:\n        interim_files = self.paths[\"interim\"].glob(\"*.pt\")\n        return [f.stem for f in interim_files]\n\n    @cache\n    def get_all_record_ids(self) -&gt; List[str]:\n        # if self.has_splits():\n        #    return self.__get_all_record_ids_from_splits()\n        return (\n            self.__get_all_record_ids_from_interim()\n        )  # interim is the ground truth for record ids\n\n    @cache\n    def __load_embeddings(self, embeddings_type: str) -&gt; Dict[str, torch.Tensor]:\n        if embeddings_type not in self.paths[\"embeddings\"].keys():\n            raise ValueError(f\"Embeddings type '{embeddings_type}' not found.\")\n\n        embeddings_path = self.paths[\"embeddings\"][embeddings_type]\n        if not embeddings_path.exists():\n            return {}\n\n        return torch.load(embeddings_path, map_location=\"cpu\")  # type: ignore\n\n    @lru_cache(maxsize=1000)\n    def __load_preprocessed_record(self, record_id: str) -&gt; PreprocessedRecord:\n        if record_id not in self.get_all_record_ids():\n            raise ValueError(f\"Record ID '{record_id}' not found in dataset.\")\n\n        interim_file = self.paths[\"interim\"] / f\"{record_id}.pt\"\n        if not interim_file.exists():\n            raise ValueError(f\"Interim file not found for record ID '{record_id}'.\")\n        return torch.load(interim_file, map_location=\"cpu\")  # type: ignore\n\n    def get_embeddings(\n        self, record_id: str, embeddings_type: str = None\n    ) -&gt; Dict[str, torch.Tensor] | torch.Tensor:\n        if embeddings_type is None:\n            return {\n                k: self.__load_embeddings(k).get(record_id, None)\n                for k in self.paths[\"embeddings\"].keys()\n            }\n\n        return self.__load_embeddings(embeddings_type).get(record_id, None)\n\n    def available_metadata_fields(self) -&gt; set[str]:\n        return self.metadata_store.available_fields()\n\n    @lru_cache(maxsize=1000)\n    def __getitem__(self, record_id: str) -&gt; UnifiedRecord:\n        preprocessed_record = self.__load_preprocessed_record(record_id)\n\n        embeddings = self.get_embeddings(record_id)\n        raw_record = self.raw_dataset.load_record(record_id)\n\n        return UnifiedRecord(\n            id=record_id,\n            raw_record=raw_record,\n            preprocessed_record=preprocessed_record,\n            embeddings=embeddings,\n        )\n\n    def verify_integrity(self) -&gt; None:\n        \"\"\"Validate dataset integrity using interim files as ground truth.\n\n        Performs the following assertions:\n            1. At least one interim file exists (raises ValueError if empty)\n            2. If splits exist:\n                a. All split IDs must exist in interim files (raises on missing IDs)\n                b. All interim IDs must exist in splits (raises on coverage mismatch)\n                c. No duplicate IDs across splits (raises on duplicates)\n                d. Split data must be stored as lists (raises on format error)\n            3. Embeddings files (if exist):\n                a. Must not contain IDs absent from interim files (raises on extra IDs)\n            4. Metadata:\n                a. Must exist for every interim ID (raises on missing metadata)\n\n        Raises:\n            ValueError: For any integrity check failure, with detailed message\n            FileNotFoundError: If critical path components are missing\n\n        Note:\n            Interim files (*.pt in interim directory) are considered the canonical\n            source of truth for valid record IDs. All other components (splits,\n            embeddings, metadata) must align with these IDs.\n        \"\"\"\n        interim_ids = set(self.get_all_record_ids())\n\n        # First check there's at least one interim record\n        if not interim_ids:\n            raise ValueError(f\"No interim files found in {self.paths['interim']}\")\n\n        # Check if every raw record was preprocessed\n        raw_record_ids = set(self.raw_dataset.get_all_record_ids())\n        missing_preprocessed = raw_record_ids - interim_ids\n        if missing_preprocessed:\n            raise ValueError(\n                f\"{len(missing_preprocessed)} raw records missing from interim files. \"\n                f\"First 5: {sorted(missing_preprocessed)[:5]}\"\n            )\n\n        # Split consistency checks (if splits exist)\n        if self.has_splits():\n            splits = self.get_splits()\n            split_ids = set()\n            all_split_ids = []\n\n            # Collect all split IDs and check per-split validity\n            for split_name, split_records in splits.items():\n                if not isinstance(split_records, list):\n                    raise ValueError(\n                        f\"Split '{split_name}' should be a list of IDs, got {type(split_records)}\"\n                    )\n\n                current_split = set(split_records)\n                split_ids.update(current_split)\n                all_split_ids.extend(split_records)\n\n                # Check individual split validity\n                missing_in_interim = current_split - interim_ids\n                if missing_in_interim:\n                    raise ValueError(\n                        f\"Split '{split_name}' contains {len(missing_in_interim)} \"\n                        f\"IDs missing from interim files. First 5: {sorted(missing_in_interim)[:5]}\"\n                    )\n\n            # Check full split coverage\n            if split_ids != interim_ids:\n                missing_in_splits = interim_ids - split_ids\n                extra_in_splits = split_ids - interim_ids\n                error_msg = []\n                if missing_in_splits:\n                    error_msg.append(\n                        f\"{len(missing_in_splits)} interim IDs missing from splits\"\n                    )\n                if extra_in_splits:\n                    error_msg.append(\n                        f\"{len(extra_in_splits)} extra IDs in splits not in interim\"\n                    )\n                raise ValueError(\"Split coverage mismatch: \" + \", \".join(error_msg))\n\n            # Check for duplicate IDs across splits\n            duplicate_ids = {id for id in all_split_ids if all_split_ids.count(id) &gt; 1}\n            if duplicate_ids:\n                raise ValueError(\n                    f\"{len(duplicate_ids)} duplicate IDs found across splits. \"\n                    f\"First 5: {sorted(duplicate_ids)[:5]}\"\n                )\n\n        # Embeddings consistency checks\n        for emb_type, emb_path in self.paths[\"embeddings\"].items():\n            if emb_path.exists():\n                embeddings = self.__load_embeddings(emb_type)\n                if not embeddings:\n                    continue  # Skip empty embeddings files\n\n                emb_ids = set(embeddings.keys())\n                extra_emb_ids = emb_ids - interim_ids\n                if extra_emb_ids:\n                    raise ValueError(\n                        f\"{emb_type} embeddings contain {len(extra_emb_ids)} IDs \"\n                        f\"not in interim files. First 5: {sorted(extra_emb_ids)[:5]}\"\n                    )\n\n        # Metadata completeness check\n        missing_metadata = []\n        for record_id in interim_ids:\n            if not self.metadata_store.get(record_id):\n                missing_metadata.append(record_id)\n            if len(missing_metadata) &gt;= 5:  # Early exit for large datasets\n                break\n        if missing_metadata:\n            raise ValueError(\n                f\"Missing metadata for {len(missing_metadata)} records. \"\n                f\"First 5: {missing_metadata[:5]}\"\n            )\n\n    def __str__(self) -&gt; str:\n        return f\"ProcessedDataset(data_root={self.data_root}, modality={self.modality}, dataset_key={self.dataset_key})\"\n</code></pre>"},{"location":"data/datasets/#src.data.unified.UnifiedDataset.verify_integrity","title":"<code>verify_integrity()</code>","text":"<p>Validate dataset integrity using interim files as ground truth.</p> Performs the following assertions <ol> <li>At least one interim file exists (raises ValueError if empty)</li> <li>If splits exist:     a. All split IDs must exist in interim files (raises on missing IDs)     b. All interim IDs must exist in splits (raises on coverage mismatch)     c. No duplicate IDs across splits (raises on duplicates)     d. Split data must be stored as lists (raises on format error)</li> <li>Embeddings files (if exist):     a. Must not contain IDs absent from interim files (raises on extra IDs)</li> <li>Metadata:     a. Must exist for every interim ID (raises on missing metadata)</li> </ol> <p>Raises:</p> Type Description <code>ValueError</code> <p>For any integrity check failure, with detailed message</p> <code>FileNotFoundError</code> <p>If critical path components are missing</p> Note <p>Interim files (*.pt in interim directory) are considered the canonical source of truth for valid record IDs. All other components (splits, embeddings, metadata) must align with these IDs.</p> Source code in <code>src/data/unified.py</code> <pre><code>def verify_integrity(self) -&gt; None:\n    \"\"\"Validate dataset integrity using interim files as ground truth.\n\n    Performs the following assertions:\n        1. At least one interim file exists (raises ValueError if empty)\n        2. If splits exist:\n            a. All split IDs must exist in interim files (raises on missing IDs)\n            b. All interim IDs must exist in splits (raises on coverage mismatch)\n            c. No duplicate IDs across splits (raises on duplicates)\n            d. Split data must be stored as lists (raises on format error)\n        3. Embeddings files (if exist):\n            a. Must not contain IDs absent from interim files (raises on extra IDs)\n        4. Metadata:\n            a. Must exist for every interim ID (raises on missing metadata)\n\n    Raises:\n        ValueError: For any integrity check failure, with detailed message\n        FileNotFoundError: If critical path components are missing\n\n    Note:\n        Interim files (*.pt in interim directory) are considered the canonical\n        source of truth for valid record IDs. All other components (splits,\n        embeddings, metadata) must align with these IDs.\n    \"\"\"\n    interim_ids = set(self.get_all_record_ids())\n\n    # First check there's at least one interim record\n    if not interim_ids:\n        raise ValueError(f\"No interim files found in {self.paths['interim']}\")\n\n    # Check if every raw record was preprocessed\n    raw_record_ids = set(self.raw_dataset.get_all_record_ids())\n    missing_preprocessed = raw_record_ids - interim_ids\n    if missing_preprocessed:\n        raise ValueError(\n            f\"{len(missing_preprocessed)} raw records missing from interim files. \"\n            f\"First 5: {sorted(missing_preprocessed)[:5]}\"\n        )\n\n    # Split consistency checks (if splits exist)\n    if self.has_splits():\n        splits = self.get_splits()\n        split_ids = set()\n        all_split_ids = []\n\n        # Collect all split IDs and check per-split validity\n        for split_name, split_records in splits.items():\n            if not isinstance(split_records, list):\n                raise ValueError(\n                    f\"Split '{split_name}' should be a list of IDs, got {type(split_records)}\"\n                )\n\n            current_split = set(split_records)\n            split_ids.update(current_split)\n            all_split_ids.extend(split_records)\n\n            # Check individual split validity\n            missing_in_interim = current_split - interim_ids\n            if missing_in_interim:\n                raise ValueError(\n                    f\"Split '{split_name}' contains {len(missing_in_interim)} \"\n                    f\"IDs missing from interim files. First 5: {sorted(missing_in_interim)[:5]}\"\n                )\n\n        # Check full split coverage\n        if split_ids != interim_ids:\n            missing_in_splits = interim_ids - split_ids\n            extra_in_splits = split_ids - interim_ids\n            error_msg = []\n            if missing_in_splits:\n                error_msg.append(\n                    f\"{len(missing_in_splits)} interim IDs missing from splits\"\n                )\n            if extra_in_splits:\n                error_msg.append(\n                    f\"{len(extra_in_splits)} extra IDs in splits not in interim\"\n                )\n            raise ValueError(\"Split coverage mismatch: \" + \", \".join(error_msg))\n\n        # Check for duplicate IDs across splits\n        duplicate_ids = {id for id in all_split_ids if all_split_ids.count(id) &gt; 1}\n        if duplicate_ids:\n            raise ValueError(\n                f\"{len(duplicate_ids)} duplicate IDs found across splits. \"\n                f\"First 5: {sorted(duplicate_ids)[:5]}\"\n            )\n\n    # Embeddings consistency checks\n    for emb_type, emb_path in self.paths[\"embeddings\"].items():\n        if emb_path.exists():\n            embeddings = self.__load_embeddings(emb_type)\n            if not embeddings:\n                continue  # Skip empty embeddings files\n\n            emb_ids = set(embeddings.keys())\n            extra_emb_ids = emb_ids - interim_ids\n            if extra_emb_ids:\n                raise ValueError(\n                    f\"{emb_type} embeddings contain {len(extra_emb_ids)} IDs \"\n                    f\"not in interim files. First 5: {sorted(extra_emb_ids)[:5]}\"\n                )\n\n    # Metadata completeness check\n    missing_metadata = []\n    for record_id in interim_ids:\n        if not self.metadata_store.get(record_id):\n            missing_metadata.append(record_id)\n        if len(missing_metadata) &gt;= 5:  # Early exit for large datasets\n            break\n    if missing_metadata:\n        raise ValueError(\n            f\"Missing metadata for {len(missing_metadata)} records. \"\n            f\"First 5: {missing_metadata[:5]}\"\n        )\n</code></pre>"},{"location":"data/datasets/#src.data.unified.UnifiedRecord","title":"<code>UnifiedRecord</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetRecord</code></p> Source code in <code>src/data/unified.py</code> <pre><code>@dataclass\nclass UnifiedRecord(DatasetRecord):\n    raw_record: RawRecord = None\n    preprocessed_record: PreprocessedRecord = None\n    embeddings: Optional[torch.Tensor] = None\n\n    def __str__(self) -&gt; str:\n        return f\"UnifiedPreprocessedRecord(id={self.id}, has_embeddings={self.embeddings is not None})\"\n</code></pre>"},{"location":"data/datasets/#implementation-guide","title":"Implementation Guide","text":"<p>Now that you have a basic understanding of the unified dataset system, let's go through the implementation steps for creating a new dataset handler given some raw data.</p>"},{"location":"data/datasets/#creating-a-new-dataset-handler","title":"Creating a New Dataset Handler","text":"<p>1. Choose the Appropriate Location:</p> <ul> <li>ECG handlers: <code>src/data/raw/ecg/&lt;dataset_name&gt;.py</code></li> <li>CMR handlers: <code>src/data/raw/cmr/&lt;dataset_name&gt;.py</code></li> </ul> <p>2. Implement Required Methods:</p> <p>All handlers must implement the <code>RawDataset</code> abstract base class, which defines the structure and contents of the raw data.</p> <p>The <code>RawRecord</code> object is defined along the same lines as the <code>RawDataset</code> class, and is used to represent a single record in the dataset. It contains the raw data and metadata for that record ready for preprocessing.</p> <p>3. Register the Dataset Handler:</p> <p>In order to use your new dataset handler, you need to register it in the <code>src/data/__init__.py</code> module.</p> <pre><code># Import datasets in order to register them\nimport src.data.raw.cmr.acdc\nimport src.data.raw.ecg.arrhythmia\nimport src.data.raw.ecg.grouped_arrhythmia\nimport src.data.raw.ecg.shandong\nimport src.data.raw.ecg.ptb_xl\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data","title":"<code>src.data.raw.data</code>","text":""},{"location":"data/datasets/#src.data.raw.data.RawDataset","title":"<code>RawDataset</code>","text":"<p>               Bases: <code>BaseDataset</code>, <code>ABC</code></p> <p>Base class for handling raw medical data.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>class RawDataset(BaseDataset, ABC):\n    \"\"\"Base class for handling raw medical data.\"\"\"\n\n    def __init__(self, data_root: Path):\n        super().__init__(data_root)\n        if not self.paths[\"raw\"].exists():\n            raise ValueError(f\"Raw data path does not exist: {self.paths['raw']}\")\n\n    @abstractmethod\n    def verify_data(self) -&gt; None:\n        \"\"\"Verify raw data structure and contents. Throw error if invalid.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Get dataset-specific metadata.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_target_labels(self) -&gt; List[str]:\n        \"\"\"Get list of target labels.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_record_ids(self) -&gt; List[str]:\n        \"\"\"Get all available record IDs without loading data.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_record(self, record_id: str) -&gt; RawRecord:\n        \"\"\"Load a single record.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_stream(self) -&gt; Generator[RawRecord, None, None]:\n        \"\"\"Stream raw records one at a time.\"\"\"\n        pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_all_record_ids","title":"<code>get_all_record_ids()</code>  <code>abstractmethod</code>","text":"<p>Get all available record IDs without loading data.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_all_record_ids(self) -&gt; List[str]:\n    \"\"\"Get all available record IDs without loading data.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_metadata","title":"<code>get_metadata()</code>  <code>abstractmethod</code>","text":"<p>Get dataset-specific metadata.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"Get dataset-specific metadata.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Stream raw records one at a time.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; Generator[RawRecord, None, None]:\n    \"\"\"Stream raw records one at a time.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.get_target_labels","title":"<code>get_target_labels()</code>  <code>abstractmethod</code>","text":"<p>Get list of target labels.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef get_target_labels(self) -&gt; List[str]:\n    \"\"\"Get list of target labels.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.load_record","title":"<code>load_record(record_id)</code>  <code>abstractmethod</code>","text":"<p>Load a single record.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef load_record(self, record_id: str) -&gt; RawRecord:\n    \"\"\"Load a single record.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data.RawDataset.verify_data","title":"<code>verify_data()</code>  <code>abstractmethod</code>","text":"<p>Verify raw data structure and contents. Throw error if invalid.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@abstractmethod\ndef verify_data(self) -&gt; None:\n    \"\"\"Verify raw data structure and contents. Throw error if invalid.\"\"\"\n    pass\n</code></pre>"},{"location":"data/datasets/#src.data.raw.data","title":"<code>src.data.raw.data</code>","text":""},{"location":"data/datasets/#src.data.raw.data.RawRecord","title":"<code>RawRecord</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DatasetRecord</code></p> <p>Data class for raw samples.</p> Source code in <code>src/data/raw/data.py</code> <pre><code>@dataclass\nclass RawRecord(DatasetRecord):\n    \"\"\"Data class for raw samples.\"\"\"\n\n    data: ndarray\n    target_labels: List[str] | None\n    metadata: Dict[str, Any]\n\n    def __str__(self) -&gt; str:\n        return f\"RawRecord(id={self.id}, targets={\";\".join(self.target_labels)})\"\n\n    def __post_init__(self):\n        if not isinstance(self.data, ndarray):\n            raise ValueError(f\"Data must be a numpy array, got {type(self.data)}\")\n        if not isinstance(self.target_labels, list):\n            raise ValueError(f\"Targets must be a list, got {type(self.target_labels)}\")\n        if not isinstance(self.metadata, dict):\n            raise ValueError(\n                f\"Metadata must be a dictionary, got {type(self.metadata)}\"\n            )\n</code></pre>"},{"location":"data/datasets/#example-implementations-of-dataset-handlers","title":"Example Implementations of Dataset Handlers","text":""},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl","title":"<code>src.data.raw.ecg.ptb_xl</code>","text":""},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL","title":"<code>PTBXL</code>","text":"<p>               Bases: <code>RawDataset</code></p> <p>Handler for raw PTB-XL ECG data at 500 Hz.</p> <p>Expected data structure in the raw data directory (self.paths[\"raw\"]):     ptbxl/     \u251c\u2500\u2500 ptbxl_database.csv         # Contains one row per record (indexed by ecg_id) with extensive metadata     \u251c\u2500\u2500 scp_statements.csv         # Contains SCP-ECG annotation mappings (optional)     \u2514\u2500\u2500 records500/                # WFDB records at 500 Hz</p> <p>The ptbxl_database.csv file contains many columns including:     - ecg_id, patient_id, age, sex, height, weight, nurse, site, device, recording_date, etc.     - scp_codes: a string representation of a dictionary mapping SCP-ECG statement codes to likelihoods.     - filename_hr: path (relative to the ptbxl folder) to the WFDB record for 500 Hz.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>@RawDatasetRegistry.register(DatasetModality.ECG.value, \"ptbxl\")\nclass PTBXL(RawDataset):\n    \"\"\"Handler for raw PTB-XL ECG data at 500 Hz.\n\n    Expected data structure in the raw data directory (self.paths[\"raw\"]):\n        ptbxl/\n        \u251c\u2500\u2500 ptbxl_database.csv         # Contains one row per record (indexed by ecg_id) with extensive metadata\n        \u251c\u2500\u2500 scp_statements.csv         # Contains SCP-ECG annotation mappings (optional)\n        \u2514\u2500\u2500 records500/                # WFDB records at 500 Hz\n\n    The ptbxl_database.csv file contains many columns including:\n        - ecg_id, patient_id, age, sex, height, weight, nurse, site, device, recording_date, etc.\n        - scp_codes: a string representation of a dictionary mapping SCP-ECG statement codes to likelihoods.\n        - filename_hr: path (relative to the ptbxl folder) to the WFDB record for 500 Hz.\n    \"\"\"\n\n    dataset_key = \"ptbxl\"\n    modality = DatasetModality.ECG\n\n    def __init__(self, data_root: Path):\n        \"\"\"\n        Args:\n            data_root: Path to the dataset root folder.\n                      It should contain the ptbxl folder with the PTB-XL files.\n        \"\"\"\n        super().__init__(data_root)\n        self.root = self.paths[\"raw\"]\n\n        self.database_csv = self.root / \"ptbxl_database.csv\"\n        self.scp_csv = self.root / \"scp_statements.csv\"\n\n        if not self.database_csv.exists():\n            raise FileNotFoundError(f\"Database CSV not found: {self.database_csv}\")\n        if not self.scp_csv.exists():\n            logger.warning(f\"SCP statements CSV not found: {self.scp_csv}\")\n\n        # Use \"ecg_id\" as the index for fast lookup.\n        self.metadata_df = pd.read_csv(self.database_csv, index_col=\"ecg_id\")\n\n        # ecg_id is unique, but patient_id is not. Group by patient_id and use random record\n        self.metadata_df = (\n            self.metadata_df.groupby(\"patient_id\")\n            .sample(n=1, random_state=1337)\n            .reset_index(drop=True)\n        )\n\n        self.metadata_df[\"patient_id\"] = self.metadata_df[\"patient_id\"].astype(int)\n        self.metadata_df[\"scp_codes\"] = self.metadata_df[\"scp_codes\"].apply(\n            lambda x: ast.literal_eval(x)\n            if isinstance(x, str)\n            else x  # scp codes are stored as dict strings\n        )\n\n        self.statements_dict = pd.read_csv(self.scp_csv, index_col=0).to_dict(\n            orient=\"index\"\n        )\n\n        self.metadata_dict: Dict[str, Any] = self.metadata_df.to_dict(orient=\"index\")\n        self.records_folder = (\n            self.root / \"records500\"\n        )  # we are only interested in the 500Hz data\n        if not self.records_folder.exists():\n            raise FileNotFoundError(f\"Records folder not found: {self.records_folder}\")\n\n        self.filename_col = (\n            \"filename_hr\"  # 500 Hz records / for 100 hz use \"filename_lr\"\n        )\n\n    def verify_data(self) -&gt; None:\n        \"\"\"Verify the raw data structure and contents.\"\"\"\n        if not self.database_csv.exists():\n            raise ValueError(f\"Database CSV does not exist: {self.database_csv}\")\n        if not self.records_folder.exists():\n            raise ValueError(f\"Records folder does not exist: {self.records_folder}\")\n\n        for record_id in self.get_all_record_ids():\n            file_path = self._get_file_path(record_id)\n            header_path = file_path.with_suffix(\".hea\")\n            dat_path = file_path.with_suffix(\".dat\")\n            if not header_path.exists():\n                raise FileNotFoundError(\n                    f\"WFDB header file for record {record_id} not found: {header_path}\"\n                )\n\n            if not dat_path.exists():\n                raise FileNotFoundError(\n                    f\"WFDB data file for record {record_id} not found: {dat_path}\"\n                )\n\n        # based on https://physionet.org/content/ptb-xl/1.0.3/, we want only unique patients with one record per patient\n        assert (\n            len(self.metadata_df) == 18869\n        ), f\"Expected 18869 records, got {len(self.metadata_df)}\"\n\n    def get_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Get dataset-specific metadata.\n\n        Returns:\n            A dictionary containing dataset details.\n        \"\"\"\n        return {\n            \"num_leads\": 12,\n            \"sampling_rate\": 500,\n            \"record_length_seconds\": 10,\n            \"total_records\": len(self.metadata_df),\n            \"num_patients\": int(self.metadata_df[\"patient_id\"].nunique()),\n        }\n\n    def _get_file_path(self, record_id: str) -&gt; Path:\n        \"\"\"Get the full path to the WFDB record file for a given record ID.\n\n        Args:\n            record_id: The unique ECG record identifier.\n\n        Returns:\n            Full path to the WFDB record file.\n        \"\"\"\n        return self.root / self.metadata_dict[int(record_id)][self.filename_col]\n\n    @cache\n    def get_target_labels(self) -&gt; List[str]:\n        \"\"\"Extract all unique SCP codes (with likelihood &gt; 0) from the metadata.\n\n        Returns:\n            Sorted list of unique target label codes.\n        \"\"\"\n        labels = set()\n        for record_id in self.get_all_record_ids():\n            codes = self._extract_scp_codes(int(record_id))\n            labels.update(self._extract_target_labels(codes))\n        return sorted(list(labels))\n\n    def get_all_record_ids(self) -&gt; List[str]:\n        \"\"\"Get all available record IDs without loading the actual data.\n\n        Returns:\n            List of record identifiers (ecg_id) as strings.\n        \"\"\"\n        return self.metadata_df.index.astype(str).tolist()\n\n    def get_stream(self) -&gt; Generator[RawRecord, None, None]:\n        \"\"\"Stream raw records one at a time.\n\n        Yields:\n            RawRecord objects for each record.\n        \"\"\"\n        for record_id in self.get_all_record_ids():\n            try:\n                yield self.load_record(record_id)\n            except Exception as e:\n                logger.error(f\"Error loading record {record_id}: {e}\")\n                continue\n\n    @cache\n    def load_record(self, record_id: str) -&gt; RawRecord:\n        \"\"\"Load a single ECG record by its identifier.\n\n        Args:\n            record_id: The unique ECG record identifier.\n\n        Returns:\n            A RawRecord object containing the ECG signal data, target labels, and metadata.\n        \"\"\"\n        record_id = int(record_id)\n        if record_id not in self.metadata_dict:\n            raise ValueError(f\"Record {record_id} not found in metadata.\")\n        meta = self.metadata_dict[record_id]\n\n        file_path = self._get_file_path(record_id)\n        record = self._read_wfdb_record(record_id)\n        data = record[0]\n\n        codes = self._extract_scp_codes(record_id)\n        target_labels = self._extract_target_labels(codes)\n\n        record_metadata = dict(meta)\n        record_metadata[\"wfdb_file\"] = str(file_path.relative_to(self.root))\n        record_metadata[\"signal_fields\"] = record[1]\n\n        record_metadata[\"scp_statements\"] = {\n            code: self._extract_scp_statement(code) for code in codes\n        }\n\n        return RawRecord(\n            id=record_id,\n            data=data,\n            target_labels=target_labels,\n            metadata=record_metadata,\n        )\n\n    def _extract_scp_statement(self, code: str) -&gt; str:\n        \"\"\"Extract the SCP statement for a given code from the SCP statements CSV.\n\n        Args:\n            code: SCP code to look up.\n\n        Returns:\n            SCP statement description.\n        \"\"\"\n        if code in self.statements_dict:\n            return self.statements_dict[code]\n        raise ValueError(f\"SCP code {code} not found in statements CSV.\")\n\n    def _extract_target_labels(self, scp_codes: Dict[str, float]) -&gt; List[str]:\n        \"\"\"Extract target labels from the SCP codes dictionary.\n\n        Args:\n            scp_codes: Dictionary mapping SCP codes to likelihoods.\n\n        Returns:\n            List of target labels with likelihood &gt; 0.\n        \"\"\"\n        return [\n            code for code, likelihood in scp_codes.items() if float(likelihood) &gt;= 0\n        ]  # include unknown codes (likelihood == 0): \"... where likelihood is set to 0 if unknown) ...\"\n\n    def _extract_scp_codes(self, record_id: int) -&gt; Dict[str, float]:\n        \"\"\"Extract SCP codes and likelihoods for a given record ID.\n\n        Args:\n            record_id: The unique ECG record identifier.\n\n        Returns:\n            Dictionary mapping SCP codes to likelihoods.\n        \"\"\"\n        codes = self.metadata_dict[record_id].get(\"scp_codes\")\n        if not isinstance(codes, dict):\n            raise ValueError(f\"SCP codes not found for record {record_id}\")\n        return codes\n\n    def _read_wfdb_record(self, record_id: int) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Read the WFDB record data for a given record ID.\n\n        Args:\n            record_id: The unique ECG record identifier.\n\n        Returns:\n            Numpy array containing the ECG signal data.\n        \"\"\"\n        file_path = self._get_file_path(record_id)\n        try:\n            record = wfdb.rdsamp(str(file_path))\n            data = np.array(record[0], dtype=np.float32)\n            data = np.swapaxes(data, 0, 1)\n            return data, record[1]\n        except Exception as e:\n            raise RuntimeError(f\"Error reading WFDB record {file_path}: {e}\") from e\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.__init__","title":"<code>__init__(data_root)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to the dataset root folder.       It should contain the ptbxl folder with the PTB-XL files.</p> required Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>def __init__(self, data_root: Path):\n    \"\"\"\n    Args:\n        data_root: Path to the dataset root folder.\n                  It should contain the ptbxl folder with the PTB-XL files.\n    \"\"\"\n    super().__init__(data_root)\n    self.root = self.paths[\"raw\"]\n\n    self.database_csv = self.root / \"ptbxl_database.csv\"\n    self.scp_csv = self.root / \"scp_statements.csv\"\n\n    if not self.database_csv.exists():\n        raise FileNotFoundError(f\"Database CSV not found: {self.database_csv}\")\n    if not self.scp_csv.exists():\n        logger.warning(f\"SCP statements CSV not found: {self.scp_csv}\")\n\n    # Use \"ecg_id\" as the index for fast lookup.\n    self.metadata_df = pd.read_csv(self.database_csv, index_col=\"ecg_id\")\n\n    # ecg_id is unique, but patient_id is not. Group by patient_id and use random record\n    self.metadata_df = (\n        self.metadata_df.groupby(\"patient_id\")\n        .sample(n=1, random_state=1337)\n        .reset_index(drop=True)\n    )\n\n    self.metadata_df[\"patient_id\"] = self.metadata_df[\"patient_id\"].astype(int)\n    self.metadata_df[\"scp_codes\"] = self.metadata_df[\"scp_codes\"].apply(\n        lambda x: ast.literal_eval(x)\n        if isinstance(x, str)\n        else x  # scp codes are stored as dict strings\n    )\n\n    self.statements_dict = pd.read_csv(self.scp_csv, index_col=0).to_dict(\n        orient=\"index\"\n    )\n\n    self.metadata_dict: Dict[str, Any] = self.metadata_df.to_dict(orient=\"index\")\n    self.records_folder = (\n        self.root / \"records500\"\n    )  # we are only interested in the 500Hz data\n    if not self.records_folder.exists():\n        raise FileNotFoundError(f\"Records folder not found: {self.records_folder}\")\n\n    self.filename_col = (\n        \"filename_hr\"  # 500 Hz records / for 100 hz use \"filename_lr\"\n    )\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.get_all_record_ids","title":"<code>get_all_record_ids()</code>","text":"<p>Get all available record IDs without loading the actual data.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of record identifiers (ecg_id) as strings.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>def get_all_record_ids(self) -&gt; List[str]:\n    \"\"\"Get all available record IDs without loading the actual data.\n\n    Returns:\n        List of record identifiers (ecg_id) as strings.\n    \"\"\"\n    return self.metadata_df.index.astype(str).tolist()\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.get_metadata","title":"<code>get_metadata()</code>","text":"<p>Get dataset-specific metadata.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing dataset details.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>def get_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"Get dataset-specific metadata.\n\n    Returns:\n        A dictionary containing dataset details.\n    \"\"\"\n    return {\n        \"num_leads\": 12,\n        \"sampling_rate\": 500,\n        \"record_length_seconds\": 10,\n        \"total_records\": len(self.metadata_df),\n        \"num_patients\": int(self.metadata_df[\"patient_id\"].nunique()),\n    }\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.get_stream","title":"<code>get_stream()</code>","text":"<p>Stream raw records one at a time.</p> <p>Yields:</p> Type Description <code>RawRecord</code> <p>RawRecord objects for each record.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>def get_stream(self) -&gt; Generator[RawRecord, None, None]:\n    \"\"\"Stream raw records one at a time.\n\n    Yields:\n        RawRecord objects for each record.\n    \"\"\"\n    for record_id in self.get_all_record_ids():\n        try:\n            yield self.load_record(record_id)\n        except Exception as e:\n            logger.error(f\"Error loading record {record_id}: {e}\")\n            continue\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.get_target_labels","title":"<code>get_target_labels()</code>  <code>cached</code>","text":"<p>Extract all unique SCP codes (with likelihood &gt; 0) from the metadata.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Sorted list of unique target label codes.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>@cache\ndef get_target_labels(self) -&gt; List[str]:\n    \"\"\"Extract all unique SCP codes (with likelihood &gt; 0) from the metadata.\n\n    Returns:\n        Sorted list of unique target label codes.\n    \"\"\"\n    labels = set()\n    for record_id in self.get_all_record_ids():\n        codes = self._extract_scp_codes(int(record_id))\n        labels.update(self._extract_target_labels(codes))\n    return sorted(list(labels))\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.load_record","title":"<code>load_record(record_id)</code>  <code>cached</code>","text":"<p>Load a single ECG record by its identifier.</p> <p>Parameters:</p> Name Type Description Default <code>record_id</code> <code>str</code> <p>The unique ECG record identifier.</p> required <p>Returns:</p> Type Description <code>RawRecord</code> <p>A RawRecord object containing the ECG signal data, target labels, and metadata.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>@cache\ndef load_record(self, record_id: str) -&gt; RawRecord:\n    \"\"\"Load a single ECG record by its identifier.\n\n    Args:\n        record_id: The unique ECG record identifier.\n\n    Returns:\n        A RawRecord object containing the ECG signal data, target labels, and metadata.\n    \"\"\"\n    record_id = int(record_id)\n    if record_id not in self.metadata_dict:\n        raise ValueError(f\"Record {record_id} not found in metadata.\")\n    meta = self.metadata_dict[record_id]\n\n    file_path = self._get_file_path(record_id)\n    record = self._read_wfdb_record(record_id)\n    data = record[0]\n\n    codes = self._extract_scp_codes(record_id)\n    target_labels = self._extract_target_labels(codes)\n\n    record_metadata = dict(meta)\n    record_metadata[\"wfdb_file\"] = str(file_path.relative_to(self.root))\n    record_metadata[\"signal_fields\"] = record[1]\n\n    record_metadata[\"scp_statements\"] = {\n        code: self._extract_scp_statement(code) for code in codes\n    }\n\n    return RawRecord(\n        id=record_id,\n        data=data,\n        target_labels=target_labels,\n        metadata=record_metadata,\n    )\n</code></pre>"},{"location":"data/datasets/#src.data.raw.ecg.ptb_xl.PTBXL.verify_data","title":"<code>verify_data()</code>","text":"<p>Verify the raw data structure and contents.</p> Source code in <code>src/data/raw/ecg/ptb_xl.py</code> <pre><code>def verify_data(self) -&gt; None:\n    \"\"\"Verify the raw data structure and contents.\"\"\"\n    if not self.database_csv.exists():\n        raise ValueError(f\"Database CSV does not exist: {self.database_csv}\")\n    if not self.records_folder.exists():\n        raise ValueError(f\"Records folder does not exist: {self.records_folder}\")\n\n    for record_id in self.get_all_record_ids():\n        file_path = self._get_file_path(record_id)\n        header_path = file_path.with_suffix(\".hea\")\n        dat_path = file_path.with_suffix(\".dat\")\n        if not header_path.exists():\n            raise FileNotFoundError(\n                f\"WFDB header file for record {record_id} not found: {header_path}\"\n            )\n\n        if not dat_path.exists():\n            raise FileNotFoundError(\n                f\"WFDB data file for record {record_id} not found: {dat_path}\"\n            )\n\n    # based on https://physionet.org/content/ptb-xl/1.0.3/, we want only unique patients with one record per patient\n    assert (\n        len(self.metadata_df) == 18869\n    ), f\"Expected 18869 records, got {len(self.metadata_df)}\"\n</code></pre>"},{"location":"data/datasets/#src.data.raw.cmr.acdc","title":"<code>src.data.raw.cmr.acdc</code>","text":""},{"location":"data/datasets/#src.data.raw.cmr.acdc.ACDC","title":"<code>ACDC</code>","text":"<p>               Bases: <code>RawDataset</code></p> Source code in <code>src/data/raw/cmr/acdc.py</code> <pre><code>@RawDatasetRegistry.register(\"cmr\", \"acdc\")\nclass ACDC(RawDataset):\n    dataset_key = \"acdc\"\n    modality = DatasetModality.CMR\n\n    required_metadata_fields = {\n        \"ed_frame\": None,\n        \"es_frame\": None,\n        \"group\": None,\n        \"height\": None,\n        \"weight\": None,\n        \"nb_frames\": None,\n    }\n\n    def __init__(self, data_root: Path):\n        super().__init__(data_root)\n        self.data_path = self.paths[\"raw\"]\n        self.__nifti_files = {\n            f.parent.name: f.absolute() for f in self.data_path.glob(\"**/*_4d.nii.gz\")\n        }\n        self.__config_files = {\n            f.parent.name: f.absolute() for f in self.data_path.glob(\"**/Info.cfg\")\n        }\n\n    def verify_data(self) -&gt; None:\n        if not self.data_path.exists():\n            raise ValueError(f\"ACDC data path does not exist: {self.data_path}\")\n\n        if len(self.__nifti_files) == 0:\n            raise ValueError(f\"No 4D NIFTI files found in {self.data_path}\")\n\n        if len(self.__config_files) == 0:\n            raise ValueError(f\"No Info.cfg files found in {self.data_path}\")\n\n        # each NIFTI file should have a corresponding Info.cfg file\n        nifti_stems = set(self.__nifti_files.keys())\n        config_stems = set(self.__config_files.keys())\n        if nifti_stems != config_stems:\n            missing_nifti = nifti_stems - config_stems\n            missing_config = config_stems - nifti_stems\n            raise ValueError(\n                f\"Missing Info.cfg files for {missing_nifti} and NIFTI files for {missing_config}\"\n            )\n\n        # based on https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html\n        assert (\n            len(self.get_all_record_ids()) == 150\n        ), f\"Expected 150 records, got {len(self.get_all_record_ids())}\"\n\n    def get_metadata(self) -&gt; Dict[str, Any]:\n        return {\"default_image_size\": 256}\n\n    def _get_record_paths(self, record_id: str) -&gt; Tuple[Path, Path]:\n        \"\"\"Get paths to .nii.gz and Info.cfg files for a given record ID.\"\"\"\n        return self.__nifti_files.get(record_id), self.__config_files.get(record_id)\n\n    @cache\n    def get_target_labels(self) -&gt; List[str]:\n        \"\"\"Get list of target labels using streaming to reduce memory usage.\"\"\"\n        groups = set()\n        for record_id in self.get_all_record_ids():\n            _, config_path = self._get_record_paths(record_id)\n            group = self._parse_config(config_path).get(\"group\")\n            if group is not None:\n                groups.add(group)\n        return sorted(list(groups))\n\n    def get_all_record_ids(self) -&gt; List[str]:\n        \"\"\"Get all record IDs without loading data.\"\"\"\n        return list(self.__nifti_files.keys())\n\n    def get_stream(self) -&gt; Generator[RawRecord, None, None]:\n        \"\"\"Stream records one at a time to reduce memory usage.\"\"\"\n        for record_id in self.get_all_record_ids():\n            yield self.load_record(record_id)\n\n    @cache\n    def load_record(self, record_id: str) -&gt; RawRecord:\n        nifti_file, config_file = self._get_record_paths(record_id)\n\n        metadata = self._parse_config(config_file)\n        frame_indices = self._calculate_frame_indices(metadata)\n        image_data = self._get_extracted_frames(nifti_file, frame_indices)\n\n        metadata.update(\n            {\n                f\"{frame_type}_frame_idx\": idx\n                for frame_type, idx in zip([\"ed\", \"mid\", \"es\"], frame_indices)\n            }\n        )\n\n        metadata[\"nifti_path\"] = str(nifti_file.relative_to(self.data_path))\n\n        group = metadata.pop(\"group\", None)\n        return RawRecord(\n            id=record_id,\n            data=image_data,\n            target_labels=[group] if group else None,\n            metadata=metadata,\n        )\n\n    @staticmethod\n    def _calculate_frame_indices(metadata: Dict[str, Any]) -&gt; List[int]:\n        required_fields = [\"ed_frame\", \"es_frame\", \"nb_frames\"]\n        if not all(metadata.get(field) is not None for field in required_fields):\n            raise ValueError(\"Missing required frame information\")\n\n        ed_frame_idx = metadata[\"ed_frame\"] - 1\n        es_frame_idx = metadata[\"es_frame\"] - 1\n        nb_frames = metadata[\"nb_frames\"]\n\n        if ed_frame_idx &lt;= es_frame_idx:\n            mid_frame_idx = (ed_frame_idx + es_frame_idx) // 2\n        else:\n            mid_frame_idx = ((ed_frame_idx + es_frame_idx + nb_frames) // 2) % nb_frames\n\n        return [ed_frame_idx, mid_frame_idx, es_frame_idx]\n\n    def _get_extracted_frames(self, path: Path, frame_indices: List[int]) -&gt; np.ndarray:\n        nifti_data = self._read_nifti(path)\n        slices = self._extract_middle_slice(nifti_data)\n        return self._extract_frames(slices, frame_indices)\n\n    @staticmethod\n    def _read_nifti(path: Path) -&gt; np.ndarray:\n        try:\n            return nib.load(str(path)).get_fdata()\n        except Exception as e:\n            raise RuntimeError(f\"Error reading NIFTI file {path}: {e}\")\n\n    @staticmethod\n    def _extract_middle_slice(data: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Gets the middle basal apical slice\"\"\"\n        # We approximate the middle slice by taking the middle slice along the z-axis\n        return data[:, :, data.shape[2] // 2, :]\n\n    @staticmethod\n    def _extract_frames(slice_data: np.ndarray, frame_indices: List[int]) -&gt; np.ndarray:\n        num_frames = slice_data.shape[-1]\n        invalid_indices = [idx for idx in frame_indices if idx &lt; 0 or idx &gt;= num_frames]\n\n        if invalid_indices:\n            raise IndexError(\n                f\"Frame indices out of bounds: {invalid_indices}. \"\n                f\"Valid range: [0, {num_frames - 1}]\"\n            )\n\n        return slice_data[:, :, frame_indices]\n\n    @cache\n    def _parse_config(self, path: Path) -&gt; Dict[str, Any]:\n        \"\"\"Parse config file with caching.\"\"\"\n        config = configparser.ConfigParser()\n        with open(path, \"r\") as f:\n            config.read_string(\"[DEFAULT]\\n\" + f.read())\n\n        parsed_values = {\n            \"ed_frame\": config[\"DEFAULT\"].getint(\"ED\"),\n            \"es_frame\": config[\"DEFAULT\"].getint(\"ES\"),\n            \"group\": config[\"DEFAULT\"].get(\"Group\"),\n            \"height\": config[\"DEFAULT\"].getfloat(\"Height\"),\n            \"weight\": config[\"DEFAULT\"].getfloat(\"Weight\"),\n            \"nb_frames\": config[\"DEFAULT\"].getint(\"NbFrame\"),\n        }\n        return {k: v for k, v in parsed_values.items() if v is not None}\n</code></pre>"},{"location":"data/datasets/#src.data.raw.cmr.acdc.ACDC.get_all_record_ids","title":"<code>get_all_record_ids()</code>","text":"<p>Get all record IDs without loading data.</p> Source code in <code>src/data/raw/cmr/acdc.py</code> <pre><code>def get_all_record_ids(self) -&gt; List[str]:\n    \"\"\"Get all record IDs without loading data.\"\"\"\n    return list(self.__nifti_files.keys())\n</code></pre>"},{"location":"data/datasets/#src.data.raw.cmr.acdc.ACDC.get_stream","title":"<code>get_stream()</code>","text":"<p>Stream records one at a time to reduce memory usage.</p> Source code in <code>src/data/raw/cmr/acdc.py</code> <pre><code>def get_stream(self) -&gt; Generator[RawRecord, None, None]:\n    \"\"\"Stream records one at a time to reduce memory usage.\"\"\"\n    for record_id in self.get_all_record_ids():\n        yield self.load_record(record_id)\n</code></pre>"},{"location":"data/datasets/#src.data.raw.cmr.acdc.ACDC.get_target_labels","title":"<code>get_target_labels()</code>  <code>cached</code>","text":"<p>Get list of target labels using streaming to reduce memory usage.</p> Source code in <code>src/data/raw/cmr/acdc.py</code> <pre><code>@cache\ndef get_target_labels(self) -&gt; List[str]:\n    \"\"\"Get list of target labels using streaming to reduce memory usage.\"\"\"\n    groups = set()\n    for record_id in self.get_all_record_ids():\n        _, config_path = self._get_record_paths(record_id)\n        group = self._parse_config(config_path).get(\"group\")\n        if group is not None:\n            groups.add(group)\n    return sorted(list(groups))\n</code></pre>"},{"location":"data/datasets/#using-the-unified-dataset","title":"Using the Unified Dataset","text":"<p>In this section, we will see how to use the <code>UnifiedDataset</code> class to load and preprocess data from a dataset.</p> <p>The <code>UnifiedDataset</code> can easily be initialized using the <code>data_root</code>, <code>modality</code> and <code>dataset_key</code> arguments. It will then load the dataset's metadata and provide methods for preprocessing and embedding the data.</p> <pre><code>from pathlib import Path\nfrom src.data.dataset import DatasetModality\nfrom src.data.unified import UnifiedDataset\n\n# Initialize dataset\ndataset = UnifiedDataset(\n    data_root=Path(\"data\"),\n    modality=DatasetModality.ECG,\n    dataset_key=\"ptbxl\"\n)\n\n# Access data\nrecord = dataset[\"patient_001\"]\nraw_data = record.raw_record.data\npreprocessed = record.preprocessed_record\nembeddings = record.embeddings\n\n# Get splits and metadata\nsplits = dataset.get_splits() if dataset.has_splits() else None\nmetadata_fields = dataset.available_metadata_fields()\n\n# Verify integrity\ndataset.verify_integrity()\n</code></pre>"},{"location":"data/datasets/#next-steps","title":"Next Steps","text":"<ul> <li>See Data Preprocessing for details on data preprocessing</li> <li>Check Training Models for using your dataset in training</li> <li>Review Data Management for handling data versions</li> </ul>"},{"location":"data/preprocessing/","title":"Data Preprocessing Pipeline","text":""},{"location":"data/preprocessing/#overview","title":"Overview","text":"<p>The preprocessing pipeline is a crucial component that transforms raw medical data into a standardized format suitable for model training and evaluation. It handles two main modalities:</p> <ul> <li>CMR (Cardiac Magnetic Resonance): Image preprocessing including resizing and normalization</li> <li>ECG (Electrocardiogram): Signal processing including sequence standardization and various normalization strategies</li> </ul> <p>The following diagram illustrates the complete preprocessing workflow for both modalities, from raw data ingestion to the final preprocessed tensors that can be picked up by the training pipeline. Note that CMR and ECG data require different preprocessing approaches due to their distinct nature (image vs signal data) and are therefore processed and stored separately.</p> <pre><code>graph TD\n    subgraph Input\n        A1[Raw CMR Data] --&gt; B1\n        A2[Raw ECG Data] --&gt; B2\n    end\n\n    subgraph Stage 1: Data Loading &amp; Validation\n        B1[CMR Loading] --&gt; C1[3D Format Validation]\n        B2[ECG Loading] --&gt; C2[2D Format Validation]\n    end\n\n    subgraph Stage 2: Preprocessing\n        subgraph CMR Steps\n            C1 --&gt; E1[Center Crop]\n            E1 --&gt; F1[Channel First]\n            F1 --&gt; G1[Normalize]\n        end\n\n        subgraph ECG Steps\n            C2 --&gt; E2[Clean NaN]\n            E2 --&gt; F2[Clip Outliers]\n            F2 --&gt; G2[Baseline Correction]\n            G2 --&gt; H2[Normalize]\n        end\n    end\n\n    subgraph Stage 3: Dataset Creation\n        subgraph CMR Dataset\n            G1 --&gt; I1[CMR Data]\n            I1 --&gt; J1[Train Split]\n            I1 --&gt; J2[Val Split]\n            I1 --&gt; J3[Test Split]\n        end\n\n        subgraph ECG Dataset\n            H2 --&gt; I2[ECG Data]\n            I2 --&gt; K1[Train Split]\n            I2 --&gt; K2[Val Split]\n            I2 --&gt; K3[Test Split]\n        end\n\n        subgraph Disk Storage\n            J1 --&gt; L1[train_split.pt]\n            J2 --&gt; L2[val_split.pt]\n            J3 --&gt; L3[test_split.pt]\n            K1 --&gt; M1[train_split.pt]\n            K2 --&gt; M2[val_split.pt]\n            K3 --&gt; M3[test_split.pt]\n        end\n    end\n\n    L1 &amp; M1 -.-&gt; N[Training Pipeline]\n    L2 &amp; M2 -.-&gt; N\n    L3 &amp; M3 -.-&gt; N\n\n    style Input fill:#e6f3ff,stroke:#4a90e2\n    style N fill:#f0fff0,stroke:#27ae60</code></pre>"},{"location":"data/preprocessing/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"data/preprocessing/#stage-1-data-loading-validation","title":"Stage 1: Data Loading &amp; Validation","text":"<p>Each modality undergoes initial validation to ensure data integrity:</p> <ul> <li>CMR Data: </li> <li>Validates 3D format (height \u00d7 width \u00d7 3 slices)</li> <li>Checks for expected dimensionality and slice count</li> <li> <p>Ensures data consistency across samples</p> </li> <li> <p>ECG Data:</p> </li> <li>Validates 2D format (12 leads \u00d7 samples)</li> <li>Verifies the presence of all 12 ECG leads</li> <li>Checks signal length (expected: 5000 samples at 500Hz)</li> </ul>"},{"location":"data/preprocessing/#stage-2-preprocessing","title":"Stage 2: Preprocessing","text":""},{"location":"data/preprocessing/#cmr-processing-steps","title":"CMR Processing Steps","text":"<ol> <li> <p>Center Crop: </p> <ul> <li>Standardizes image size to 210\u00d7210 pixels</li> <li>Centers the heart in the frame using center crop/pad operations</li> </ul> </li> <li> <p>Channel First: </p> <ul> <li>Transposes data to PyTorch format (3\u00d7210\u00d7210)</li> <li>Arranges slices as channels for model input</li> </ul> </li> <li> <p>Normalize: </p> <ul> <li>Scales pixel intensities to [0,1] range per slice</li> <li>Enhances contrast and standardizes input range</li> </ul> </li> </ol>"},{"location":"data/preprocessing/#ecg-processing-steps","title":"ECG Processing Steps","text":"<ol> <li> <p>Clean NaN: </p> <ul> <li>Replaces NaN values with appropriate numerical values</li> <li>Ensures signal continuity</li> </ul> </li> <li> <p>Clip Outliers: </p> <ul> <li>Removes extreme values beyond \u00b14 standard deviations</li> <li>Reduces impact of artifacts and noise</li> </ul> </li> <li> <p>Baseline Correction: </p> <ul> <li>Applies Asymmetric Least Squares Smoothing</li> <li>Removes baseline wander while preserving signal peaks</li> <li>Uses optimized parameters (\u03bb=1e7, p=0.3, iterations=5)</li> </ul> </li> <li> <p>Normalize: </p> <ul> <li>Applies group-wise normalization</li> <li>Groups leads into anatomical sets [1-3, 4-6, 7-12]</li> <li>Standardizes signal amplitude across lead groups</li> </ul> </li> </ol>"},{"location":"data/preprocessing/#stage-3-dataset-creation","title":"Stage 3: Dataset Creation","text":"<p>The preprocessed data can be organized into either standard train/val/test splits or k-fold cross-validation splits:</p> <ol> <li> <p>Standard Split Creation:</p> <ul> <li>Data is split while maintaining patient-level separation</li> <li>Each modality maintains its own splits</li> <li>Default split ratios: 80% train, 10% validation, 10% test</li> <li>Storage format:     <pre><code>train_split.pt\nval_split.pt\ntest_split.pt\n</code></pre></li> </ul> </li> <li> <p>K-Fold Cross-Validation:</p> <ul> <li>Supports stratified k-fold partitioning for robust model evaluation</li> <li>Maintains class balance across folds</li> <li>Each fold creates train and validation splits</li> <li>Storage format for k folds:     <pre><code>fold_1_train_split.pt, fold_1_val_split.pt\nfold_2_train_split.pt, fold_2_val_split.pt\n...\nfold_k_train_split.pt, fold_k_val_split.pt\n</code></pre></li> </ul> </li> </ol> <p>To create k-fold splits, use the partitioning script with the following options:</p> <pre><code>python src/data/partitioning.py \\\n    --split_type stratified_kfold \\\n    --n_folds 5 \\\n    --output_dir path/to/output\n</code></pre> <p>For running experiments with k-fold cross-validation, Hydra sweeps are used:</p> <pre><code># Run all folds\npython train.py experiment=fine_tuning/cmr_acdc_cv # multirun is defined in the experiment config\n\n# Run specific folds\npython train.py experiment=fine_tuning/cmr_acdc_cv hydra.sweeper.params.+fold=1,3,5\n</code></pre> <p>The cross-validation configuration automatically: - Runs the experiment for each fold - Uses the appropriate train/val splits for each fold - Maintains consistent data loading and augmentation across folds</p>"},{"location":"data/preprocessing/#usage","title":"Usage","text":"<p>The preprocessing pipeline can be run using the main script:</p> <pre><code>python src/data/preprocessing/main.py --config-name preprocess\n</code></pre> <p>For configuration options and customization, refer to the Configuration Guide.</p>"},{"location":"data/preprocessing/#stage-1-data-preprocessing","title":"Stage 1: Data Preprocessing","text":"<p>The preprocessing script (<code>src/data/preprocessing/main.py</code>) provides a unified interface for handling different medical imaging modalities. It's designed to be:</p> <ul> <li>Modality-Agnostic: Common interface for both CMR and ECG data</li> <li>Efficient: Supports parallel processing for large datasets</li> <li>Robust: Includes data validation and comprehensive error handling</li> <li>Flexible: Configurable preprocessing parameters per modality</li> <li>Maintainable: Detailed logging and progress tracking</li> </ul>"},{"location":"data/preprocessing/#src.data.preprocessing.main.process_dataset","title":"<code>process_dataset(args)</code>","text":"<p>Process a single dataset identified by a unique key.</p> Source code in <code>src/data/preprocessing/main.py</code> <pre><code>def process_dataset(args: argparse.Namespace) -&gt; None:\n    \"\"\"\n    Process a single dataset identified by a unique key.\n    \"\"\"\n    abs_data_root = Path(args.data_root).absolute()\n\n    available_datasets = RawDatasetRegistry.list_datasets()\n    modality_found = None\n\n    for modality, datasets in available_datasets.items():\n        if args.dataset in datasets:\n            modality_found = modality\n            break\n\n    if modality_found is None:\n        logger.error(\"Dataset key '%s' not found.\", args.dataset)\n        logger.info(\"Available datasets:\")\n        for modality, datasets in available_datasets.items():\n            logger.info(\"%s:\", modality.upper())\n            for ds in datasets:\n                logger.info(\"  - %s\", ds)\n        sys.exit(1)\n\n    logger.info(\n        \"Processing dataset '%s' for modality '%s'.\",\n        args.dataset,\n        modality_found.upper(),\n    )\n    try:\n        raw_data_class = RawDatasetRegistry.get_handler(modality_found, args.dataset)\n        raw_data = raw_data_class(data_root=abs_data_root)\n        raw_data.verify_data()\n\n        if modality_found == \"ecg\":\n            preprocessor = ECGPreprocessor(\n                raw_data_handler=raw_data,\n                sequence_length=args.sequence_length,\n                normalize_mode=args.normalize_mode,\n                max_workers=args.max_workers,\n                force_restart=args.force_restart,\n                clean_interim=args.clean_interim,\n            )\n        elif modality_found == \"cmr\":\n            preprocessor = CMRPreprocessor(\n                raw_data_handler=raw_data,\n                image_size=args.image_size,\n                normalize=args.normalize,\n                max_workers=args.max_workers,\n                force_restart=args.force_restart,\n                clean_interim=args.clean_interim,\n            )\n        else:\n            raise ValueError(f\"Unknown modality: {modality_found}\")\n\n        preprocessor.preprocess_all()\n        logger.info(\"Finished processing dataset '%s'.\", args.dataset)\n    except Exception as e:\n        logger.error(\n            \"Error processing dataset '%s': %s\", args.dataset, e, exc_info=True\n        )\n        raise\n</code></pre>"},{"location":"data/preprocessing/#getting-started","title":"Getting Started","text":"<p>The basic workflow involves selecting a dataset to process and configuring the preprocessing parameters:</p> <pre><code># Basic usage\nrye run preprocess --data_root data --dataset acdc --max_workers 5\n</code></pre>"},{"location":"data/preprocessing/#common-configuration-options","title":"Common Configuration Options","text":"<p>These options apply to all datasets regardless of modality:</p> <ul> <li><code>--data_root</code>: Root directory containing raw data</li> <li><code>--dataset</code>: Unique dataset key to process (e.g., 'acdc', 'ptbxl')</li> <li><code>--max_workers</code>: Number of workers for parallel processing (optional)</li> <li><code>--force_restart</code>: Force restart preprocessing from scratch</li> <li><code>--clean_interim</code>: Clean interim directory after processing</li> </ul>"},{"location":"data/preprocessing/#modality-specific-options","title":"Modality-Specific Options","text":"<p>Different modalities require different preprocessing steps and parameters:</p>"},{"location":"data/preprocessing/#cmr-processing","title":"CMR Processing","text":"<pre><code>rye run preprocess \\\n    --data_root data \\\n    --dataset acdc \\\n    --image_size 256 \\\n    --normalize \\\n    --max_workers 5\n</code></pre> <p>Parameters: - <code>--image_size</code>: Target size for image resizing (default: 256) - <code>--normalize</code>: Apply intensity normalization (default: True)</p>"},{"location":"data/preprocessing/#ecg-processing","title":"ECG Processing","text":"<pre><code>rye run preprocess \\\n    --data_root data \\\n    --dataset ptbxl \\\n    --sequence_length 5000 \\\n    --normalize_mode group_wise \\\n    --max_workers 5\n</code></pre> <p>Parameters: - <code>--sequence_length</code>: Desired sequence length (default: 5000) - <code>--normalize_mode</code>: Normalization strategy (choices: sample_wise, channel_wise, group_wise)</p>"},{"location":"data/preprocessing/#monitoring-and-debugging","title":"Monitoring and Debugging","text":"<p>The preprocessing script maintains detailed logs to help track progress and debug issues:</p> <ul> <li>Log Location: <code>logs/preprocessing.log</code></li> <li>Log Features:<ul> <li>Rotating file handler (max 1MB per file, 5 backup files)</li> <li>Console output for immediate feedback</li> <li>Debug-level logging to file</li> <li>Detailed error tracking and stack traces</li> </ul> </li> </ul>"},{"location":"data/preprocessing/#stage-2-dataset-partitioning","title":"Stage 2: Dataset Partitioning","text":"<p>After preprocessing, the data needs to be split into training, validation, and test sets. The partitioning script (<code>src/data/partitioning.py</code>) provides:</p> <ul> <li>Random and stratified splitting strategies</li> <li>Configurable split sizes</li> <li>Optional train subset creation</li> <li>Automatic sanity checks for data leakage</li> <li>Comprehensive split statistics</li> </ul>"},{"location":"data/preprocessing/#src.data.partitioning","title":"<code>src.data.partitioning</code>","text":""},{"location":"data/preprocessing/#src.data.partitioning.DatasetSplit","title":"<code>DatasetSplit</code>  <code>dataclass</code>","text":"<p>Container for split data and metadata.</p> Source code in <code>src/data/partitioning.py</code> <pre><code>@dataclass\nclass DatasetSplit:\n    \"\"\"Container for split data and metadata.\"\"\"\n\n    data: torch.Tensor\n    targets: List[Optional[List[str]]]\n    record_ids: List[str]\n\n    def to_dict(self):\n        return {\n            \"data\": self.data,\n            \"targets\": self.targets,\n            \"record_ids\": self.record_ids,\n        }\n\n    @staticmethod\n    def from_dict(data_dict):\n        return DatasetSplit(\n            data=data_dict[\"data\"],\n            targets=data_dict[\"targets\"],\n            record_ids=data_dict[\"record_ids\"],\n        )\n\n    def __post_init__(self):\n        if len(self.targets) != len(self.record_ids):\n            raise ValueError(\"Targets and record IDs must have the same length\")\n        if len(self.data) != len(self.targets):\n            raise ValueError(\"Data and targets must have the same length\")\n</code></pre>"},{"location":"data/preprocessing/#src.data.partitioning.PartitioningConfig","title":"<code>PartitioningConfig</code>  <code>dataclass</code>","text":"<p>Configuration for data partitioning.</p> Source code in <code>src/data/partitioning.py</code> <pre><code>@dataclass\nclass PartitioningConfig:\n    \"\"\"Configuration for data partitioning.\"\"\"\n\n    split_type: str = \"random\"  # One of: random, stratified, kfold, stratified_kfold\n    val_size: float = 0.1\n    test_size: float = 0.1\n    train_subsample_size: Optional[float] = None\n    random_seed: int = 1337\n    output_dir: Optional[Path] = None\n    n_folds: int = 5  # Number of folds for k-fold cross validation\n\n    def validate(self):\n        if self.train_subsample_size and not 0 &lt; self.train_subsample_size &lt;= 1:\n            raise ValueError(\"Train subsample size must be between 0 and 1.\")\n        if self.split_type not in [\"random\", \"stratified\", \"kfold\", \"stratified_kfold\"]:\n            raise ValueError(f\"Invalid split type: {self.split_type}\")\n        if self.split_type in [\"kfold\", \"stratified_kfold\"] and self.n_folds &lt; 2:\n            raise ValueError(\"Number of folds must be at least 2\")\n\n    def to_dict(self):\n        return {\n            \"split_type\": self.split_type,\n            \"val_size\": self.val_size,\n            \"test_size\": self.test_size,\n            \"train_subsample_size\": self.train_subsample_size,\n            \"random_seed\": self.random_seed,\n            \"n_folds\": self.n_folds,\n        }\n</code></pre>"},{"location":"data/preprocessing/#src.data.partitioning.create_splits","title":"<code>create_splits(sample_ids, config, targets)</code>","text":"<p>Create dataset splits based on configuration.</p> Source code in <code>src/data/partitioning.py</code> <pre><code>def create_splits(\n    sample_ids: List[str],\n    config: PartitioningConfig,\n    targets: List[Any],\n) -&gt; Dict[str, List[str]]:\n    \"\"\"Create dataset splits based on configuration.\"\"\"\n    logger.info(f\"Creating {config.split_type} splits\")\n\n    if config.split_type in [\"kfold\", \"stratified_kfold\"]:\n        if config.split_type == \"kfold\":\n            kf = KFold(\n                n_splits=config.n_folds, shuffle=True, random_state=config.random_seed\n            )\n            fold_indices = list(kf.split(sample_ids))\n        else:  # stratified_kfold\n            if not targets:\n                raise ValueError(\"Targets required for stratified k-fold split\")\n\n            composite_labels = create_composite_labels(targets)\n            kf = StratifiedKFold(\n                n_splits=config.n_folds, shuffle=True, random_state=config.random_seed\n            )\n            fold_indices = list(kf.split(sample_ids, composite_labels))\n\n        # Create splits dictionary with folds\n        splits = {}\n        for fold_idx, (train_idx, val_idx) in enumerate(fold_indices):\n            fold_num = fold_idx + 1\n            splits[f\"fold_{fold_num}_train\"] = [sample_ids[i] for i in train_idx]\n            splits[f\"fold_{fold_num}_val\"] = [sample_ids[i] for i in val_idx]\n\n    elif config.split_type == \"random\":\n        train_val, test = train_test_split(\n            sample_ids, test_size=config.test_size, random_state=config.random_seed\n        )\n        train, val = train_test_split(\n            train_val,\n            test_size=config.val_size / (1 - config.test_size),\n            random_state=config.random_seed,\n        )\n        splits = {\"train\": train, \"val\": val, \"test\": test}\n    elif config.split_type == \"stratified\":\n        if not targets:\n            raise ValueError(\"Targets required for stratified split\")\n\n        composite_labels = create_composite_labels(targets)\n\n        # First split: train_val vs test\n        sss = StratifiedShuffleSplit(\n            test_size=config.test_size, random_state=config.random_seed, n_splits=1\n        )\n        train_val_idx, test_idx = next(sss.split(sample_ids, composite_labels))\n\n        # Second split: train vs val\n        train_val_ids = [sample_ids[i] for i in train_val_idx]\n        train_val_labels = [composite_labels[i] for i in train_val_idx]\n\n        sss_val = StratifiedShuffleSplit(\n            test_size=config.val_size / (1 - config.test_size),\n            random_state=config.random_seed,\n            n_splits=1,\n        )\n        train_idx, val_idx = next(sss_val.split(train_val_ids, train_val_labels))\n\n        splits = {\n            \"train\": [train_val_ids[i] for i in train_idx],\n            \"val\": [train_val_ids[i] for i in val_idx],\n            \"test\": [sample_ids[i] for i in test_idx],\n        }\n    else:\n        raise ValueError(f\"Invalid split type: {config.split_type}\")\n\n    if config.train_subsample_size and config.split_type not in [\n        \"kfold\",\n        \"stratified_kfold\",\n    ]:\n        logger.debug(f\"Creating train subsample ({config.train_subsample_size})\")\n        subsample_size = int(len(splits[\"train\"]) * config.train_subsample_size)\n        splits[\"train_subsample\"], _ = train_test_split(\n            splits[\"train\"], train_size=subsample_size, random_state=config.random_seed\n        )\n\n    return splits\n</code></pre>"},{"location":"data/preprocessing/#basic-usage","title":"Basic Usage","text":"<p>Here's how to create dataset splits programmatically:</p> <pre><code>from src.data.partitioning import PartitioningConfig, create_splits\nfrom pathlib import Path\n\n# Configure splitting\nconfig = PartitioningConfig(\n    split_type=\"random\",  # or \"stratified\"\n    val_size=0.1,\n    test_size=0.1,\n    train_subsample_size=None,  # Optional: create smaller training subset\n    random_seed=1337,\n    output_dir=Path(\"data/processed/acdc\")\n)\n\n# Create splits\nsplits = create_splits(\n    sample_ids=sample_ids,\n    config=config,\n    targets=targets  # Optional, required for stratified splits\n)\n</code></pre>"},{"location":"data/preprocessing/#command-line-interface","title":"Command-Line Interface","text":"<p>For command-line usage:</p> <pre><code># Random split\nrye run partition \\\n    --data_dir data/interim/acdc \\\n    --output_dir data/processed/acdc \\\n    --split_type random\n\n# Stratified split (requires targets)\nrye run partition \\\n    --data_dir data/interim/acdc \\\n    --output_dir data/processed/acdc \\\n    --split_type stratified\n</code></pre>"},{"location":"data/preprocessing/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>--split_type</code>: Type of split to create</li> <li><code>random</code>: Simple random split</li> <li><code>stratified</code>: Split preserving class distribution</li> <li><code>--val_size</code>: Validation set size as fraction (default: 0.1)</li> <li><code>--test_size</code>: Test set size as fraction (default: 0.1)</li> <li><code>--train_subsample_size</code>: Optional size for train subset as fraction for faster debugging</li> <li><code>--random_seed</code>: Random seed for reproducibility (default: 1337)</li> </ul>"},{"location":"data/preprocessing/#output-structure","title":"Output Structure","text":"<p>The partitioning script generates several output files:</p> <ul> <li><code>splits.json</code>: Contains the mapping of record IDs to each split</li> <li><code>partitioning_cfg.json</code>: Records the configuration used for splitting</li> <li><code>{split_name}_split.pt</code>: Tensor data files for each split (train/val/test)</li> </ul> <p>Each split file contains: - Preprocessed data tensors - Associated targets/labels - Record IDs for traceability - Metadata for reproduction</p>"},{"location":"data/preprocessing/#complete-pipeline-example","title":"Complete Pipeline Example","text":"<p>Here's a complete example showing how to preprocess and partition a CMR dataset:</p> <pre><code># 1. Preprocess the raw data\nrye run preprocess \\\n    --data_root data \\\n    --dataset acdc \\\n    --max_workers 5\n\n# 2. Create dataset splits\nrye run partition \\\n    --data_dir data/interim/acdc \\\n    --output_dir data/processed/acdc \\\n    --split_type random\n</code></pre>"},{"location":"data/preprocessing/#expected-directory-structure","title":"Expected Directory Structure","text":"<p>After running the complete pipeline, your data directory should look like this:</p> <pre><code>data/\n\u251c\u2500\u2500 raw/                            # Original raw data\n\u2502   \u2514\u2500\u2500 acdc/                       # ACDC dataset raw files\n\u2502       \u251c\u2500\u2500 patient001/             # Patient-specific directories\n\u2502       \u251c\u2500\u2500 patient002/\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 interim/                        # Preprocessed data\n\u2502   \u2514\u2500\u2500 acdc/                       # ACDC preprocessed files\n\u2502       \u251c\u2500\u2500 patient001.pt           # Preprocessed patient data\n\u2502       \u251c\u2500\u2500 patient002.pt\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 processed/                      # Final processed and split data\n    \u2514\u2500\u2500 acdc/                       # ACDC dataset splits\n        \u251c\u2500\u2500 train_split.pt          # Training data\n        \u251c\u2500\u2500 val_split.pt            # Validation data\n        \u251c\u2500\u2500 test_split.pt           # Test data\n        \u251c\u2500\u2500 splits.json             # Split configuration and indices\n        \u2514\u2500\u2500 partitioning_cfg.json   # Partitioning configuration\n        \u2514\u2500\u2500 metadata.db             # Record metadata\n</code></pre> <p>Reproducibility with Weights &amp; Biases</p> <p>When using Weights &amp; Biases logging during training, the splits configuration file (<code>splits.json</code>) is automatically uploaded as a W&amp;B artifact. This ensures:</p> <ul> <li>Complete reproducibility of dataset splits across runs</li> <li>Easy tracking of which splits were used in each experiment</li> <li>Ability to reuse exact splits for comparative experiments</li> <li>Version control of dataset partitioning</li> </ul> <p>You can access these splits through the W&amp;B UI or programmatically using the W&amp;B API.</p> <p>After all steps, your should be able to start training. See Training Models for more information.</p>"},{"location":"development/ci-cd/","title":"CI/CD Pipeline","text":"<p>This project uses GitLab CI/CD for automated testing, building, and deployment. The pipeline is designed to be efficient and provide fast feedback while ensuring code quality.</p>"},{"location":"development/ci-cd/#pipeline-structure","title":"Pipeline Structure","text":"<p>The pipeline is organized into three main stages:</p> <ol> <li><code>test</code> - Runs tests and quality checks</li> <li><code>build</code> - Builds documentation and Docker images</li> <li><code>deploy</code> - Deploys documentation to GitLab Pages</li> </ol> <pre><code>graph TD\n    A[Test Stage] --&gt; B[Build Stage]\n    B --&gt; C[Deploy Stage]\n\n    subgraph \"Test Stage\"\n        A1[pytest]\n    end\n\n    subgraph \"Build Stage\"\n        B1[build-docs]\n        B2[docker]\n    end\n\n    subgraph \"Deploy Stage\"\n        C1[pages]\n    end</code></pre>"},{"location":"development/ci-cd/#jobs-overview","title":"Jobs Overview","text":""},{"location":"development/ci-cd/#test-stage","title":"Test Stage","text":""},{"location":"development/ci-cd/#pytest","title":"<code>pytest</code>","text":"<ul> <li>Runs Python tests using pytest</li> <li>Uses Rye for dependency management</li> <li>Generates test reports and coverage information</li> <li>Must pass before proceeding to build stage</li> </ul>"},{"location":"development/ci-cd/#build-stage","title":"Build Stage","text":""},{"location":"development/ci-cd/#build-docs","title":"<code>build-docs</code>","text":"<ul> <li>Builds project documentation using MkDocs</li> <li>Creates documentation site in <code>site/</code> directory</li> <li>Supports preview builds for merge requests</li> </ul>"},{"location":"development/ci-cd/#docker","title":"<code>docker</code>","text":"<ul> <li>Builds and publishes Docker images</li> <li>Pushes to both GitLab Registry and Docker Hub</li> <li>Tags:</li> <li>For merge requests: <code>mr-{MR_ID}</code></li> <li>For branches: Branch name (sanitized)</li> <li>For default branch: Also tags as <code>latest</code></li> </ul>"},{"location":"development/ci-cd/#deploy-stage","title":"Deploy Stage","text":""},{"location":"development/ci-cd/#pages","title":"<code>pages</code>","text":"<ul> <li>Deploys documentation to GitLab Pages</li> <li>Combines Quarto book and MkDocs documentation</li> <li>Supports preview deployments for merge requests</li> <li>URLs:</li> <li>Main site: <code>https://&lt;namespace&gt;.gitlab.io/&lt;project&gt;/</code></li> <li>MR previews: <code>https://&lt;namespace&gt;.gitlab.io/&lt;project&gt;/mr-{MR_ID}/</code></li> </ul>"},{"location":"development/ci-cd/#pipeline-features","title":"Pipeline Features","text":""},{"location":"development/ci-cd/#auto-cancellation","title":"Auto-cancellation","text":"<ul> <li>Redundant pipelines are automatically cancelled</li> <li>Only the latest pipeline for a branch/MR runs</li> <li>All jobs are interruptible for quick cancellation</li> </ul>"},{"location":"development/ci-cd/#caching","title":"Caching","text":"<ul> <li>Python dependencies cached using Rye</li> <li>Docker layer caching for faster builds</li> <li>Test cache for pytest</li> </ul>"},{"location":"development/ci-cd/#dependencies","title":"Dependencies","text":"<ul> <li>Build stage requires successful tests</li> <li>Pages deployment requires successful documentation build</li> </ul>"},{"location":"development/ci-cd/#preview-environments","title":"Preview Environments","text":"<ul> <li>Merge requests get their own preview environments</li> <li>Documentation and Docker images are tagged with MR ID</li> <li>Easy review of changes before merging</li> </ul>"},{"location":"development/ci-cd/#common-rules","title":"Common Rules","text":"<p>The pipeline runs in two scenarios:</p> <ol> <li>On the default branch (main/master)</li> <li>For merge requests</li> </ol> <p>This ensures we:</p> <ul> <li>Always test and build merge requests</li> <li>Keep the default branch deployments up to date</li> <li>Don't waste CI minutes on other branches</li> </ul>"},{"location":"development/ci-cd/#environment-variables-required","title":"Environment Variables Required","text":"<p>The pipeline needs these variables configured in GitLab:</p> <ul> <li><code>DOCKER_HUB_USERNAME</code> - Docker Hub username</li> <li><code>DOCKER_HUB_PASSWORD</code> - Docker Hub password/token</li> <li>GitLab automatically provides CI/CD variables like <code>CI_REGISTRY_*</code> </li> </ul>"},{"location":"development/docker/","title":"Docker Development Environment","text":"<p>This guide explains how to use Docker for development and deployment in the Aether project. Our Docker setup provides a consistent environment with CUDA support, development tools, and the Rye package manager.</p>"},{"location":"development/docker/#container-architecture","title":"Container Architecture","text":""},{"location":"development/docker/#base-image","title":"Base Image","text":"<p>We use <code>nvidia/cuda:11.7.1-cudnn8-devel-ubuntu22.04</code> as our base image for:</p> <ul> <li>CUDA 11.7.1 and cuDNN 8 support</li> <li>Development tools compatibility</li> <li>Ubuntu 22.04 LTS stability</li> </ul>"},{"location":"development/docker/#key-components","title":"Key Components","text":"<ol> <li> <p>Environment Configuration <pre><code>ENV DEBIAN_FRONTEND=noninteractive \\\n    LANG=C.UTF-8 \\\n    RYE_HOME=/app/.rye \\\n    LC_ALL=C.UTF-8\n</code></pre></p> </li> <li> <p>Development Tools</p> <ul> <li>Git for version control</li> <li>Build essentials for compilation</li> <li>Rye package manager for Python dependencies</li> </ul> </li> <li> <p>Python Environment</p> <ul> <li>Managed by Rye package manager</li> <li>Dependencies from <code>pyproject.toml</code></li> <li>Locked versions in <code>requirements.lock</code></li> </ul> </li> </ol>"},{"location":"development/docker/#building-images","title":"Building Images","text":""},{"location":"development/docker/#using-the-build-script","title":"Using the Build Script","text":"<p>The <code>build.sh</code> script provides a flexible way to build and manage Docker images:</p> <pre><code># Basic build\n./docker/build.sh\n\n# Build with custom name and tag\n./docker/build.sh --name myproject --tag v1.0\n\n# Build and convert to Singularity\n./docker/build.sh --singularity\n\n# Build and push to registry\n./docker/build.sh --push --registry your.registry.com/username\n</code></pre>"},{"location":"development/docker/#build-script-options","title":"Build Script Options","text":"Option Description Default <code>--name</code> Image name \"projection\" <code>--tag</code> Image tag \"latest\" <code>--singularity</code> Convert to Singularity false <code>--push</code> Push to registry false <code>--registry</code> Registry URL \"\""},{"location":"development/docker/#development-workflow","title":"Development Workflow","text":""},{"location":"development/docker/#local-development","title":"Local Development","text":"<ol> <li> <p>Build the Image <pre><code>./docker/build.sh\n</code></pre></p> </li> <li> <p>Run Development Container <pre><code>docker run --gpus all -it \\\n    -v $(pwd):/workspace \\\n    -p 8888:8888 \\  # for Jupyter\n    projection:latest\n</code></pre></p> </li> <li> <p>Working with the Container</p> <ul> <li>Code changes in host are reflected in container</li> <li>Python environment is pre-configured</li> <li>GPU support is enabled</li> </ul> </li> </ol>"},{"location":"development/docker/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Volume Mounting</p> <ul> <li>Mount your project directory as <code>/workspace</code></li> <li>Consider mounting data directories separately</li> <li>Use named volumes for persistent storage</li> </ul> </li> <li> <p>Environment Variables <pre><code>docker run -it \\\n    -e CUDA_VISIBLE_DEVICES=0,1 \\\n    -e WANDB_API_KEY=your_key \\\n    projection:latest\n</code></pre></p> </li> <li> <p>Resource Management <pre><code>docker run -it \\\n    --gpus '\"device=0,1\"' \\\n    --cpus=4 \\\n    --memory=16g \\\n    projection:latest\n</code></pre></p> </li> </ol>"},{"location":"development/docker/#cluster-deployment","title":"Cluster Deployment","text":""},{"location":"development/docker/#converting-to-singularity","title":"Converting to Singularity","text":"<ol> <li> <p>Build and Convert <pre><code>./docker/build.sh --singularity\n</code></pre></p> </li> <li> <p>Transfer to Cluster <pre><code>scp projection_latest.sif username@cluster:/path/to/home/\n</code></pre></p> </li> <li> <p>Running on Cluster <pre><code>singularity run --nv projection_latest.sif\n</code></pre></p> </li> </ol>"},{"location":"development/docker/#slurm-integration","title":"SLURM Integration","text":"<p>For SLURM-based clusters, create job scripts like:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=aether_training\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n\nsingularity run --nv \\\n    -B /path/to/data:/data \\\n    projection_latest.sif \\\n    python train.py\n</code></pre>"},{"location":"development/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/docker/#common-issues","title":"Common Issues","text":"<ol> <li> <p>CUDA/GPU Access</p> <ul> <li>Ensure NVIDIA drivers are installed</li> <li>Use <code>nvidia-smi</code> to verify GPU access</li> <li>Check <code>--gpus</code> flag in docker run</li> </ul> </li> <li> <p>Volume Permissions <pre><code># Fix permission issues\ndocker run -it \\\n    -u $(id -u):$(id -g) \\\n    -v $(pwd):/workspace \\\n    projection:latest\n</code></pre></p> </li> <li> <p>Package Installation</p> <ul> <li>Update <code>pyproject.toml</code> and rebuild</li> <li>Use <code>rye add package_name</code> inside container</li> <li>Check Rye environment activation</li> </ul> </li> </ol>"},{"location":"development/docker/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Container Inspection <pre><code># Enter running container\ndocker exec -it container_name bash\n\n# Check logs\ndocker logs container_name\n</code></pre></p> </li> <li> <p>Build Issues <pre><code># Build with verbose output\ndocker build --progress=plain -t projection:latest .\n</code></pre></p> </li> </ol>"},{"location":"development/tools/","title":"Development Tools and Practices","text":"<p>This guide covers the development tools and configurations used in the Aether project.</p>"},{"location":"development/tools/#code-quality-tools","title":"Code Quality Tools","text":""},{"location":"development/tools/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>We use pre-commit hooks to maintain code quality and consistency. The configuration is in <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  rev: v0.8.5\n  hooks:\n    - id: ruff-format\n</code></pre> <p>To set up pre-commit hooks:</p> <pre><code># Install pre-commit\npip install pre-commit\n\n# Install the hooks\npre-commit install\n</code></pre>"},{"location":"development/tools/#code-formatting","title":"Code Formatting","text":"<p>We use Ruff for Python code formatting. The formatter is configured through pre-commit and runs automatically on git commits.</p>"},{"location":"development/tools/#environment-configuration","title":"Environment Configuration","text":""},{"location":"development/tools/#environment-variables","title":"Environment Variables","text":"<p>We use a <code>.env</code> file for local development. Copy <code>sample.env</code> to <code>.env</code> and configure:</p> <pre><code># API Keys for Experiment Tracking\nNEPTUNE_API_TOKEN=\"YOUR_API_TOKEN\"\nWANDB_API_KEY=\"YOUR_API_TOKEN\"\n\n# AWS Configuration\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nAWS_REGION=sa-east-1\n\n# Development Settings\nHYDRA_FULL_ERROR=1\n</code></pre>"},{"location":"development/tools/#documentation","title":"Documentation","text":""},{"location":"development/tools/#mkdocs-configuration","title":"MkDocs Configuration","text":"<p>The documentation is built using MkDocs with the Material theme. Configuration in <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  name: material\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    - scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n</code></pre>"},{"location":"development/tools/#python-api-documentation","title":"Python API Documentation","text":"<p>We use mkdocstrings for automatic Python API documentation. Configuration in <code>mkdocs.yml</code>:</p> <pre><code>plugins:\n  - mkdocstrings:\n      default_handler: python\n      handlers:\n        python:\n          paths: [src/, .]\n          options:\n            show_source: true\n            show_root_heading: true\n            heading_level: 2\n</code></pre>"},{"location":"development/tools/#markdown-extensions","title":"Markdown Extensions","text":"<p>The documentation supports various Markdown extensions for enhanced content:</p> <pre><code>markdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - admonition\n  - pymdownx.details\n  - pymdownx.tabbed:\n      alternate_style: true\n  - tables\n  - toc:\n      permalink: true\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration System","text":"<p>The project uses Hydra for managing configurations. This provides a flexible, hierarchical configuration system that allows for easy experiment management and reproducibility.</p>"},{"location":"getting-started/configuration/#structure","title":"Structure","text":"<p>The configuration is organized into several main components. Each component covers one part of the entire deep learning pipeline and serves as a base and be selectively overridden in subsequent configurations, which makes the system very versatile and adaptable.</p> <pre><code>configs/\n\u251c\u2500\u2500 data/               # Dataset configurations\n\u251c\u2500\u2500 model/              # Model architectures and parameters\n\u251c\u2500\u2500 trainer/            # Training settings and hyperparameters\n\u251c\u2500\u2500 callbacks/          # Training callbacks (early stopping, checkpointing)\n\u251c\u2500\u2500 logger/             # Logging configurations (W&amp;B, TensorBoard)\n\u251c\u2500\u2500 paths/              # Path configurations\n\u251c\u2500\u2500 experiment/         # Complete experiment configurations\n\u251c\u2500\u2500 modality/           # Modality-specific settings\n\u251c\u2500\u2500 debug/              # Debug configurations\n\u251c\u2500\u2500 extras/             # Additional configuration components\n\u251c\u2500\u2500 hparams_search/     # Hyperparameter optimization settings\n\u2514\u2500\u2500 hydra/              # Hydra-specific configurations \n</code></pre> <p>The following graph shows the configuration relationship.</p> <pre><code>graph TD\n    E[Experiment Config] --&gt; M[Model Config]\n    E --&gt; D[Data Config]\n    E --&gt; T[Trainer Config]\n    E --&gt; L[Logger Config]\n    E --&gt; C[Callbacks Config]\n\n    D --&gt; MD[Modality Config]\n    D --&gt; P[Paths Config]\n\n    M --&gt; MD\n\n    subgraph \"Experiment Types\"\n        PT[Pre-training] --&gt; E\n        FT[Fine-tuning] --&gt; E\n        EM[Artifacts] --&gt; E\n    end\n\n    subgraph \"Runtime Options\"\n        HP[Hyperparameter Search] -.-&gt; E\n        DB[Debug Config] -.-&gt; E\n    end\n\n    style E fill:#f9f,stroke:#333\n    style PT fill:#bbf,stroke:#333\n    style FT fill:#bbf,stroke:#333\n    style EM fill:#bbf,stroke:#333\n    style HP fill:#ddd,stroke:#333\n    style DB fill:#ddd,stroke:#333</code></pre>"},{"location":"getting-started/configuration/#core-components","title":"Core Components","text":""},{"location":"getting-started/configuration/#data-configuration","title":"Data Configuration","text":"<p>The data configuration controls dataset loading and preprocessing:</p> <pre><code>data_root: ${paths.data_dir}\ndataset_key: ${modality.dataset_key}\n\nnum_workers: 4\npin_memory: true\npersistent_workers: true\nuse_train_subsample: false\n</code></pre> <p>Modality-specific configurations extend the default, like in the following example for CMR:</p> <pre><code>_target_: src.data.pytorch.datamodules.cmr_datamodule.CMRDataModule\n\ndefaults:\n  - default\n\nbatch_size: 32\naugmentation_rate: 0.95\nlive_loading: false\n\ncross_validation: false\nfold_number: null\n\n# Not done in eval dataset: https://github.com/oetu/MMCL-ECG-CMR/blob/bd3c18672de8e5fa73bb753613df94547bd6245b/mmcl/datasets/EvalImageDataset.py#L35\n# Not done in imaging contrastive dataset: https://github.com/oetu/MMCL-ECG-CMR/blob/main/mmcl/datasets/ContrastiveImageDataset.py\nmanual_crop: null\n# Manual cropping parameters (relative to img_size)\n# This is not used for the finetuning as per https://github.com/oetu/MMCL-ECG-CMR/blob/bd3c18672de8e5fa73bb753613df94547bd6245b/mmcl/datasets/EvalImageDataset.py#L35 \n# This appears faulty to us.\n# manual_crop:\n#   top: 0.21\n#   left: 0.325\n#   height: 0.375\n#   width: 0.375\n\nimg_size: 210\n\n# From Turgut et. al (2025):\n#   \"The CMR images are augmented using\n#   horizontal flips (probability=0.5),\n#   rotations (probability=0.5, degrees=45),\n#   color jitter (brightness=0.5, contrast=0.5, saturation=0.25),\n#   random resized cropping (size=210, scale=(0.6, 1)).\"\n\n# Code ref eval: \n# https://github.com/oetu/MMCL-ECG-CMR/blob/main/mmcl/datasets/EvalImageDataset.py#L11\n# leads to: https://github.com/oetu/MMCL-ECG-CMR/blob/bd3c18672de8e5fa73bb753613df94547bd6245b/mmcl/utils/utils.py#L70\nrotation_degrees: 45\nbrightness: 0.5\ncontrast: 0.5\nsaturation: 0.25\nrandom_crop_scale: [0.6, 1.0]\n</code></pre>"},{"location":"getting-started/configuration/#modality-configuration","title":"Modality Configuration","text":"<p>Modality-specific configurations are closely tied to the data configuration but yet distinct. They define dataset specifics suchas the number of classes for a supervised task. For example, the <code>acdc.yaml</code> configuration for the CMR modality defines the number of classes as <code>5</code>.</p> <pre><code>dataset_key: acdc\n\nimg_size: 224\n# downstream task\nnum_classes: 5\n</code></pre>"},{"location":"getting-started/configuration/#model-configuration","title":"Model Configuration","text":"<p>Model configurations define architecture and training specifics:</p> <pre><code>_target_: src.models.ecg_classifier.ECGClassifier\n\ndefaults:\n  - ecg_encoder # Same because both use ViT backbone\n\n\nlearning_rate: 3e-6 # TODO: Check what we can reference as baseline either from paper or code\nweight_decay: .05   # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L112\nlayer_decay: 0.75   # Paper does not mention final value used, but made a sweep: (0.5, 0.75)\ndrop_path_rate: 0.1 # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L82\n\n# https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L86\nmask_ratio: 0.0\nmask_c_ratio: 0.0\nmask_t_ratio: 0.0\n\n\n# Training parameters\n# \u201c[..] over 400 epochs with a 5% warmup.\u201d (Turgut et al., 2025, p. 5)\nwarmup_epochs: 5 # This should be 5% of $trainer.max_epochs\nmax_epochs: ${trainer.max_epochs}\nsmoothing: 0.1 # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L135\n\n# Downstream task parameters\nnum_classes: ${modality.num_classes}\nglobal_pool: \"attention_pool\" # \u201cWe replace the global average pooling of fs(\u00b7) used during pre-training with the attention pooling described in [28].\u201d (Turgut et al., 2025, p. 5)\n\npretrained_weights: \"model_weights/signal_encoder_mmcl.pth\"\n\ntask_type: ${modality.task_type}\n</code></pre> <p>For more details on model specifics, see the Model Architectures section.</p>"},{"location":"getting-started/configuration/#training-configuration","title":"Training Configuration","text":"<p>Training settings control the training process:</p> <pre><code>_target_: lightning.pytorch.trainer.Trainer\n\ndefault_root_dir: ${paths.output_dir}\n\nmin_epochs: 1\nmax_epochs: 30\n\naccelerator: auto\ndevices: 1\n\nprecision: 16-mixed\n\ncheck_val_every_n_epoch: 1\nlog_every_n_steps: 0\n\n# makes training slower but gives more reproducibility than just setting seeds\ndeterministic: False\n</code></pre>"},{"location":"getting-started/configuration/#experiment-configuration","title":"Experiment Configuration","text":"<p>Experiments are the highest-level configuration component, bringing together all other core components into a complete setup. An experiment configuration defines how all components interact to create a complete training system. Here's a comprehensive example of the <code>ecg_arrhythmia</code> experiment coupling the ECG modality, Arrhythmia dataset and ECG classifier into a complete configuration:</p> <pre><code># @package _global_\ndefaults:\n  - override /model: ecg_classifier\n  - override /modality: ${data}/arrhythmia\n  - override /data: ecg\n\ndata:\n  batch_size: 128\n  downstream: true\n\n  # From Turgut et. al (2025):\n  #   \"We augment the 12-lead ECG data using\n  #   random cropping (scale=0.5 only during pre-training),\n  #   Gaussian noise (sigma=0.25 during pre-training, 0.2 during finetuning),\n  #   amplitude rescaling (sigma=0.5 during pretraining and fine-tuning),\n  #   and Fourier transform surrogates (phase noise magnitude=0.1 during pre-training, 0.075 during fine-tuning).\"\n  jitter_sigma: 0.2\n  rescaling_sigma: 0.5\n  ft_surr_phase_noise: 0.075\n\nmodel:\n  warmup_epochs: 10 # \u201c[..] over 400 epochs with a 5% warmup.\u201d (Turgut et al., 2025, p. 5)\n  learning_rate: 3e-6 # ([..] and the learning rate (10\u22126, 3\u00b710\u22126, 10\u22125, 3\u00b710\u22125)) (Turgut et al., 2025, p. 5)\n\ntrainer:\n  max_epochs: 200\n</code></pre> <p>This example demonstrates how an experiment configuration:</p> <ul> <li>Specifies the model architecture and its hyperparameters</li> <li>Defines data loading and augmentation pipeline</li> <li>Sets up training parameters and optimization strategy</li> <li>Selectively overrides desired components</li> </ul>"},{"location":"getting-started/configuration/#types","title":"Types","text":"<p>The following sections describe the three main archetypes of experiments supported by our configuration system. Each type builds upon the core components above but serves a distinct purpose in the model development lifecycle.</p> <ul> <li>Pre-training: Pre-training a model on a large dataset for self-supervised learning.</li> <li>Fine-tuning: Fine-tuning a model on a specific task using a pre-trained backbone.</li> <li>Embeddings/Artifacts: Generating embeddings and saliency maps for a given dataset using a pre-trained model.</li> </ul>"},{"location":"getting-started/configuration/#runtime-features","title":"Runtime Features","text":"<p>The configuration system provides several powerful runtime features that allow you to modify and extend experiment configurations without changing the base configuration files.</p>"},{"location":"getting-started/configuration/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>For hyperparameter search:</p> <pre><code># Run hyperparameter search for training\nrye run train -m hparams_search=cmr_classifier\n\n# Evaluate best model from sweep\nrye run eval experiment=cmr_acdc ckpt_path=path/to/best_model\n</code></pre>"},{"location":"getting-started/configuration/#multi-run-experiments","title":"Multi-Run Experiments","text":"<p>Run multiple configurations:</p> <pre><code># Training with different mask ratios\nrye run train -m model.mask_ratio=0.65,0.75,0.85\n\n# Evaluate multiple checkpoints\nrye run eval -m ckpt_path=path/to/ckpt1,path/to/ckpt2\n\n# Generate artifacts for multiple models\nrye run generate_artifacts -m ckpt_path=path/to/ckpt1,path/to/ckpt2\n</code></pre>"},{"location":"getting-started/configuration/#command-line-overrides","title":"Command Line Overrides","text":"<p>Override any configuration value:</p> <pre><code># Training overrides\nrye run train experiment=mae_pretraining trainer.max_epochs=200 data.batch_size=32\n\n# Evaluation overrides\nrye run eval experiment=mae_pretraining ckpt_path=path/to/model data.batch_size=64\n\n# Artifact generation overrides\nrye run generate_artifacts experiment=extract_features \\\n    ckpt_path=path/to/model accelerator=gpu splits=[train,test]\n</code></pre>"},{"location":"getting-started/configuration/#debug-mode","title":"Debug Mode","text":"<p>For development and debugging:</p> <pre><code># Debug training\nrye run train +debug=default\n\n# Debug evaluation\nrye run eval +debug=default debug.log_level=DEBUG\n\n# Debug artifact generation\nrye run generate_artifacts +debug=default debug.enabled=true\n</code></pre>"},{"location":"getting-started/configuration/#configuration-reference","title":"Configuration Reference","text":""},{"location":"getting-started/configuration/#available-configurations","title":"Available Configurations","text":""},{"location":"getting-started/configuration/#data-configs","title":"Data Configs","text":"<ul> <li><code>data/default.yaml</code>: Base dataset configuration</li> <li><code>data/cmr.yaml</code>: CMR dataset settings</li> <li><code>data/ecg.yaml</code>: ECG dataset settings</li> </ul>"},{"location":"getting-started/configuration/#model-configs","title":"Model Configs","text":"<ul> <li><code>model/mae.yaml</code>: Masked Autoencoder</li> <li><code>model/sim_clr.yaml</code>: SimCLR model</li> <li><code>model/cmr_encoder.yaml</code>: CMR encoder (backbone of CMR classifier)</li> <li><code>model/ecg_encoder.yaml</code>: ECG encoder (backbone of ECG classifier)</li> <li><code>model/cmr_classifier.yaml</code>: CMR classifier</li> <li><code>model/ecg_classifier.yaml</code>: ECG classifier</li> </ul>"},{"location":"getting-started/configuration/#trainer-configs","title":"Trainer Configs","text":"<ul> <li><code>trainer/default.yaml</code>: Default training settings</li> </ul>"},{"location":"getting-started/configuration/#experiment-configs","title":"Experiment Configs","text":"<p>Various predefined experiment configurations combining the above components.</p>"},{"location":"getting-started/configuration/#debugging-configurations","title":"Debugging Configurations","text":"<p>As this can grow quite complex, debugging can be useful.</p> <p>To debug your configuration:</p> <pre><code># Print the full config\nrye run train --cfg job\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation Guide","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>A Unix-like operating system (Linux, macOS) or Windows (not recommended)</li> <li>Rye installed on your system</li> </ul>"},{"location":"getting-started/installation/#step-by-step-installation","title":"Step-by-Step Installation","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://gitlab.fhnw.ch/ipole-bat/projection\ncd projection\n</code></pre></p> </li> <li> <p>Sync the environment to install all dependencies:    <pre><code>rye sync --no-lock\n</code></pre></p> </li> <li> <p>Set up environment variables:</p> </li> <li>Copy the <code>.env.example</code> file to <code>.env</code></li> <li> <p>Add required API keys for logging:      <pre><code>NEPTUNE_API_TOKEN=your_neptune_token\nWANDB_API_KEY=your_wandb_token\n</code></pre></p> </li> <li> <p>Activate the environment:    <pre><code>. .venv/bin/activate\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify your installation:</p> <ol> <li>Run the test suite:    <pre><code>rye run test\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues:</p> <ol> <li>Make sure Rye is properly installed</li> <li>Check that all dependencies are installed correctly</li> <li>Verify your Python version (requires &gt;= 3.12)</li> <li>Ensure all environment variables are set correctly</li> </ol>"},{"location":"models/architectures/","title":"Model Architectures","text":"<p>This section provides detailed documentation for all model architectures in the project. Our models are organized into several categories:</p> <ul> <li>Self-Supervised Learning: Pre-training architectures for learning representations without labels</li> <li>Encoders: Architectures for extracting representations from data</li> <li>Classifiers: Architectures for classifying data and modeling downstream tasks</li> </ul>"},{"location":"models/architectures/#base-components","title":"Base Components","text":"<p>The foundational interfaces and base classes that models build upon.</p> <p>Our models implement a sophisticated weights management system through the <code>PretrainedWeightsMixin</code>. This system is designed with several key principles:</p> <ol> <li>Robustness: Gracefully handle different weight file formats and structures</li> <li>Flexibility: Support partial loading and key matching</li> <li>Safety: Validate shapes and provide meaningful errors</li> <li>Transparency: Detailed logging of the loading process</li> </ol> <p>The mixin provides intelligent weight loading with features like: - Automatic nested state dict extraction - Configurable missing key tolerance - Shape validation - Detailed loading statistics - Extensible key matching logic</p> <p>Usage Example</p> <pre><code>class MyModel(nn.Module, PretrainedWeightsMixin):\n    def __init__(self):\n        super().__init__()\n        # ... model definition ...\n\n    def load_my_weights(self, path):\n        # Load with 20% missing key tolerance\n        self.load_pretrained_weights(path, strict=False, missing_key_threshold=0.2)\n</code></pre> <p>Design Philosophy</p> <p>The mixin is designed to solve common issues in deep learning weight management:</p> <ul> <li>Versioning: Models evolve, but weights should remain usable</li> <li>Flexibility: Support both exact and partial loading</li> <li>Debugging: Clear feedback about what was loaded</li> <li>Safety: Prevent silent failures with shape mismatches</li> <li>Extensibility: Easy to customize key matching logic</li> </ul> <p>The encoder interface abstracts the data preprocessing pipeline and provides a unified interface for all encoders in the project that can be used to extract features. This is mainly useful for the generation of representations and was specifically designed for this.</p>"},{"location":"models/architectures/#src.utils.model_weights","title":"<code>src.utils.model_weights</code>","text":""},{"location":"models/architectures/#src.utils.model_weights.PretrainedWeightsMixin","title":"<code>PretrainedWeightsMixin</code>","text":"<p>Mixin class for loading pretrained weights with intelligent key matching.</p> Source code in <code>src/utils/model_weights.py</code> <pre><code>class PretrainedWeightsMixin:\n    \"\"\"Mixin class for loading pretrained weights with intelligent key matching.\"\"\"\n\n    def load_pretrained_weights(\n        self,\n        weights_path: str,\n        strict: bool = False,\n        missing_key_threshold: float = 0.1,\n    ) -&gt; None:\n        \"\"\"Load pretrained weights with intelligent key matching.\n\n        Args:\n            weights_path: Path to the pretrained weights file\n            strict: Whether to strictly enforce matching keys\n            missing_key_threshold: Maximum allowed percentage of missing keys\n\n        Raises:\n            RuntimeError: If loading criteria are not met\n        \"\"\"\n        try:\n            state_dict = self._load_and_extract_state_dict(weights_path)\n\n            target_keys = set(self.state_dict().keys())\n            target_keys = self._get_filtered_state_dict_keys(target_keys)\n\n            new_state_dict, stats = self._match_and_validate_state_dict_keys(\n                state_dict, target_keys\n            )\n\n            self._validate_weights_loading_criteria(\n                stats, strict, missing_key_threshold\n            )\n            self.load_state_dict(new_state_dict, strict=False)\n\n            self._log_weights_loading_results(stats, weights_path)\n\n        except Exception as e:\n            logger.error(\n                f\"Error loading pretrained weights from {weights_path}: {str(e)}\"\n            )\n            raise\n\n    def _load_and_extract_state_dict(self, weights_path: str) -&gt; Dict[str, Any]:\n        state_dict = torch.load(weights_path, map_location=\"cpu\")\n\n        # Extract nested state dict if necessary\n        for key in [\"state_dict\", \"model\", \"network\"]:\n            if isinstance(state_dict, dict) and key in state_dict:\n                state_dict = state_dict[key]\n\n        return state_dict\n\n    def _get_filtered_state_dict_keys(self, target_keys: Set[str]) -&gt; Set[str]:\n        before_filter = len(target_keys)\n        before_filter_keys = target_keys.copy()\n        target_keys = {k for k in target_keys if self._load_state_dict_key(k)}\n        if len(target_keys) &lt; before_filter:\n            logger.info(\n                f\"Filtered {before_filter} keys to {len(target_keys)} keys after filtering. \"\n                f\"Filtered keys: {before_filter_keys - target_keys}\"\n            )\n        return target_keys\n\n    def _match_and_validate_state_dict_keys(\n        self, source_dict: Dict[str, torch.Tensor], target_keys: Set[str]\n    ) -&gt; Tuple[OrderedDict, Dict]:\n        new_state_dict = OrderedDict()\n        missing_keys = []\n        matched_keys = set()\n        shape_mismatches = []\n\n        source_keys = set(source_dict.keys())\n\n        for target_key in target_keys:\n            matching_key = self._find_matching_state_dict_key(target_key, source_keys)\n\n            if not matching_key:\n                missing_keys.append(target_key)\n                continue\n\n            if self._shapes_match(\n                source_dict[matching_key], self.state_dict()[target_key]\n            ):\n                new_state_dict[target_key] = source_dict[matching_key]\n                matched_keys.add(matching_key)\n            else:\n                shape_mismatches.append(f\"Shape mismatch for {target_key}\")\n\n        return new_state_dict, {\n            \"missing_keys\": missing_keys,\n            \"unexpected_keys\": list(source_keys - matched_keys),\n            \"shape_mismatches\": shape_mismatches,\n            \"total_keys\": len(target_keys),\n            \"matched_keys\": len(matched_keys),\n        }\n\n    @staticmethod\n    def _shapes_match(source_tensor: torch.Tensor, target_tensor: torch.Tensor) -&gt; bool:\n        return source_tensor.shape == target_tensor.shape\n\n    def _validate_weights_loading_criteria(\n        self, stats: Dict, strict: bool, missing_key_threshold: float\n    ) -&gt; None:\n        if stats[\"shape_mismatches\"]:\n            raise RuntimeError(\"\\n\".join(stats[\"shape_mismatches\"]))\n\n        missing_ratio = len(stats[\"missing_keys\"]) / stats[\"total_keys\"]\n        if missing_ratio &gt; missing_key_threshold:\n            raise RuntimeError(\n                f\"Too many missing keys: {len(stats['missing_keys'])}/{stats['total_keys']} \"\n                f\"({missing_ratio:.1%} &gt; {missing_key_threshold:.1%}) threshold. \"\n                f\"Missing keys: {stats['missing_keys']}\"\n            )\n\n        if strict and stats[\"missing_keys\"]:\n            raise RuntimeError(f\"Strict loading failed: {stats['missing_keys']}\")\n\n    def _log_weights_loading_results(self, stats: Dict, weights_path: str) -&gt; None:\n        if stats[\"missing_keys\"]:\n            logger.warning(\n                f\"Missing keys: {len(stats['missing_keys'])}/{stats['total_keys']} \"\n                f\"({len(stats['missing_keys']) / stats['total_keys']:.1%}).\"\n                f\"Missing keys: {stats['missing_keys']}\"\n            )\n        if stats[\"unexpected_keys\"]:\n            logger.warning(f\"Unexpected keys: {stats['unexpected_keys']}\")\n\n        logger.info(\n            f\"Successfully loaded weights from {weights_path} \"\n            f\"({stats['matched_keys']}/{stats['total_keys']} layers). \"\n            f\"Coverage: {stats['matched_keys'] / stats['total_keys']:.1%}\"\n        )\n\n    def _find_matching_state_dict_key(\n        self, target_key: str, available_keys: Set[str]\n    ) -&gt; Optional[str]:\n        \"\"\"Find matching key by handling the encoder prefix in model's state dict. Can be overridden.\"\"\"\n        if target_key in available_keys:\n            return target_key\n        return None\n\n    def _load_state_dict_key(self, key: str) -&gt; bool:\n        \"\"\"Filter keys to load. Can be overridden.\"\"\"\n        return True\n</code></pre>"},{"location":"models/architectures/#src.utils.model_weights.PretrainedWeightsMixin.load_pretrained_weights","title":"<code>load_pretrained_weights(weights_path, strict=False, missing_key_threshold=0.1)</code>","text":"<p>Load pretrained weights with intelligent key matching.</p> <p>Parameters:</p> Name Type Description Default <code>weights_path</code> <code>str</code> <p>Path to the pretrained weights file</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce matching keys</p> <code>False</code> <code>missing_key_threshold</code> <code>float</code> <p>Maximum allowed percentage of missing keys</p> <code>0.1</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If loading criteria are not met</p> Source code in <code>src/utils/model_weights.py</code> <pre><code>def load_pretrained_weights(\n    self,\n    weights_path: str,\n    strict: bool = False,\n    missing_key_threshold: float = 0.1,\n) -&gt; None:\n    \"\"\"Load pretrained weights with intelligent key matching.\n\n    Args:\n        weights_path: Path to the pretrained weights file\n        strict: Whether to strictly enforce matching keys\n        missing_key_threshold: Maximum allowed percentage of missing keys\n\n    Raises:\n        RuntimeError: If loading criteria are not met\n    \"\"\"\n    try:\n        state_dict = self._load_and_extract_state_dict(weights_path)\n\n        target_keys = set(self.state_dict().keys())\n        target_keys = self._get_filtered_state_dict_keys(target_keys)\n\n        new_state_dict, stats = self._match_and_validate_state_dict_keys(\n            state_dict, target_keys\n        )\n\n        self._validate_weights_loading_criteria(\n            stats, strict, missing_key_threshold\n        )\n        self.load_state_dict(new_state_dict, strict=False)\n\n        self._log_weights_loading_results(stats, weights_path)\n\n    except Exception as e:\n        logger.error(\n            f\"Error loading pretrained weights from {weights_path}: {str(e)}\"\n        )\n        raise\n</code></pre>"},{"location":"models/architectures/#src.models.encoder_interface","title":"<code>src.models.encoder_interface</code>","text":""},{"location":"models/architectures/#src.models.encoder_interface.EncoderInterface","title":"<code>EncoderInterface</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for neural network encoders that extract features from input data.</p> Source code in <code>src/models/encoder_interface.py</code> <pre><code>class EncoderInterface(ABC):\n    \"\"\"Interface for neural network encoders that extract features from input data.\"\"\"\n\n    @abstractmethod\n    def forward_features(\n        self, x: torch.Tensor, localized: bool = False\n    ) -&gt; torch.Tensor:\n        \"\"\"Extract features from input tensor.\n\n        Args:\n            x: Input tensor\n            localized: Whether to return localized features instead of global features\n\n        Returns:\n            Tensor of extracted features\n        \"\"\"\n        pass\n</code></pre>"},{"location":"models/architectures/#src.models.encoder_interface.EncoderInterface.forward_features","title":"<code>forward_features(x, localized=False)</code>  <code>abstractmethod</code>","text":"<p>Extract features from input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <code>localized</code> <code>bool</code> <p>Whether to return localized features instead of global features</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of extracted features</p> Source code in <code>src/models/encoder_interface.py</code> <pre><code>@abstractmethod\ndef forward_features(\n    self, x: torch.Tensor, localized: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Extract features from input tensor.\n\n    Args:\n        x: Input tensor\n        localized: Whether to return localized features instead of global features\n\n    Returns:\n        Tensor of extracted features\n    \"\"\"\n    pass\n</code></pre>"},{"location":"models/architectures/#self-supervised-learning","title":"Self-Supervised Learning","text":"<p>Our self-supervised learning implementations are based on state-of-the-art approaches adapted for medical data.</p> <p>Implementation Credits</p> <p>Our MAE implementation is based on:</p> <ul> <li>Original paper: \"Masked Autoencoders Are Scalable Vision Learners\" by He et al.</li> <li>Code adapted from Turgut et al.'s MAE implementation, which is a fork of Facebook Research's MAE</li> <li>Adapted for time series data with 1D signal masking and ECG-specific components</li> </ul> <p>Implementation Credits</p> <p>Our SimCLR implementation is based on:</p> <ul> <li>Original paper: \"A Simple Framework for Contrastive Learning of Visual Representations\" by Chen et al.</li> <li>Code adapted from MMCL-ECG-CMR by Turgut et al.</li> </ul>"},{"location":"models/architectures/#src.models.mae","title":"<code>src.models.mae</code>","text":""},{"location":"models/architectures/#src.models.mae_lit","title":"<code>src.models.mae_lit</code>","text":""},{"location":"models/architectures/#src.models.mae_lit.LitMAE","title":"<code>LitMAE</code>","text":"<p>               Bases: <code>MaskedAutoencoderViT</code>, <code>LightningModule</code></p> Source code in <code>src/models/mae_lit.py</code> <pre><code>class LitMAE(MaskedAutoencoderViT, pl.LightningModule):\n    def __init__(\n        self,\n        img_size,\n        patch_size,\n        embedding_dim,\n        depth,\n        num_heads,\n        decoder_embed_dim,\n        decoder_depth,\n        decoder_num_heads,\n        mlp_ratio,\n        norm_layer,\n        norm_pix_loss,\n        ncc_weight,\n        mask_ratio,\n        learning_rate,\n        weight_decay,\n        warmup_epochs,\n        max_epochs,\n        pretrained_weights,\n        min_lr=0.0,\n    ):\n        super().__init__(\n            img_size=img_size,\n            patch_size=patch_size,\n            embed_dim=embedding_dim,\n            depth=depth,\n            num_heads=num_heads,\n            decoder_embed_dim=decoder_embed_dim,\n            decoder_depth=decoder_depth,\n            decoder_num_heads=decoder_num_heads,\n            mlp_ratio=mlp_ratio,\n            norm_layer=norm_layer,\n            norm_pix_loss=norm_pix_loss,\n            ncc_weight=ncc_weight,\n            pretrained_weights=pretrained_weights,\n        )\n        self.save_hyperparameters()\n\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.warmup_epochs = warmup_epochs\n        self.min_lr = min_lr\n        self.max_epochs = max_epochs\n\n        self.mask_ratio = mask_ratio\n\n    def _step(self, batch):\n        samples, _, _ = batch\n        loss, samples_hat, samples_hat_masked = self(\n            samples, mask_ratio=self.mask_ratio\n        )\n\n        normalized_corr = self.ncc(samples, samples_hat)\n        batch_size = samples.shape[0]\n        return loss, normalized_corr, batch_size\n\n    def training_step(self, batch, batch_idx):\n        loss, normalized_corr, batch_size = self._step(batch)\n        loss_value = loss.item()\n        self.log_dict(\n            {\n                \"train/loss\": loss_value,\n                \"train/ncc\": normalized_corr,\n                \"lr\": self.optimizers().param_groups[0][\"lr\"],\n            },\n            on_step=True,\n            on_epoch=True,\n            batch_size=batch_size,\n            sync_dist=True,\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, normalized_corr, batch_size = self._step(batch)\n        loss_value = loss.item()\n        self.log_dict(\n            {\n                \"val/loss\": loss,\n                \"val/ncc\": normalized_corr,\n                \"lr\": self.optimizers().param_groups[0][\"lr\"],\n            },\n            on_epoch=True,\n            batch_size=batch_size,\n            sync_dist=True,\n        )\n\n        return loss_value\n\n    def configure_optimizers(self):\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_pretrain.py#L256\n        param_groups = optim_factory.add_weight_decay(self, self.weight_decay)\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_pretrain.py#L257\n        optimizer = torch.optim.AdamW(\n            param_groups, lr=self.learning_rate, betas=(0.9, 0.95)\n        )\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/util/lr_sched.py\n        scheduler = LinearWarmupCosineAnnealingLR(\n            optimizer,\n            warmup_epochs=self.warmup_epochs,\n            max_epochs=self.max_epochs,\n            eta_min=self.min_lr,\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\",  # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_pretrain.py#L79\n                \"frequency\": 1,\n            },\n        }\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_pretrain.py#L25\n    def norm(self, data: torch.Tensor()) -&gt; torch.Tensor():\n        \"\"\"\n        Zero-Normalize data to have mean=0 and standard_deviation=1\n\n        Parameters\n        ----------\n        data:  tensor\n        \"\"\"\n        mean = torch.mean(data, dim=-1, keepdim=True)\n        var = torch.var(data, dim=-1, keepdim=True)\n\n        return (data - mean) / (var + 1e-12) ** 0.5\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_pretrain.py#L38\n    def ncc(self, data_0: torch.Tensor(), data_1: torch.Tensor()) -&gt; torch.Tensor():\n        \"\"\"\n        Zero-Normalized cross-correlation coefficient between two data sets\n\n        Zero-Normalized cross-correlation equals the cosine of the angle between the unit vectors F and T,\n        being thus 1 if and only if F equals T multiplied by a positive scalar.\n\n        Parameters\n        ----------\n        data_0, data_1 :  tensors of same size\n        \"\"\"\n\n        nb_of_signals = 1\n        for dim in range(\n            data_0.dim() - 1\n        ):  # all but the last dimension (which is the actual signal)\n            nb_of_signals = nb_of_signals * data_0.shape[dim]\n\n        cross_corrs = (1.0 / (data_0.shape[-1] - 1)) * torch.sum(\n            self.norm(data=data_0) * self.norm(data=data_1), dim=-1\n        )\n        return cross_corrs.sum() / nb_of_signals\n</code></pre>"},{"location":"models/architectures/#src.models.mae_lit.LitMAE.ncc","title":"<code>ncc(data_0, data_1)</code>","text":"<p>Zero-Normalized cross-correlation coefficient between two data sets</p> <p>Zero-Normalized cross-correlation equals the cosine of the angle between the unit vectors F and T, being thus 1 if and only if F equals T multiplied by a positive scalar.</p>"},{"location":"models/architectures/#src.models.mae_lit.LitMAE.ncc--parameters","title":"Parameters","text":"<p>data_0, data_1 :  tensors of same size</p> Source code in <code>src/models/mae_lit.py</code> <pre><code>def ncc(self, data_0: torch.Tensor(), data_1: torch.Tensor()) -&gt; torch.Tensor():\n    \"\"\"\n    Zero-Normalized cross-correlation coefficient between two data sets\n\n    Zero-Normalized cross-correlation equals the cosine of the angle between the unit vectors F and T,\n    being thus 1 if and only if F equals T multiplied by a positive scalar.\n\n    Parameters\n    ----------\n    data_0, data_1 :  tensors of same size\n    \"\"\"\n\n    nb_of_signals = 1\n    for dim in range(\n        data_0.dim() - 1\n    ):  # all but the last dimension (which is the actual signal)\n        nb_of_signals = nb_of_signals * data_0.shape[dim]\n\n    cross_corrs = (1.0 / (data_0.shape[-1] - 1)) * torch.sum(\n        self.norm(data=data_0) * self.norm(data=data_1), dim=-1\n    )\n    return cross_corrs.sum() / nb_of_signals\n</code></pre>"},{"location":"models/architectures/#src.models.mae_lit.LitMAE.norm","title":"<code>norm(data)</code>","text":"<p>Zero-Normalize data to have mean=0 and standard_deviation=1</p>"},{"location":"models/architectures/#src.models.mae_lit.LitMAE.norm--parameters","title":"Parameters","text":"<p>data:  tensor</p> Source code in <code>src/models/mae_lit.py</code> <pre><code>def norm(self, data: torch.Tensor()) -&gt; torch.Tensor():\n    \"\"\"\n    Zero-Normalize data to have mean=0 and standard_deviation=1\n\n    Parameters\n    ----------\n    data:  tensor\n    \"\"\"\n    mean = torch.mean(data, dim=-1, keepdim=True)\n    var = torch.var(data, dim=-1, keepdim=True)\n\n    return (data - mean) / (var + 1e-12) ** 0.5\n</code></pre>"},{"location":"models/architectures/#src.models.sim_clr","title":"<code>src.models.sim_clr</code>","text":""},{"location":"models/architectures/#src.models.sim_clr.SimCLR","title":"<code>SimCLR</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Lightning module for imaging SimCLR.</p> <p>Alternates training between contrastive model and online classifier.</p> Source code in <code>src/models/sim_clr.py</code> <pre><code>class SimCLR(pl.LightningModule):\n    \"\"\"\n    Lightning module for imaging SimCLR.\n\n    Alternates training between contrastive model and online classifier.\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_backbone_model_name: str,\n        projection_dim: int,\n        temperature: float,\n        num_classes: int,\n        init_strat: str,\n        weights: Optional[List[float]],\n        learning_rate: float,\n        weight_decay: float,\n        lr_classifier: float,\n        weight_decay_classifier: float,\n        scheduler: str,\n        anneal_max_epochs: int,\n        warmup_epochs: int,\n        max_epochs: int,\n        check_val_every_n_epoch: int,\n        log_images: bool = False,\n        pretrained_weights: Optional[str] = None,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.encoder_backbone_model_name = encoder_backbone_model_name\n        self.projection_dim = projection_dim\n        self.temperature = temperature\n        self.num_classes = num_classes\n        self.init_strat = init_strat\n        self.weights = weights\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.lr_classifier = lr_classifier\n        self.weight_decay_classifier = weight_decay_classifier\n        self.scheduler = scheduler\n        self.anneal_max_epochs = anneal_max_epochs\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.check_val_every_n_epoch = check_val_every_n_epoch\n        self.log_images = log_images\n\n        # Manual optimization for multiple optimizers\n        self.automatic_optimization = False\n\n        # Initialize encoder\n        self.encoder_imaging = CMREncoder(\n            backbone_model_name=encoder_backbone_model_name,\n            pretrained_weights=pretrained_weights,\n        )\n        pooled_dim = self.encoder_imaging.pooled_dim\n\n        self.projection_head = SimCLRProjectionHead(\n            pooled_dim, pooled_dim, projection_dim\n        )\n        self.criterion_train = NTXentLoss(temperature=temperature)\n        self.criterion_val = NTXentLoss(temperature=temperature)\n\n        # Defines weights to be used for the classifier in case of imbalanced data\n        if not weights:\n            weights = [1.0 for _ in range(num_classes)]\n        self.weights = torch.tensor(weights)\n\n        # Classifier\n        self.classifier = LinearClassifier(\n            in_size=pooled_dim, num_classes=num_classes, init_type=init_strat\n        )\n        self.classifier_criterion = torch.nn.CrossEntropyLoss(weight=self.weights)\n\n        self.top1_acc_train = torchmetrics.Accuracy(\n            task=\"multiclass\", top_k=1, num_classes=num_classes\n        )\n        self.top1_acc_val = torchmetrics.Accuracy(\n            task=\"multiclass\", top_k=1, num_classes=num_classes\n        )\n\n        self.top5_acc_train = torchmetrics.Accuracy(\n            task=\"multiclass\", top_k=5, num_classes=num_classes\n        )\n        self.top5_acc_val = torchmetrics.Accuracy(\n            task=\"multiclass\", top_k=5, num_classes=num_classes\n        )\n\n        self.f1_train = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n        self.f1_val = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n\n        self.classifier_acc_train = torchmetrics.Accuracy(\n            task=\"multiclass\", num_classes=num_classes, average=\"weighted\"\n        )\n        self.classifier_acc_val = torchmetrics.Accuracy(\n            task=\"multiclass\", num_classes=num_classes, average=\"weighted\"\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Generates projection of data.\n        \"\"\"\n        x = self.encoder_imaging(x).flatten(start_dim=1)\n        z = self.projection_head(x)\n        return z\n\n    def training_step(\n        self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Alternates calculation of loss for training between contrastive model and online classifier.\n        \"\"\"\n        x0, x1, y, indices = batch\n\n        opt1, opt2 = self.optimizers()\n\n        # Train contrastive model using opt1\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n\n        loss, _, _ = self.criterion_train(z0, z1)\n\n        self.log(\n            \"imaging.train.loss\", loss, on_epoch=True, on_step=False, sync_dist=True\n        )\n        self.log(\n            \"imaging.train.top1\",\n            self.top1_acc_train,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n        self.log(\n            \"imaging.train.top5\",\n            self.top5_acc_train,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n\n        opt1.zero_grad()\n        self.manual_backward(loss)\n        opt1.step()\n\n        # Train classifier using opt2\n        embedding = torch.squeeze(self.encoder_imaging(x0))\n        y_hat = self.classifier(embedding)\n        cls_loss = self.classifier_criterion(y_hat, y)\n\n        y_hat = y_hat.argmax(dim=1)\n        y = y.argmax(dim=1)\n\n        self.f1_train(y_hat, y)\n        self.classifier_acc_train(y_hat, y)\n\n        self.log(\n            \"classifier.train.loss\",\n            cls_loss,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n        self.log(\n            \"classifier.train.f1\",\n            self.f1_train,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n        self.log(\n            \"classifier.train.accuracy\",\n            self.classifier_acc_train,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n\n        opt2.zero_grad()\n        self.manual_backward(cls_loss)\n        opt2.step()\n\n        return loss\n\n    def validation_step(\n        self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Validate both contrastive model and classifier\n        \"\"\"\n        x0, x1, y, indices = batch\n\n        # Validate contrastive model\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss, _, _ = self.criterion_val(z0, z1)\n\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n        self.log(\"imaging.val.loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n        self.log(\n            \"imaging.val.top1\",\n            self.top1_acc_val,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n        self.log(\n            \"imaging.val.top5\",\n            self.top5_acc_val,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n\n        # Validate classifier\n        self.classifier.eval()\n        embedding = torch.squeeze(self.encoder_imaging(x0))\n        y_hat = self.classifier(embedding)\n        loss = self.classifier_criterion(y_hat, y)\n\n        y_hat = y_hat.argmax(dim=1)\n        y = y.argmax(dim=1)\n\n        self.f1_val(y_hat, y)\n        self.classifier_acc_val(y_hat, y)\n\n        self.log(\n            \"classifier.val.loss\", loss, on_epoch=True, on_step=False, sync_dist=True\n        )\n        self.log(\n            \"classifier.val.f1\",\n            self.f1_val,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n        self.log(\n            \"classifier.val.accuracy\",\n            self.classifier_acc_val,\n            on_epoch=True,\n            on_step=False,\n            sync_dist=True,\n        )\n        self.classifier.train()\n\n        if not hasattr(self, \"validation_step_outputs\"):\n            self.validation_step_outputs = []\n        self.validation_step_outputs.append(x0)\n        return x0\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"\n        Log an image from each validation step using the appropriate logger.\n        \"\"\"\n        if self.log_images and hasattr(self, \"validation_step_outputs\"):\n            example_img = (\n                self.validation_step_outputs[0]\n                .cpu()\n                .detach()\n                .numpy()[0][0]  # First image in batch, first channel\n            )\n\n            if isinstance(self.logger, NeptuneLogger):\n                self.logger.run[\"Image Example\"].upload(File.as_image(example_img))\n            elif isinstance(self.logger, WandbLogger):\n                self.logger.log_image(key=\"Image Example\", images=[example_img])\n\n    def configure_optimizers(self) -&gt; Tuple[Dict, Dict]:\n        \"\"\"\n        Define and return optimizer and scheduler for contrastive model and online classifier.\n        Scheduler for online classifier often disabled\n        \"\"\"\n        optimizer = torch.optim.Adam(\n            [\n                {\"params\": self.encoder_imaging.parameters()},\n                {\"params\": self.projection_head.parameters()},\n            ],\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n        )\n        classifier_optimizer = torch.optim.Adam(\n            self.classifier.parameters(),\n            lr=self.lr_classifier,\n            weight_decay=self.weight_decay_classifier,\n        )\n\n        if self.scheduler == \"cosine\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer, T_max=self.anneal_max_epochs, eta_min=0, last_epoch=-1\n            )\n        elif self.scheduler == \"anneal\":\n            scheduler = LinearWarmupCosineAnnealingLR(\n                optimizer, warmup_epochs=self.warmup_epochs, max_epochs=self.max_epochs\n            )\n\n        classifier_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            classifier_optimizer,\n            patience=int(20 / self.check_val_every_n_epoch),\n            min_lr=self.lr_classifier * 0.0001,\n        )\n\n        return (\n            {\"optimizer\": optimizer, \"lr_scheduler\": scheduler},  # Contrastive\n            {\"optimizer\": classifier_optimizer},  # Classifier\n        )\n</code></pre>"},{"location":"models/architectures/#src.models.sim_clr.SimCLR.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Define and return optimizer and scheduler for contrastive model and online classifier. Scheduler for online classifier often disabled</p> Source code in <code>src/models/sim_clr.py</code> <pre><code>def configure_optimizers(self) -&gt; Tuple[Dict, Dict]:\n    \"\"\"\n    Define and return optimizer and scheduler for contrastive model and online classifier.\n    Scheduler for online classifier often disabled\n    \"\"\"\n    optimizer = torch.optim.Adam(\n        [\n            {\"params\": self.encoder_imaging.parameters()},\n            {\"params\": self.projection_head.parameters()},\n        ],\n        lr=self.learning_rate,\n        weight_decay=self.weight_decay,\n    )\n    classifier_optimizer = torch.optim.Adam(\n        self.classifier.parameters(),\n        lr=self.lr_classifier,\n        weight_decay=self.weight_decay_classifier,\n    )\n\n    if self.scheduler == \"cosine\":\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=self.anneal_max_epochs, eta_min=0, last_epoch=-1\n        )\n    elif self.scheduler == \"anneal\":\n        scheduler = LinearWarmupCosineAnnealingLR(\n            optimizer, warmup_epochs=self.warmup_epochs, max_epochs=self.max_epochs\n        )\n\n    classifier_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        classifier_optimizer,\n        patience=int(20 / self.check_val_every_n_epoch),\n        min_lr=self.lr_classifier * 0.0001,\n    )\n\n    return (\n        {\"optimizer\": optimizer, \"lr_scheduler\": scheduler},  # Contrastive\n        {\"optimizer\": classifier_optimizer},  # Classifier\n    )\n</code></pre>"},{"location":"models/architectures/#src.models.sim_clr.SimCLR.forward","title":"<code>forward(x)</code>","text":"<p>Generates projection of data.</p> Source code in <code>src/models/sim_clr.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Generates projection of data.\n    \"\"\"\n    x = self.encoder_imaging(x).flatten(start_dim=1)\n    z = self.projection_head(x)\n    return z\n</code></pre>"},{"location":"models/architectures/#src.models.sim_clr.SimCLR.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Log an image from each validation step using the appropriate logger.</p> Source code in <code>src/models/sim_clr.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"\n    Log an image from each validation step using the appropriate logger.\n    \"\"\"\n    if self.log_images and hasattr(self, \"validation_step_outputs\"):\n        example_img = (\n            self.validation_step_outputs[0]\n            .cpu()\n            .detach()\n            .numpy()[0][0]  # First image in batch, first channel\n        )\n\n        if isinstance(self.logger, NeptuneLogger):\n            self.logger.run[\"Image Example\"].upload(File.as_image(example_img))\n        elif isinstance(self.logger, WandbLogger):\n            self.logger.log_image(key=\"Image Example\", images=[example_img])\n</code></pre>"},{"location":"models/architectures/#src.models.sim_clr.SimCLR.training_step","title":"<code>training_step(batch, _)</code>","text":"<p>Alternates calculation of loss for training between contrastive model and online classifier.</p> Source code in <code>src/models/sim_clr.py</code> <pre><code>def training_step(\n    self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n) -&gt; torch.Tensor:\n    \"\"\"\n    Alternates calculation of loss for training between contrastive model and online classifier.\n    \"\"\"\n    x0, x1, y, indices = batch\n\n    opt1, opt2 = self.optimizers()\n\n    # Train contrastive model using opt1\n    z0 = self.forward(x0)\n    z1 = self.forward(x1)\n\n    loss, _, _ = self.criterion_train(z0, z1)\n\n    self.log(\n        \"imaging.train.loss\", loss, on_epoch=True, on_step=False, sync_dist=True\n    )\n    self.log(\n        \"imaging.train.top1\",\n        self.top1_acc_train,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n    self.log(\n        \"imaging.train.top5\",\n        self.top5_acc_train,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n\n    opt1.zero_grad()\n    self.manual_backward(loss)\n    opt1.step()\n\n    # Train classifier using opt2\n    embedding = torch.squeeze(self.encoder_imaging(x0))\n    y_hat = self.classifier(embedding)\n    cls_loss = self.classifier_criterion(y_hat, y)\n\n    y_hat = y_hat.argmax(dim=1)\n    y = y.argmax(dim=1)\n\n    self.f1_train(y_hat, y)\n    self.classifier_acc_train(y_hat, y)\n\n    self.log(\n        \"classifier.train.loss\",\n        cls_loss,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n    self.log(\n        \"classifier.train.f1\",\n        self.f1_train,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n    self.log(\n        \"classifier.train.accuracy\",\n        self.classifier_acc_train,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n\n    opt2.zero_grad()\n    self.manual_backward(cls_loss)\n    opt2.step()\n\n    return loss\n</code></pre>"},{"location":"models/architectures/#src.models.sim_clr.SimCLR.validation_step","title":"<code>validation_step(batch, _)</code>","text":"<p>Validate both contrastive model and classifier</p> Source code in <code>src/models/sim_clr.py</code> <pre><code>def validation_step(\n    self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n) -&gt; torch.Tensor:\n    \"\"\"\n    Validate both contrastive model and classifier\n    \"\"\"\n    x0, x1, y, indices = batch\n\n    # Validate contrastive model\n    z0 = self.forward(x0)\n    z1 = self.forward(x1)\n    loss, _, _ = self.criterion_val(z0, z1)\n\n    self.log(\"val_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n    self.log(\"imaging.val.loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n    self.log(\n        \"imaging.val.top1\",\n        self.top1_acc_val,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n    self.log(\n        \"imaging.val.top5\",\n        self.top5_acc_val,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n\n    # Validate classifier\n    self.classifier.eval()\n    embedding = torch.squeeze(self.encoder_imaging(x0))\n    y_hat = self.classifier(embedding)\n    loss = self.classifier_criterion(y_hat, y)\n\n    y_hat = y_hat.argmax(dim=1)\n    y = y.argmax(dim=1)\n\n    self.f1_val(y_hat, y)\n    self.classifier_acc_val(y_hat, y)\n\n    self.log(\n        \"classifier.val.loss\", loss, on_epoch=True, on_step=False, sync_dist=True\n    )\n    self.log(\n        \"classifier.val.f1\",\n        self.f1_val,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n    self.log(\n        \"classifier.val.accuracy\",\n        self.classifier_acc_val,\n        on_epoch=True,\n        on_step=False,\n        sync_dist=True,\n    )\n    self.classifier.train()\n\n    if not hasattr(self, \"validation_step_outputs\"):\n        self.validation_step_outputs = []\n    self.validation_step_outputs.append(x0)\n    return x0\n</code></pre>"},{"location":"models/architectures/#encoders","title":"Encoders","text":"<p>Model definitions mainly designed to obtain representations.</p> <p>Implementation Credits</p> <p>The classifier also bases on the MAE implementation by Turgut et al. as outlined in the MAE section.</p> <p>Implementation Credits</p> <p>The encoder also bases on the MMCL-ECG-CMR implementation by Turgut et al. as outlined in the MMCL-ECG-CMR section.</p>"},{"location":"models/architectures/#src.models.ecg_encoder","title":"<code>src.models.ecg_encoder</code>","text":""},{"location":"models/architectures/#src.models.ecg_encoder.ECGEncoder","title":"<code>ECGEncoder</code>","text":"<p>               Bases: <code>PretrainedWeightsMixin</code>, <code>EncoderInterface</code>, <code>VisionTransformer</code></p> Source code in <code>src/models/ecg_encoder.py</code> <pre><code>class ECGEncoder(\n    PretrainedWeightsMixin,\n    EncoderInterface,\n    timm.models.vision_transformer.VisionTransformer,\n):\n    def __init__(\n        self,\n        img_size: Union[Tuple[int, int, int], List[int]],\n        patch_size: Union[Tuple[int, int], List[int]],\n        embedding_dim: int,\n        depth: int,\n        num_heads: int,\n        mlp_ratio: float,\n        qkv_bias: bool,\n        norm_layer: Optional[nn.Module],\n        global_pool: str,\n        pretrained_weights: Optional[str] = None,\n    ):\n        if norm_layer is None:\n            norm_layer = partial(nn.LayerNorm, eps=1e-6)\n\n        super().__init__(\n            img_size=img_size,\n            patch_size=patch_size,\n            embed_dim=embedding_dim,\n            depth=depth,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            norm_layer=norm_layer,\n        )\n\n        self.global_pool = global_pool\n        self.embed_dim = embedding_dim\n        self.num_heads = num_heads\n        self.norm_layer = norm_layer\n\n        if self.global_pool == \"attention_pool\":\n            self.attention_pool = nn.MultiheadAttention(\n                embed_dim=embedding_dim, num_heads=num_heads, batch_first=True\n            )\n        if self.global_pool:\n            self.fc_norm = norm_layer(embedding_dim)\n            del self.norm  # remove the original norm\n\n        if pretrained_weights:\n            self.load_pretrained_weights(pretrained_weights)\n\n    def forward_features(self, x, localized=False):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(\n            B, -1, -1\n        )  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        if localized:\n            outcome = x[:, 1:]\n        elif self.global_pool == \"attention_pool\":\n            q = x[:, 1:, :].mean(dim=1, keepdim=True)\n            k = x[:, 1:, :]\n            v = x[:, 1:, :]\n            x, x_weights = self.attention_pool(\n                q, k, v\n            )  # attention pool without cls token\n            outcome = self.fc_norm(x.squeeze(dim=1))\n        elif self.global_pool:\n            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n            outcome = self.fc_norm(x)\n        else:\n            x = self.norm(x)\n            outcome = x[:, 0]\n\n        return outcome\n\n    def _find_matching_state_dict_key(\n        self, target_key: str, available_keys: set\n    ) -&gt; Optional[str]:\n        \"\"\"Find matching key by handling the encoder prefix in model's state dict.\n\n        Args:\n            target_key: Key from model's state dict (with 'encoder.' prefix)\n            available_keys: Keys available in the loaded weights\n\n        Returns:\n            Optional[str]: Matching key from available_keys if found, None otherwise\n        \"\"\"\n        if target_key.startswith(\"head.\"):\n            return None\n\n        return super()._find_matching_state_dict_key(target_key, available_keys)\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_encoder.vit_patchX","title":"<code>vit_patchX(**kwargs)</code>","text":"<p>Function to create Vision Transformer conforming to the pre-trained weights by Turgut et. al (2025)</p> Source code in <code>src/models/ecg_encoder.py</code> <pre><code>def vit_patchX(**kwargs):\n    \"\"\"Function to create Vision Transformer conforming to the pre-trained weights by Turgut et. al (2025)\"\"\"\n    model = ECGEncoder(\n        patch_size=(1, 100),  # To match patch_embed.proj.weight: [384, 1, 1, 100]\n        img_size=(1, 12, 2500),\n        embedding_dim=384,  # To match embedding dimension\n        depth=3,  # 3 transformer blocks\n        num_heads=6,  # 384/64=6 heads (standard head dim of 64)\n        mlp_ratio=4,  # Matches the 1536 dimension in mlp layers\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        **kwargs,\n    )\n    return model\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_encoder","title":"<code>src.models.cmr_encoder</code>","text":""},{"location":"models/architectures/#src.models.cmr_encoder.CMREncoder","title":"<code>CMREncoder</code>","text":"<p>               Bases: <code>PretrainedWeightsMixin</code>, <code>EncoderInterface</code>, <code>Module</code></p> Source code in <code>src/models/cmr_encoder.py</code> <pre><code>class CMREncoder(PretrainedWeightsMixin, EncoderInterface, nn.Module):\n    BACKBONE_MODELS = [\"resnet18\", \"resnet50\"]\n\n    def __init__(\n        self, backbone_model_name: str, pretrained_weights: Optional[str] = None\n    ):\n        super().__init__()\n        if backbone_model_name not in self.BACKBONE_MODELS:\n            raise ValueError(f\"Unknown backbone model: {backbone_model_name}\")\n\n        if backbone_model_name == \"resnet18\":\n            resnet = torchvision.models.resnet18()\n            self.pooled_dim = 512\n        elif backbone_model_name == \"resnet50\":\n            resnet = torchvision.models.resnet50()\n            self.pooled_dim = 2048\n        else:\n            raise ValueError(f\"Unknown model type: {backbone_model_name}\")\n\n        self.encoder = self._remove_last_layer(resnet)\n\n        if pretrained_weights:\n            self.load_pretrained_weights(pretrained_weights)\n\n    def _find_matching_state_dict_key(\n        self, target_key: str, available_keys: set\n    ) -&gt; Optional[str]:\n        \"\"\"Find matching key by handling the encoder prefix in model's state dict.\n\n        Args:\n            target_key: Key from model's state dict (with 'encoder.' prefix)\n            available_keys: Keys available in the loaded weights\n\n        Returns:\n            Optional[str]: Matching key from available_keys if found, None otherwise\n        \"\"\"\n        for prefix in (\"encoder.\", \"\"):\n            imaging_key = f\"encoder_imaging.{target_key[len(prefix):]}\"\n            if imaging_key in available_keys:\n                return imaging_key\n\n        return super()._find_matching_state_dict_key(target_key, available_keys)\n\n    def _remove_last_layer(self, resnet):\n        \"\"\"\n        Remove the fully connected layer and pooling layer from the resnet model.\n        \"\"\"\n        return nn.Sequential(*list(resnet.children())[:-1])\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.encoder(x).squeeze()\n\n    def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.forward(x)\n</code></pre>"},{"location":"models/architectures/#classifiers","title":"Classifiers","text":"<p>Model definitions mainly designed to classify data. Generally, these models can be extended to perform any kind of downstream task.</p> <p>Implementation Credits</p> <p>The classifier also bases on the MAE implementation by Turgut et al. as outlined in the MAE section.</p> <p>Implementation Credits</p> <p>The classifier also bases on the SimCLR implementation by Turgut et al. as outlined in the SimCLR section.</p>"},{"location":"models/architectures/#src.models.ecg_classifier","title":"<code>src.models.ecg_classifier</code>","text":""},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier","title":"<code>ECGClassifier</code>","text":"<p>               Bases: <code>PretrainedWeightsMixin</code>, <code>MetricsMixin</code>, <code>VisionTransformer</code>, <code>LightningModule</code></p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>class ECGClassifier(\n    PretrainedWeightsMixin,\n    MetricsMixin,\n    timm.models.vision_transformer.VisionTransformer,\n    pl.LightningModule,\n):\n    def __init__(\n        self,\n        img_size: Union[Tuple[int, int, int], List[int]],\n        patch_size: Union[Tuple[int, int], List[int]],\n        embedding_dim: int,\n        depth: int,\n        num_heads: int,\n        mlp_ratio: float,\n        qkv_bias: bool,\n        num_classes: int,\n        learning_rate: float,\n        weight_decay: float,\n        warmup_epochs: int,\n        max_epochs: int,\n        layer_decay: float,\n        norm_layer: nn.Module,\n        drop_path_rate: float,\n        smoothing: float,\n        task_type: Literal[\"multiclass\", \"multilabel\"],\n        global_pool=False,\n        masking_blockwise=False,\n        mask_ratio=0.0,\n        mask_c_ratio=0.0,\n        mask_t_ratio=0.0,\n        min_lr=0.0,\n        pretrained_weights: Optional[str] = None,\n        pos_weight: Optional[torch.Tensor] = None,\n    ):\n        self.save_hyperparameters()\n\n        super().__init__(\n            img_size=img_size,\n            patch_size=patch_size,\n            embed_dim=embedding_dim,\n            depth=depth,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            norm_layer=norm_layer,\n            drop_rate=drop_path_rate,\n            num_classes=num_classes,\n        )\n\n        self.pretrained_weights = pretrained_weights\n        if pretrained_weights:\n            self.load_pretrained_weights(pretrained_weights)\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L373\n        self.blocks[-1].attn.forward = ECGClassifier.attention_forward_wrapper(\n            self.blocks[-1].attn\n        )  # required to read out the attention map of the last layer\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L402\n        # manually initialize fc layer (as presumably not part of pretrained weights)\n        trunc_normal_(self.head.weight, std=0.01)  # 2e-5)\n\n        self.masking_blockwise = masking_blockwise\n        self.mask_ratio = mask_ratio\n        self.mask_c_ratio = mask_c_ratio\n        self.mask_t_ratio = mask_t_ratio\n\n        self.learning_rate = learning_rate\n        self.min_lr = min_lr\n        self.weight_decay = weight_decay\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.layer_decay = layer_decay\n\n        self.task_type = task_type\n        self.downstream_task = \"classification\"\n        self.pos_weight = pos_weight\n\n        self.global_pool = global_pool\n        if self.global_pool == \"attention_pool\":\n            self.attention_pool = nn.MultiheadAttention(\n                embed_dim=embedding_dim,\n                num_heads=num_heads,\n                batch_first=True,\n            )\n        if self.global_pool:\n            self.fc_norm = norm_layer(embedding_dim)\n            del self.norm  # remove the original norm\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L289\n        # self.class_weights = 2.0 / (\n        #    2.0 * torch.Tensor([1.0, 1.0])\n        # )  # total_nb_samples / (nb_classes * samples_per_class)\n        self.class_weights = None\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L445\n        # We deviate here in favor of BCEWithLogitsLoss which is multi-label compatible.\n        if task_type == \"multilabel\":\n            self.criterion = torch.nn.BCEWithLogitsLoss(\n                weight=self.class_weights, pos_weight=self.pos_weight\n            )\n        else:  # multiclass\n            self.criterion = torch.nn.CrossEntropyLoss(\n                weight=self.class_weights,\n                label_smoothing=smoothing,\n            )\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/models_vit.py#L44\n    def random_masking(self, x, mask_ratio):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling.\n        Per-sample shuffling is done by argsort random noise.\n        x: [N, L, D], sequence\n        \"\"\"\n        N, L, D = x.shape  # batch, length, dim\n        len_keep = int(L * (1 - mask_ratio))\n\n        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(\n            noise, dim=1\n        )  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([N, L], device=x.device)\n        mask[:, :len_keep] = 0\n\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/models_vit.py#L72\n    def random_masking_blockwise(self, x, mask_c_ratio, mask_t_ratio):\n        \"\"\"\n        2D: ECG recording (N, 1, C, T) (masking c and t under mask_c_ratio and mask_t_ratio)\n        Perform per-sample random masking by per-sample shuffling.\n        Per-sample shuffling is done by argsort random noise.\n        x: [N, L, D], sequence\n        \"\"\"\n        N, L, D = x.shape  # batch, length, dim\n        C, T = (\n            int(self.img_size[-2] / self.patch_size[-2]),\n            int(self.img_size[-1] / self.patch_size[-1]),\n        )\n\n        # mask C\n        x = x.reshape(N, C, T, D)\n        len_keep_C = int(C * (1 - mask_c_ratio))\n        noise = torch.rand(N, C, device=x.device)  # noise in [0, 1]\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(\n            noise, dim=1\n        )  # ascend: small is keep, large is remove\n        ids_keep = ids_shuffle[:, :len_keep_C]\n        index = ids_keep.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, T, D)\n        x = torch.gather(x, dim=1, index=index)  # N, len_keep_C(C'), T, D\n\n        # mask T\n        x = x.permute(0, 2, 1, 3)  # N C' T D =&gt; N T C' D\n        len_keep_T = int(T * (1 - mask_t_ratio))\n        noise = torch.rand(N, T, device=x.device)  # noise in [0, 1]\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(\n            noise, dim=1\n        )  # ascend: small is keep, large is remove\n        ids_keep = ids_shuffle[:, :len_keep_T]\n        index = ids_keep.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, len_keep_C, D)\n        x_masked = torch.gather(x, dim=1, index=index)\n        x_masked = x_masked.permute(0, 2, 1, 3)  # N T' C' D =&gt; N C' T' D\n\n        x_masked = x_masked.reshape(\n            N, len_keep_T * len_keep_C, D\n        )  # N C' T' D =&gt; N L' D\n\n        return x_masked, None, None\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/models_vit.py#L107\n    def forward_features(self, x):\n        \"\"\"\n        x: [B=N, L, D], sequence\n        \"\"\"\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        x = x + self.pos_embed[:, 1:, :]\n        if self.masking_blockwise:\n            x, _, _ = self.random_masking_blockwise(\n                x, self.mask_c_ratio, self.mask_t_ratio\n            )\n        else:\n            x, _, _ = self.random_masking(x, self.mask_ratio)\n\n        cls_token = self.cls_token + self.pos_embed[:, 0, :]\n        cls_tokens = cls_token.expand(\n            B, -1, -1\n        )  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        if self.global_pool == \"attention_pool\":\n            q = x[:, 1:, :].mean(dim=1, keepdim=True)\n            k = x[:, 1:, :]\n            v = x[:, 1:, :]\n            x, x_weights = self.attention_pool(\n                q, k, v\n            )  # attention pool without cls token\n            outcome = self.fc_norm(x.squeeze(dim=1))\n        elif self.global_pool:\n            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n            outcome = self.fc_norm(x)\n        else:\n            x = self.norm(x)\n            outcome = x[:, 0]\n\n        return outcome\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/models_vit.py#L144\n    def forward_head(self, x, pre_logits: bool = False):\n        if self.global_pool:\n            x = (\n                x[:, self.num_prefix_tokens :].mean(dim=1)\n                if self.global_pool == \"avg\"\n                else x[:, :]\n            )\n        x = self.fc_norm(x)\n\n        if self.downstream_task == \"classification\":\n            return x if pre_logits else self.head(x)\n        elif self.downstream_task == \"regression\":\n            return x if pre_logits else self.head(x)  # .sigmoid()\n\n    def training_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], _\n    ) -&gt; torch.Tensor:\n        \"\"\"Training step for downstream task.\"\"\"\n        x, y, _ = batch\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L261\n        # We inherit forward from VisionTransformer, which calls forward_features and forward_head as per reference above\n        y_hat = self(x)\n\n        loss = self.criterion(y_hat, y)\n        self.log(\"train_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n\n        self.compute_metrics(y_hat, y.long(), \"train\")\n        return loss\n\n    def validation_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor, int], _\n    ) -&gt; torch.Tensor:\n        \"\"\"Validation step for downstream task.\"\"\"\n        x, y, _ = batch\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L261\n        # We inherit forward from VisionTransformer, which calls forward_features and forward_head as per reference above\n        y_hat = self(x)\n\n        loss = self.criterion(y_hat, y)\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n\n        self.compute_metrics(y_hat, y.long(), \"val\")\n        return loss\n\n    def test_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor, int], _\n    ) -&gt; torch.Tensor:\n        \"\"\"Test step for downstream task.\"\"\"\n        x, y, _ = batch\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L261\n        # We inherit forward from VisionTransformer, which calls forward_features and forward_head as per reference above\n        y_hat = self(x)\n\n        loss = self.criterion(y_hat, y)\n        self.log(\"test_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n\n        self.compute_metrics(y_hat, y.long(), \"test\")\n        return loss\n\n    def configure_optimizers(self) -&gt; Dict:\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L438\n        param_groups = lrd.param_groups_lrd(\n            self,\n            self.weight_decay,\n            no_weight_decay_list=self.no_weight_decay(),\n            layer_decay=self.layer_decay,\n        )\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L442C29-L442C33\n        optimizer = torch.optim.AdamW(param_groups, lr=self.learning_rate)\n\n        # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/util/lr_sched.py\n        scheduler = LinearWarmupCosineAnnealingLR(\n            optimizer,\n            warmup_epochs=self.warmup_epochs,\n            max_epochs=self.max_epochs,\n            eta_min=self.min_lr,\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\",\n                # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L74\n                \"frequency\": 1,\n            },\n        }\n\n    def on_train_epoch_end(self):\n        self.finalize_metrics(\"train\")\n\n    def on_validation_epoch_end(self):\n        self.finalize_metrics(\"val\")\n\n    def on_test_epoch_end(self):\n        self.finalize_metrics(\"test\")\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/main_finetune.py#L237C1-L259C22\n    @staticmethod\n    def attention_forward_wrapper(attn_obj):\n        \"\"\"\n        Modified version of def forward() of class Attention() in timm.models.vision_transformer\n        \"\"\"\n\n        def my_forward(x):\n            B, N, C = x.shape  # C = embed_dim\n            # (3, B, Heads, N, head_dim)\n            qkv = (\n                attn_obj.qkv(x)\n                .reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads)\n                .permute(2, 0, 3, 1, 4)\n            )\n            q, k, v = qkv.unbind(\n                0\n            )  # make torchscript happy (cannot use tensor as tuple)\n\n            # (B, Heads, N, N)\n            attn = (q @ k.transpose(-2, -1)) * attn_obj.scale\n            attn = attn.softmax(dim=-1)\n            attn = attn_obj.attn_drop(attn)\n            # (B, Heads, N, N)\n            attn_obj.attn_map = attn  # this was added\n\n            # (B, N, Heads*head_dim)\n            x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n            x = attn_obj.proj(x)\n            x = attn_obj.proj_drop(x)\n            return x\n\n        return my_forward\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.attention_forward_wrapper","title":"<code>attention_forward_wrapper(attn_obj)</code>  <code>staticmethod</code>","text":"<p>Modified version of def forward() of class Attention() in timm.models.vision_transformer</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>@staticmethod\ndef attention_forward_wrapper(attn_obj):\n    \"\"\"\n    Modified version of def forward() of class Attention() in timm.models.vision_transformer\n    \"\"\"\n\n    def my_forward(x):\n        B, N, C = x.shape  # C = embed_dim\n        # (3, B, Heads, N, head_dim)\n        qkv = (\n            attn_obj.qkv(x)\n            .reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = qkv.unbind(\n            0\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        # (B, Heads, N, N)\n        attn = (q @ k.transpose(-2, -1)) * attn_obj.scale\n        attn = attn.softmax(dim=-1)\n        attn = attn_obj.attn_drop(attn)\n        # (B, Heads, N, N)\n        attn_obj.attn_map = attn  # this was added\n\n        # (B, N, Heads*head_dim)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = attn_obj.proj(x)\n        x = attn_obj.proj_drop(x)\n        return x\n\n    return my_forward\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.forward_features","title":"<code>forward_features(x)</code>","text":"<p>x: [B=N, L, D], sequence</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>def forward_features(self, x):\n    \"\"\"\n    x: [B=N, L, D], sequence\n    \"\"\"\n    B = x.shape[0]\n    x = self.patch_embed(x)\n\n    x = x + self.pos_embed[:, 1:, :]\n    if self.masking_blockwise:\n        x, _, _ = self.random_masking_blockwise(\n            x, self.mask_c_ratio, self.mask_t_ratio\n        )\n    else:\n        x, _, _ = self.random_masking(x, self.mask_ratio)\n\n    cls_token = self.cls_token + self.pos_embed[:, 0, :]\n    cls_tokens = cls_token.expand(\n        B, -1, -1\n    )  # stole cls_tokens impl from Phil Wang, thanks\n    x = torch.cat((cls_tokens, x), dim=1)\n\n    x = self.pos_drop(x)\n\n    for blk in self.blocks:\n        x = blk(x)\n\n    if self.global_pool == \"attention_pool\":\n        q = x[:, 1:, :].mean(dim=1, keepdim=True)\n        k = x[:, 1:, :]\n        v = x[:, 1:, :]\n        x, x_weights = self.attention_pool(\n            q, k, v\n        )  # attention pool without cls token\n        outcome = self.fc_norm(x.squeeze(dim=1))\n    elif self.global_pool:\n        x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n        outcome = self.fc_norm(x)\n    else:\n        x = self.norm(x)\n        outcome = x[:, 0]\n\n    return outcome\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.random_masking","title":"<code>random_masking(x, mask_ratio)</code>","text":"<p>Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random noise. x: [N, L, D], sequence</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>def random_masking(self, x, mask_ratio):\n    \"\"\"\n    Perform per-sample random masking by per-sample shuffling.\n    Per-sample shuffling is done by argsort random noise.\n    x: [N, L, D], sequence\n    \"\"\"\n    N, L, D = x.shape  # batch, length, dim\n    len_keep = int(L * (1 - mask_ratio))\n\n    noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n\n    # sort noise for each sample\n    ids_shuffle = torch.argsort(\n        noise, dim=1\n    )  # ascend: small is keep, large is remove\n    ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n    # keep the first subset\n    ids_keep = ids_shuffle[:, :len_keep]\n    x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n    # generate the binary mask: 0 is keep, 1 is remove\n    mask = torch.ones([N, L], device=x.device)\n    mask[:, :len_keep] = 0\n\n    # unshuffle to get the binary mask\n    mask = torch.gather(mask, dim=1, index=ids_restore)\n\n    return x_masked, mask, ids_restore\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.random_masking_blockwise","title":"<code>random_masking_blockwise(x, mask_c_ratio, mask_t_ratio)</code>","text":"<p>2D: ECG recording (N, 1, C, T) (masking c and t under mask_c_ratio and mask_t_ratio) Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random noise. x: [N, L, D], sequence</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>def random_masking_blockwise(self, x, mask_c_ratio, mask_t_ratio):\n    \"\"\"\n    2D: ECG recording (N, 1, C, T) (masking c and t under mask_c_ratio and mask_t_ratio)\n    Perform per-sample random masking by per-sample shuffling.\n    Per-sample shuffling is done by argsort random noise.\n    x: [N, L, D], sequence\n    \"\"\"\n    N, L, D = x.shape  # batch, length, dim\n    C, T = (\n        int(self.img_size[-2] / self.patch_size[-2]),\n        int(self.img_size[-1] / self.patch_size[-1]),\n    )\n\n    # mask C\n    x = x.reshape(N, C, T, D)\n    len_keep_C = int(C * (1 - mask_c_ratio))\n    noise = torch.rand(N, C, device=x.device)  # noise in [0, 1]\n    # sort noise for each sample\n    ids_shuffle = torch.argsort(\n        noise, dim=1\n    )  # ascend: small is keep, large is remove\n    ids_keep = ids_shuffle[:, :len_keep_C]\n    index = ids_keep.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, T, D)\n    x = torch.gather(x, dim=1, index=index)  # N, len_keep_C(C'), T, D\n\n    # mask T\n    x = x.permute(0, 2, 1, 3)  # N C' T D =&gt; N T C' D\n    len_keep_T = int(T * (1 - mask_t_ratio))\n    noise = torch.rand(N, T, device=x.device)  # noise in [0, 1]\n    # sort noise for each sample\n    ids_shuffle = torch.argsort(\n        noise, dim=1\n    )  # ascend: small is keep, large is remove\n    ids_keep = ids_shuffle[:, :len_keep_T]\n    index = ids_keep.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, len_keep_C, D)\n    x_masked = torch.gather(x, dim=1, index=index)\n    x_masked = x_masked.permute(0, 2, 1, 3)  # N T' C' D =&gt; N C' T' D\n\n    x_masked = x_masked.reshape(\n        N, len_keep_T * len_keep_C, D\n    )  # N C' T' D =&gt; N L' D\n\n    return x_masked, None, None\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.test_step","title":"<code>test_step(batch, _)</code>","text":"<p>Test step for downstream task.</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>def test_step(\n    self, batch: Tuple[torch.Tensor, torch.Tensor, int], _\n) -&gt; torch.Tensor:\n    \"\"\"Test step for downstream task.\"\"\"\n    x, y, _ = batch\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L261\n    # We inherit forward from VisionTransformer, which calls forward_features and forward_head as per reference above\n    y_hat = self(x)\n\n    loss = self.criterion(y_hat, y)\n    self.log(\"test_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n\n    self.compute_metrics(y_hat, y.long(), \"test\")\n    return loss\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.training_step","title":"<code>training_step(batch, _)</code>","text":"<p>Training step for downstream task.</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>def training_step(\n    self, batch: Tuple[torch.Tensor, torch.Tensor], _\n) -&gt; torch.Tensor:\n    \"\"\"Training step for downstream task.\"\"\"\n    x, y, _ = batch\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L261\n    # We inherit forward from VisionTransformer, which calls forward_features and forward_head as per reference above\n    y_hat = self(x)\n\n    loss = self.criterion(y_hat, y)\n    self.log(\"train_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n\n    self.compute_metrics(y_hat, y.long(), \"train\")\n    return loss\n</code></pre>"},{"location":"models/architectures/#src.models.ecg_classifier.ECGClassifier.validation_step","title":"<code>validation_step(batch, _)</code>","text":"<p>Validation step for downstream task.</p> Source code in <code>src/models/ecg_classifier.py</code> <pre><code>def validation_step(\n    self, batch: Tuple[torch.Tensor, torch.Tensor, int], _\n) -&gt; torch.Tensor:\n    \"\"\"Validation step for downstream task.\"\"\"\n    x, y, _ = batch\n\n    # https://github.com/oetu/mae/blob/ba56dd91a7b8db544c1cb0df3a00c5c8a90fbb65/engine_finetune.py#L261\n    # We inherit forward from VisionTransformer, which calls forward_features and forward_head as per reference above\n    y_hat = self(x)\n\n    loss = self.criterion(y_hat, y)\n    self.log(\"val_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n\n    self.compute_metrics(y_hat, y.long(), \"val\")\n    return loss\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_classifier","title":"<code>src.models.cmr_classifier</code>","text":""},{"location":"models/architectures/#src.models.cmr_classifier.CMRClassifier","title":"<code>CMRClassifier</code>","text":"<p>               Bases: <code>CMREncoder</code>, <code>MetricsMixin</code>, <code>LightningModule</code></p> Source code in <code>src/models/cmr_classifier.py</code> <pre><code>class CMRClassifier(CMREncoder, MetricsMixin, pl.LightningModule):\n    def __init__(\n        self,\n        backbone_model_name: str,\n        num_classes: int,\n        weights: Optional[List[float]],\n        learning_rate: float,\n        weight_decay: float,\n        scheduler: str,\n        anneal_max_epochs: int,\n        warmup_epochs: int,\n        max_epochs: int,\n        freeze_encoder: bool,\n        classifier_type: str,\n        task_type: Literal[\"multiclass\", \"multilabel\"] = \"multiclass\",\n        pretrained_weights: Optional[str] = None,\n    ):\n        self.task_type = task_type\n        if task_type != \"multiclass\":\n            raise ValueError(\"CMRClassifier only supports multiclass classification\")\n\n        super().__init__(backbone_model_name, pretrained_weights)\n        self.save_hyperparameters()\n\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.initial_lr = learning_rate\n        self.weight_decay = weight_decay\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.anneal_max_epochs = anneal_max_epochs\n        self.scheduler = scheduler\n        self.freeze_encoder = freeze_encoder\n\n        if freeze_encoder:\n            for param in self.encoder.parameters():\n                param.requires_grad = False\n\n        #  https://github.com/oetu/MMCL-ECG-CMR/blob/bd3c18672de8e5fa73bb753613df94547bd6245b/mmcl/models/ResnetEvalModel.py#L50\n        #  NOTE: We specifically omit the possibility of using a projection head as the available model weights come all without it\n\n        # https://github.com/oetu/MMCL-ECG-CMR/blob/bd3c18672de8e5fa73bb753613df94547bd6245b/mmcl/models/ResnetEvalModel.py#L77\n        input_dim = self.pooled_dim\n        if classifier_type == \"mlp\":\n            self.head = nn.Sequential(\n                OrderedDict(\n                    [\n                        (\"fc1\", nn.Linear(input_dim, input_dim // 4)),\n                        (\"relu1\", nn.ReLU(inplace=True)),\n                        (\"fc2\", nn.Linear(input_dim // 4, input_dim // 16)),\n                        (\"relu2\", nn.ReLU(inplace=True)),\n                        (\"fc3\", nn.Linear(input_dim // 16, num_classes)),\n                    ]\n                )\n            )\n        else:\n            self.head = nn.Linear(input_dim, num_classes)\n\n        # https://github.com/oetu/MMCL-ECG-CMR/blob/bd3c18672de8e5fa73bb753613df94547bd6245b/mmcl/models/Evaluator.py#L41\n        if weights:\n            self.weights = torch.tensor(weights)\n        else:\n            self.weights = None\n        self.criterion = nn.CrossEntropyLoss(weight=self.weights)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through encoder and classifier.\"\"\"\n        x = super().forward(x)\n        x = self.head(x)\n        return x\n\n    def training_step(\n        self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n    ) -&gt; torch.Tensor:\n        \"\"\"Training step for classification.\"\"\"\n        x0, _, y, _ = batch\n        y_hat = self(x0)\n        y_true = y.argmax(dim=1)\n\n        loss = self.criterion(y_hat, y_true)\n        self.compute_metrics(y_hat, y_true, \"train\")\n        self.log(\"train_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n        return loss\n\n    def validation_step(\n        self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n    ) -&gt; torch.Tensor:\n        \"\"\"Validation step for classification.\"\"\"\n        x0, _, y, _ = batch\n        y_hat = self(x0)\n        y_true = y.argmax(dim=1)\n\n        loss = self.criterion(y_hat, y_true)\n        self.compute_metrics(y_hat, y_true, \"val\")\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n        return loss\n\n    def test_step(\n        self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n    ) -&gt; torch.Tensor:\n        \"\"\"Test step for classification.\"\"\"\n        x0, _, y, _ = batch\n        y_hat = self(x0)\n        y_true = y.argmax(dim=1)\n\n        loss = self.criterion(y_hat, y_true)\n        self.compute_metrics(y_hat, y_true, \"test\")\n        self.log(\"test_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self) -&gt; Dict:\n        \"\"\"Configure optimizer for classification task.\"\"\"\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n        )\n\n        if self.scheduler == \"cosine\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer, T_max=self.anneal_max_epochs, eta_min=0, last_epoch=-1\n            )\n\n        elif self.scheduler == \"anneal\":\n            scheduler = LinearWarmupCosineAnnealingLR(\n                optimizer, warmup_epochs=self.warmup_epochs, max_epochs=self.max_epochs\n            )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n        }\n\n    def on_train_epoch_end(self):\n        self.finalize_metrics(\"train\")\n\n    def on_validation_epoch_end(self):\n        self.finalize_metrics(\"val\")\n\n    def on_test_epoch_end(self):\n        self.finalize_metrics(\"test\")\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_classifier.CMRClassifier.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizer for classification task.</p> Source code in <code>src/models/cmr_classifier.py</code> <pre><code>def configure_optimizers(self) -&gt; Dict:\n    \"\"\"Configure optimizer for classification task.\"\"\"\n    optimizer = torch.optim.Adam(\n        self.parameters(),\n        lr=self.learning_rate,\n        weight_decay=self.weight_decay,\n    )\n\n    if self.scheduler == \"cosine\":\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=self.anneal_max_epochs, eta_min=0, last_epoch=-1\n        )\n\n    elif self.scheduler == \"anneal\":\n        scheduler = LinearWarmupCosineAnnealingLR(\n            optimizer, warmup_epochs=self.warmup_epochs, max_epochs=self.max_epochs\n        )\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": scheduler,\n    }\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_classifier.CMRClassifier.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through encoder and classifier.</p> Source code in <code>src/models/cmr_classifier.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through encoder and classifier.\"\"\"\n    x = super().forward(x)\n    x = self.head(x)\n    return x\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_classifier.CMRClassifier.test_step","title":"<code>test_step(batch, _)</code>","text":"<p>Test step for classification.</p> Source code in <code>src/models/cmr_classifier.py</code> <pre><code>def test_step(\n    self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n) -&gt; torch.Tensor:\n    \"\"\"Test step for classification.\"\"\"\n    x0, _, y, _ = batch\n    y_hat = self(x0)\n    y_true = y.argmax(dim=1)\n\n    loss = self.criterion(y_hat, y_true)\n    self.compute_metrics(y_hat, y_true, \"test\")\n    self.log(\"test_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n    return loss\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_classifier.CMRClassifier.training_step","title":"<code>training_step(batch, _)</code>","text":"<p>Training step for classification.</p> Source code in <code>src/models/cmr_classifier.py</code> <pre><code>def training_step(\n    self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n) -&gt; torch.Tensor:\n    \"\"\"Training step for classification.\"\"\"\n    x0, _, y, _ = batch\n    y_hat = self(x0)\n    y_true = y.argmax(dim=1)\n\n    loss = self.criterion(y_hat, y_true)\n    self.compute_metrics(y_hat, y_true, \"train\")\n    self.log(\"train_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n    return loss\n</code></pre>"},{"location":"models/architectures/#src.models.cmr_classifier.CMRClassifier.validation_step","title":"<code>validation_step(batch, _)</code>","text":"<p>Validation step for classification.</p> Source code in <code>src/models/cmr_classifier.py</code> <pre><code>def validation_step(\n    self, batch: Tuple[List[torch.Tensor], torch.Tensor], _\n) -&gt; torch.Tensor:\n    \"\"\"Validation step for classification.\"\"\"\n    x0, _, y, _ = batch\n    y_hat = self(x0)\n    y_true = y.argmax(dim=1)\n\n    loss = self.criterion(y_hat, y_true)\n    self.compute_metrics(y_hat, y_true, \"val\")\n    self.log(\"val_loss\", loss, on_epoch=True, on_step=False, sync_dist=True)\n    return loss\n</code></pre>"},{"location":"models/architectures/#utility-models","title":"Utility Models","text":"<p>Helper models for evaluation and feature extraction. These models are not part of the main model architecture and are not meant to be used directly but rather as building blocks for other models.</p>"},{"location":"models/architectures/#src.models.linear_classifier","title":"<code>src.models.linear_classifier</code>","text":""},{"location":"models/architectures/#src.models.linear_classifier.LinearClassifier","title":"<code>LinearClassifier</code>","text":"<p>               Bases: <code>Module</code></p> <p>Simple linear classifier that is a single fully connected layer from input to class prediction.</p> Source code in <code>src/models/linear_classifier.py</code> <pre><code>class LinearClassifier(nn.Module):\n    \"\"\"\n    Simple linear classifier that is a single fully connected layer from input to class prediction.\n    \"\"\"\n\n    def __init__(self, in_size: int, num_classes: int, init_type: str) -&gt; None:\n        super(LinearClassifier, self).__init__()\n        self.model = nn.Linear(in_size, num_classes)\n        self.init_type = init_type\n        self.model.apply(self.init_weights)\n\n    def init_weights(self, m, init_gain=0.02) -&gt; None:\n        \"\"\"\n        Initializes weights according to desired strategy\n        \"\"\"\n        if isinstance(m, nn.Linear):\n            if self.init_type == \"normal\":\n                nn.init.normal_(m.weight.data, 0, 0.001)\n            elif self.init_type == \"xavier\":\n                nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif self.init_type == \"kaiming\":\n                nn.init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")\n            elif self.init_type == \"orthogonal\":\n                nn.init.orthogonal_(m.weight.data, gain=init_gain)\n            if hasattr(m, \"bias\") and m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.model(x)\n</code></pre>"},{"location":"models/architectures/#src.models.linear_classifier.LinearClassifier.init_weights","title":"<code>init_weights(m, init_gain=0.02)</code>","text":"<p>Initializes weights according to desired strategy</p> Source code in <code>src/models/linear_classifier.py</code> <pre><code>def init_weights(self, m, init_gain=0.02) -&gt; None:\n    \"\"\"\n    Initializes weights according to desired strategy\n    \"\"\"\n    if isinstance(m, nn.Linear):\n        if self.init_type == \"normal\":\n            nn.init.normal_(m.weight.data, 0, 0.001)\n        elif self.init_type == \"xavier\":\n            nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n        elif self.init_type == \"kaiming\":\n            nn.init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")\n        elif self.init_type == \"orthogonal\":\n            nn.init.orthogonal_(m.weight.data, gain=init_gain)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            nn.init.constant_(m.bias.data, 0.0)\n</code></pre>"},{"location":"models/training/","title":"Model Training","text":"<p>This guide explains how to train models using our PyTorch Lightning-based training framework.</p>"},{"location":"models/training/#overview","title":"Overview","text":"<p>The training system consists of several key components:</p> <ol> <li>Lightning Modules: Models implemented as PyTorch Lightning modules</li> <li>Training Script: Central <code>train.py</code> script for experiment execution</li> <li>Docker Environment: Containerized training environment</li> <li>Slurm Integration: Scripts for cluster training</li> </ol>"},{"location":"models/training/#model-implementation","title":"Model Implementation","text":"<p>All models should be implemented as PyTorch Lightning modules.</p>"},{"location":"models/training/#key-components","title":"Key Components","text":"<ol> <li>Model Definition: Inherit from <code>LightningModule</code></li> <li>Training Step: Implement <code>training_step</code></li> <li>Validation Step: Implement <code>validation_step</code></li> <li>Test Step: Implement <code>test_step</code></li> <li>Configure Optimizers: Implement <code>configure_optimizers</code></li> </ol> <pre><code>from lightning import LightningModule\nimport torch.nn as nn\n\nclass MyModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(...)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criterion(y_hat, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters())\n</code></pre>"},{"location":"models/training/#training-script","title":"Training Script","text":"<p>The main training script (<code>src/train.py</code>) handles:</p> <ol> <li>Configuration management via Hydra</li> <li>DataModule initialization</li> <li>Model instantiation</li> <li>Logger and callback setup</li> <li>Training execution</li> </ol> <p>Training is recommended to be done in conjuction with experiment management.</p>"},{"location":"models/training/#basic-usage","title":"Basic Usage","text":"<pre><code># Train with experiment configuration\nrye run train experiment=experiment_name\n</code></pre>"},{"location":"models/training/#configuration","title":"Configuration","text":"<p>Training configurations are managed by Hydra and stored in <code>configs/</code>:</p> <pre><code># configs/train.yaml\ndefaults:\n  - model: base_model\n  - data: ptbxl\n  - trainer: default\n  - callbacks: default\n  - logger: wandb\n\nmodel:\n  lr: 0.001\n  hidden_size: 128\n\ndata:\n  batch_size: 64\n  num_workers: 4\n\ntrainer:\n  max_epochs: 100\n  accelerator: gpu\n  devices: 1\n</code></pre>"},{"location":"models/training/#training-environments","title":"Training Environments","text":""},{"location":"models/training/#local-development","title":"Local Development","text":"<p>For local development and testing, you can run training directly:</p> <pre><code># Install dependencies\npip install -r requirements.txt\n\n# Run training\npython src/train.py\n</code></pre>"},{"location":"models/training/#docker-environment","title":"Docker Environment","text":"<p>For reproducible training, use the provided Docker environment:</p> <pre><code># Build the container\ncd docker\n./build.sh\n\n# Run training in container\ndocker run --gpus all ipoleprojection/projection:latest python src/train.py\n</code></pre>"},{"location":"models/training/#cluster-training-slurm","title":"Cluster Training (Slurm)","text":"<p>For large-scale training on a Slurm cluster, use the provided scripts in <code>scripts/slurm/</code>:</p>"},{"location":"models/training/#basic-job-submission","title":"Basic Job Submission","text":"<pre><code># Submit a training job\nsbatch scripts/slurm/train.sh -o \"experiment=experiment_name\"\n</code></pre>"},{"location":"models/training/#resource-configuration","title":"Resource Configuration","text":"<p>The Slurm scripts include default resource configurations:</p> <pre><code>#SBATCH -t 1-5:00:00     # Run time (days-hours:minutes:seconds)\n#SBATCH -p performance   # Partition (queue)\n#SBATCH --gpus=1        # Number of GPUs\n#SBATCH --cpus-per-gpu=4 # CPUs per GPU\n#SBATCH --mem=32G       # Memory per node\n</code></pre> <p>Modify these settings in <code>scripts/slurm/train.sh</code> based on your needs.</p>"},{"location":"models/training/#job-management","title":"Job Management","text":"<pre><code># Check job status\nsqueue -u $USER\n\n# Cancel a job\nscancel &lt;job_id&gt;\n\n# View job logs\ntail -f logs/projection_train_&lt;job_id&gt;.out\ntail -f logs/projection_train_&lt;job_id&gt;.err\n</code></pre>"},{"location":"models/training/#singularity-container","title":"Singularity Container","text":"<p>The training jobs run within a Singularity container to ensure reproducibility and consistent environments across cluster nodes. The container setup includes:</p> <ol> <li>Container Location: Uses <code>$HOME/projection_latest.sif</code> as the container image</li> <li> <p>Mounted Directories:</p> <ul> <li>Workspace: Job-specific workspace at <code>/tmp/projection_${SLURM_JOB_ID}</code> \u2192 <code>/workspace</code></li> <li>Data: Project data from <code>$DATA_PATH</code> \u2192 <code>/workspace/data</code></li> </ul> </li> <li> <p>Environment Configuration:    <pre><code>SINGULARITYENV_LC_ALL=C.UTF-8\nSINGULARITYENV_LANG=C.UTF-8\nSINGULARITYENV_WANDB_API_KEY=$WANDB_API_KEY\n</code></pre></p> </li> <li> <p>GPU Support: Container runs with <code>--nv</code> flag for NVIDIA GPU access</p> </li> </ol> <p>To update the container image, use the provided script: <pre><code>scripts/slurm/pull_latest.sh\n</code></pre></p> <p>This containerized approach ensures consistent software environments, proper isolation, and reproducible experiments across different compute nodes in the cluster.</p>"},{"location":"models/training/#aws-sagemaker-training","title":"AWS SageMaker Training","text":"<p>Note: The AWS SageMaker integration is currently in beta and serves as a proof of concept. While the setup provides a solid foundation for cloud training, it has not undergone extensive testing and can encounter issues.</p> <p>The framework supports training on AWS SageMaker using custom ECR (Elastic Container Registry) images. This setup provides scalable cloud training with managed infrastructure.</p>"},{"location":"models/training/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>AWS CLI Setup:    <pre><code># Configure AWS CLI with your credentials\naws configure\n\n# Login to Amazon ECR\naws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;your-account&gt;.dkr.ecr.&lt;your-region&gt;.amazonaws.com\n</code></pre></p> </li> <li> <p>Docker Image:    Base image: PyTorch training container from AWS Deep Learning Containers    Location: <code>docker/aws/Dockerfile</code>    Custom additions:</p> <ul> <li>Rye package management</li> <li>SageMaker directory structure</li> <li>NVIDIA CUDA support</li> </ul> </li> </ol>"},{"location":"models/training/#directory-structure","title":"Directory Structure","text":"<pre><code>docker/aws/\n  \u2514\u2500\u2500 Dockerfile           # SageMaker-compatible container definition\n\nscripts/aws/\n  \u251c\u2500\u2500 entrypoint.sh       # Container entry point\n  \u251c\u2500\u2500 sm_entrypoint.py    # Hydra configuration adapter\n  \u251c\u2500\u2500 train.py            # SageMaker training script\n  \u2514\u2500\u2500 test.py            # Test script for validation\n</code></pre>"},{"location":"models/training/#configuration_1","title":"Configuration","text":"<p>The training job uses Hydra configurations with SageMaker-specific adaptations:</p> <ol> <li>Hyperparameters: Passed as SageMaker training job parameters are automatically translated to Hydra format</li> <li>Environment:     Uses standard SageMaker paths:<ul> <li><code>/opt/ml/input/data/training</code>: Training data</li> <li><code>/opt/ml/model</code>: Model artifacts</li> <li><code>/opt/ml/output</code>: Training outputs</li> </ul> </li> </ol>"},{"location":"models/training/#usage","title":"Usage","text":"<ol> <li> <p>Build and Push Image:    <pre><code># From project root\ndocker build -t &lt;your-repo&gt;/projection:latest -f docker/aws/Dockerfile .\ndocker push &lt;your-repo&gt;/projection:latest\n</code></pre></p> </li> <li> <p>Launch Training:    Use the AWS SageMaker SDK or console to launch a training job with:</p> <ul> <li>Training image: Your pushed ECR image</li> <li>Entry point: <code>/opt/ml/code/entrypoint.sh</code></li> <li>Hyperparameters: Passed as standard SageMaker hyperparameters</li> </ul> </li> </ol> <p>The SageMaker setup automatically handles infrastructure provisioning, data transfer, and artifact management while maintaining compatibility with the local training workflow through Hydra configurations.</p>"},{"location":"models/analysis/embedding-visualization/","title":"Embedding Space Visualization","text":"<p>This guide explains how to visualize and analyze embedding spaces using UMAP projections. The tools provided help you understand how embeddings evolve during training and how different groups of data are distributed in the embedding space.</p>"},{"location":"models/analysis/embedding-visualization/#core-visualization-functions","title":"Core Visualization Functions","text":""},{"location":"models/analysis/embedding-visualization/#umap-projection","title":"UMAP Projection","text":""},{"location":"models/analysis/embedding-visualization/#src.visualization.embedding_viz","title":"<code>src.visualization.embedding_viz</code>","text":"<p>Visualization utilities for embedding analysis and visualization. This module provides functions for dimensionality reduction, group analysis, and visualization of embeddings with various plotting utilities.</p>"},{"location":"models/analysis/embedding-visualization/#src.visualization.embedding_viz.run_umap","title":"<code>run_umap(embeddings, metric='euclidean', min_dist=0.1, random_state=42, n_neighbors=15, n_components=2)</code>","text":"Source code in <code>src/visualization/embedding_viz.py</code> <pre><code>def run_umap(\n    embeddings, metric=\"euclidean\", min_dist=0.1, random_state=42, n_neighbors=15, n_components=2\n):\n    reducer = umap.UMAP(\n        metric=metric,\n        min_dist=min_dist,\n        random_state=random_state,\n        n_neighbors=n_neighbors,\n        n_components=n_components,\n    )\n    embedding_2d = reducer.fit_transform(embeddings)\n    return embedding_2d\n</code></pre>"},{"location":"models/analysis/embedding-visualization/#global-visualization","title":"Global Visualization","text":""},{"location":"models/analysis/embedding-visualization/#src.visualization.embedding_viz","title":"<code>src.visualization.embedding_viz</code>","text":"<p>Visualization utilities for embedding analysis and visualization. This module provides functions for dimensionality reduction, group analysis, and visualization of embeddings with various plotting utilities.</p>"},{"location":"models/analysis/embedding-visualization/#group-analysis-tools","title":"Group Analysis Tools","text":""},{"location":"models/analysis/embedding-visualization/#src.visualization.embedding_viz","title":"<code>src.visualization.embedding_viz</code>","text":"<p>Visualization utilities for embedding analysis and visualization. This module provides functions for dimensionality reduction, group analysis, and visualization of embeddings with various plotting utilities.</p>"},{"location":"models/analysis/embedding-visualization/#src.visualization.embedding_viz.prepare_group_data","title":"<code>prepare_group_data(df, group_name, max_samples=100)</code>","text":"<p>For a given disease group, filter records that have EXACTLY one label with 'group_name' (but can have other labels in different groups), then store the other integration names for multi-labeled records.</p> Source code in <code>src/visualization/embedding_viz.py</code> <pre><code>def prepare_group_data(df, group_name, max_samples=100):\n    \"\"\"\n    For a given disease group, filter records that have EXACTLY one label with 'group_name'\n    (but can have other labels in different groups),\n    then store the other integration names for multi-labeled records.\n    \"\"\"\n    def has_exact_one_in_group(labels_meta):\n        count = sum(lbl.get(\"group\") == group_name for lbl in labels_meta)\n        return count == 1\n\n    df_group = df[df[\"labels_meta\"].apply(has_exact_one_in_group)].copy()\n\n    # Single integration_name for this group\n    df_group[\"integration_name\"] = df_group[\"labels_meta\"].apply(\n        lambda lm: extract_integration_name_for_group(lm, group_name)\n    )\n    df_group = df_group.dropna(subset=[\"integration_name\"])\n\n    # total label count\n    df_group[\"total_labels\"] = df_group[\"labels_meta\"].apply(len)\n\n    # Identify other integration names if multi-labeled\n    def get_other_integration_names(labels_meta):\n        \"\"\"\n        Return integration names for labels that do NOT belong to `group_name`.\n        \"\"\"\n        others = []\n        for lbl in labels_meta:\n            if lbl.get(\"group\") != group_name:\n                iname = lbl.get(\"integration_name\", \"Unknown\")\n                others.append(iname)\n        return sorted(set(others))\n\n    df_group[\"other_inames\"] = df_group[\"labels_meta\"].apply(get_other_integration_names)\n\n    # Mark \"exclusive\" vs \"multi\" based on whether there are other integration names\n    def is_multi(other_inames):\n        return \"multi\" if len(other_inames) &gt; 0 else \"exclusive\"\n\n    df_group[\"mlabel_flag\"] = df_group[\"other_inames\"].apply(is_multi)\n\n    # sort by total_labels asc, then sample\n    df_group_sorted = df_group.sort_values(\"total_labels\")\n    df_group_sub = df_group_sorted.head(max_samples)\n    return df_group, df_group_sub\n</code></pre>"},{"location":"models/analysis/embedding-visualization/#example-usage","title":"Example Usage","text":""},{"location":"models/analysis/embedding-visualization/#basic-embedding-comparison","title":"Basic Embedding Comparison","text":"<pre><code># Prepare embeddings\npretrained_embeddings = ...  # shape: (N, D)\nfinetuned_embeddings = ...   # shape: (N, D)\n\n# Create UMAP projections\numap_pretrained = run_umap(pretrained_embeddings)\numap_finetuned = run_umap(finetuned_embeddings)\n\n# Visualize\numaps_dict = {\n    'Pre-trained': umap_pretrained,\n    'Fine-tuned': umap_finetuned\n}\nplot_global_umap_grid(umaps_dict, metadata_df)\n</code></pre>"},{"location":"models/analysis/embedding-visualization/#group-analysis","title":"Group Analysis","text":"<pre><code># Analyze a specific disease group\ntarget_group = \"Atrial Fibrillation\"\ndf_group_full, df_group_sub = prepare_group_data(metadata_df, target_group)\n\n# Visualize group distribution\nfig, ax = overlay_group_on_embedding(umap_coords, metadata_df, df_group_sub)\nplt.title(f\"{target_group} Distribution\")\nplt.show()\n</code></pre>"},{"location":"models/analysis/embedding-visualization/#best-practices","title":"Best Practices","text":"<ol> <li>Consistency: Use the same UMAP parameters (metric, random_state) when comparing different embeddings.</li> <li>Sampling: For large datasets, consider using <code>prepare_group_data</code> to sample a manageable subset.</li> <li>Visual Clarity: <ul> <li>Use appropriate alpha values for background points</li> <li>Choose distinct colors for different groups</li> <li>Add legends and titles for clear interpretation</li> </ul> </li> </ol>"},{"location":"models/analysis/embedding-visualization/#advanced-customization","title":"Advanced Customization","text":"<p>The visualization functions are designed to be flexible:</p> <ul> <li>Modify color schemes by adjusting the <code>highlight_color</code> and background colors</li> <li>Customize marker styles for different types of samples</li> <li>Adjust figure sizes and grid layouts for different numbers of embeddings</li> <li>Add additional metadata overlays or annotations</li> </ul>"},{"location":"models/analysis/model-artifacts/","title":"Model Artifacts Generation","text":"<p>This guide explains how to generate and analyze model artifacts using our artifact generation pipeline. These artifacts help understand and visualize how models process and interpret data.</p>"},{"location":"models/analysis/model-artifacts/#overview","title":"Overview","text":"<p>The artifact generation system provides three main types of outputs:</p> <ol> <li>Embeddings: Feature vectors extracted from the model's encoder</li> <li>GradCAM Visualizations: Class activation maps showing which regions influence model decisions</li> <li>Attention Maps: Visualizations of the model's attention patterns (for transformer-based models)</li> </ol>"},{"location":"models/analysis/model-artifacts/#basic-usage","title":"Basic Usage","text":"<pre><code># Generate all artifacts for a model\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    ckpt_path=path/to/model.ckpt\n\n# Generate artifacts for specific splits\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    ckpt_path=path/to/model.ckpt splits=[train,val]\n\n# Generate artifacts for a specific record\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    ckpt_path=path/to/model.ckpt record_id=patient_123\n</code></pre>"},{"location":"models/analysis/model-artifacts/#configuration","title":"Configuration","text":"<p>Artifact generation is configured through Hydra, similar to training. Configurations are stored in <code>configs/experiment/embeddings/</code>:</p> <pre><code># configs/experiment/embeddings/default.yaml\ndefaults:\n  - model: mae_vit\n  - data: cmr_acdc\n  - paths: default\n\nmodel:\n  # Model-specific settings\n  backbone: vit_base_patch16\n\ndata:\n  # Data loading settings\n  batch_size: 32\n  num_workers: 4\n\noutput_dir: ${paths.artifact_dir}/${experiment}\n</code></pre>"},{"location":"models/analysis/model-artifacts/#artifact-types","title":"Artifact Types","text":""},{"location":"models/analysis/model-artifacts/#embeddings","title":"Embeddings","text":"<p>Embeddings are feature vectors extracted from the model's encoder layer. They represent high-level features learned by the model and can be used for:</p> <ul> <li>Similarity analysis</li> <li>Clustering</li> <li>Downstream tasks</li> <li>Model interpretation</li> </ul> <p>Embeddings are saved as PyTorch tensors and can be loaded using:</p> <pre><code>embeddings = torch.load('path/to/embeddings.pt')\n</code></pre>"},{"location":"models/analysis/model-artifacts/#gradcam-visualizations","title":"GradCAM Visualizations","text":"<p>GradCAM (Gradient-weighted Class Activation Mapping) highlights regions in the input that are important for the model's predictions. This is particularly useful for:</p> <ul> <li>Understanding model decisions</li> <li>Identifying relevant features</li> <li>Validating model behavior</li> <li>Debugging model predictions</li> </ul> <p>GradCAM images are generated for each class and saved as PNG files.</p>"},{"location":"models/analysis/model-artifacts/#attention-maps","title":"Attention Maps","text":"<p>For transformer-based models, attention maps visualize how different parts of the input attend to each other. These visualizations help:</p> <ul> <li>Understand attention patterns</li> <li>Analyze model behavior</li> <li>Debug attention mechanisms</li> <li>Validate model architecture</li> </ul> <p>Attention maps are saved as high-resolution PNG files with multiple subplots showing different attention heads.</p>"},{"location":"models/analysis/model-artifacts/#output-structure","title":"Output Structure","text":"<p>Generated artifacts are organized as follows:</p> <pre><code>artifact_dir/\n\u251c\u2500\u2500 embeddings/\n\u2502   \u251c\u2500\u2500 train_embeddings.pt\n\u2502   \u251c\u2500\u2500 val_embeddings.pt\n\u2502   \u2514\u2500\u2500 test_embeddings.pt\n\u251c\u2500\u2500 gradcam/\n\u2502   \u2514\u2500\u2500 patient_123/\n\u2502       \u251c\u2500\u2500 class_0.png\n\u2502       \u251c\u2500\u2500 class_1.png\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 attention_maps/\n    \u2514\u2500\u2500 patient_123_attention.png\n</code></pre>"},{"location":"models/analysis/model-artifacts/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/analysis/model-artifacts/#customizing-output-directory","title":"Customizing Output Directory","text":"<pre><code>rye run generate_artifacts experiment=embeddings/my_experiment \\\n    output_dir=path/to/custom/dir\n</code></pre>"},{"location":"models/analysis/model-artifacts/#generating-specific-artifact-types","title":"Generating Specific Artifact Types","text":"<pre><code># Only generate embeddings\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    generate_embeddings=true generate_gradcam=false generate_attention=false\n\n# Only generate GradCAM for specific classes\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    generate_embeddings=false generate_gradcam=true \\\n    target_classes=[0,1]\n</code></pre>"},{"location":"models/analysis/model-artifacts/#memory-management","title":"Memory Management","text":"<p>For large datasets or memory-intensive models:</p> <pre><code># Process with smaller batch size\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    data.batch_size=16\n\n# Generate artifacts for splits separately\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    splits=[train]\nrye run generate_artifacts experiment=embeddings/my_experiment \\\n    splits=[val,test]\n</code></pre>"}]}